[["introduction-to-regression-analysis.html", "46 Introduction to Regression Analysis 46.1 Generalize linear models 46.2 Model syntax and interpretation 46.3 Family history and diabetes", " 46 Introduction to Regression Analysis This chapter is under heavy development and may still undergo significant changes. As a reminder, we said that epidemiology is usually defined as something like, “the study of the occurrence and distribution of health-related states or events in specified populations, including the study of the determinants influencing such states, and the application of this knowledge to control the health problems.”1 In previous chapters, we reviewed some of the ways we can measure the occurrence of those states or events. We also reviewed measures of association – measures in the average change in the probability of a health state or event across levels of some other exposure, or exposures, of interest. In this part of the book, we are going to add another useful tool to our toolbox for measuring occurrence and association – regression analysis. If you haven’t heard the word “regression” before, its meaning probably isn’t self-evident. It may even sound weird or intimidating. Formally, regression can be defined in the following way. “A regression of a variable Y on another variable, say an exposure X, is a function that describes how some feature of the distribution of Y changes across population subgroups defined by values of X.”2 As an equation, the statement above can be written like this: \\[\\begin{equation} E(Y|X = x) \\tag{46.1} \\end{equation}\\] Where, \\(E()\\) means “expected value” or “average” of whatever is inside of the parentheses. So, we could articulate this equation in words as, The average value of Y when the variable X has a specific value, x. For example, we could write a statement like “the average grade in an introduction to epidemiology class is 95” as \\(E(Grade) = 95\\). Further, we could write a statement like “The average grade in an introduction to epidemiology class is a 98 among people who study 10 hours per week” as \\(E(Grade|Hours = 10) = 98\\). Notice that the \\(Y\\) term is commonly called the outcome variable, dependent variable, or the regressand. Similarly, the \\(X\\) term is commonly called the predictor variable, independent variable, or the regressor. We will try to use the terms regressand and regressor in this book – even though they are not the most commonly used terms and probably have no intuitive meaning to the vast majority of you. In fact, we will use those terms precisely because they probably don’t have any intuitive meaning to you. And because they don’t have any intuitive meaning to you, it is unlikely that they will invoke any of the preconceived notions in your mind, which may or may not be accurate, that the other terms would. For example, “outcome variable” and “dependent variable” both imply a cause-and-effect relationship for many people. But as we discussed in the measures of association chapter, associations frequently exist between two variables even when there isn’t a cause and effect relationship between two variables (i.e., spurious correlations). Conversely, the terms “regressand” and “regressor” don’t tend to imply a preference for causal, or even temporal, relationships. For example, let’s plug some example variables into the regression equation from above. Specifically, let’s make study hours the regressand and grade the regressor. The equation above is a totally valid, and possibly even useful, model of the relationship between study hours and grades, but it is not a valid causal model. How do we know that? Well, most of us intuitively know that our grade in this class (as opposed to a previous class) does not cause the number of hours we study. It couldn’t possibly. The studying occurs before we ever get the grade. Further, it doesn’t even really make sense to say that our grade in this class predicts the number of hours we will study. Again, the studying occurs before we ever get the grade. In this case, our intuition will help prevent us from making inaccurate statements about the results of regression analysis (i.e., grade causes study hours), but that won’t always be true. Causal and temporal relationships won’t always be so obvious. In those cases, the results can sometimes inadvertently invite us to make statements about the relationship between the variables that we can’t really justify. 46.1 Generalize linear models So far, we’ve been discussion regression as though it is a singular thing. In practice, regression models can take many different forms. Many of the most commonly used forms fall into a category of regression models called Generalized Linear Models, or GLMs for short. GLMs are generally made up of three components: A linear function that describes the average value of the regressand of the outcome. A transformation of the mean of the regressand (the link function). An assumed distribution of the regressand (from the exponential family). The table below gives the name, generalized model form (equation), link function, and assumed distribution of the regressand for linear regression, logistic regression, Poisson regression, and Cox proportional hazards regression models. Together, these four models represent the majority of the generalized linear models used in epidemiology. Figure 46.1: Four commonly used generalized linear models. 46.1.1 The glm function In R, we fill one of our generalized linear models to our data using the glm() function. Actually, this isn’t totally true. We the Cox proportional hazard model is fit to data using a different function. The Cox proportional hazards model is technically a generalized linear model, but it has some important differences from the other three shown above. Therefore, we will introduce linear, logistic, and Poisson regression only in this chapter. We will discuss the Cox proportional hazards model later as part of our broader discussion of survival analysis. In general, we will need to pass the following the following information to the glm() function when fitting a generalized linear model to our data in R: glm( # The regression formula Y ~ X # The distribution family and link function # The data frame ) First, we will pass a formula the glm() function takes the form Y ~ X, where Y is the regressand, tilde (~) means “equals”, and X is the regressor. This formula is very similar to the general model form shown in table 46.1 above. Second, we will use one of the following distribution family and link functions: Regression Model Distribution Family Link Function Linear gaussian() link = “identity” Logistic binomial() link = “logit” Poisson poisson() link = “log” And finally, we will pass the name of a data frame to the data argument of the glm() function. Although not strictly necessary, we will also use the broom package to to make the results from the glm() function a little easier to work with. # Load the packages we will need below library(dplyr, warn.conflicts = FALSE) library(broom) library(ggplot2) library(meantables) library(freqtables) 46.2 Model syntax and interpretation The introduction to R4Epi says that our philosophy is “to start each concept by showing you the end result and then deconstruct how we arrived at that result, where possible. We find that it is easier for many people to understand new concepts when learning them as a component of a final product.” In that spirit, this section will briefly cover the R syntax we will use to fit linear, logistic, and Poisson regression models. We will also briefly cover interpretation of the results. Before we can fit any models to our data, we need to have some data. Therefore, we will simulate some very simple data below that we can use for illustrative purposes. set.seed(123) n &lt;- 20 df &lt;- tibble( x_cont = rnorm(n, 10, 1), x_cat = sample(0:1, n, TRUE, c(.7, .3)), y_cont = if_else( x_cat == 0, x_cont + rnorm(n, 1, 0.1), x_cont - rnorm(n, 1, 0.1) ), y_cat = if_else( x_cat == 0, sample(0:1, n, TRUE, c(.9, .1)), sample(0:1, n, TRUE, c(.5, .5)) ), y_count = if_else( x_cat == 0, sample(1:5, n, TRUE, c(.1, .2, .3, .2, .2)), sample(1:5, n, TRUE, c(.3, .3, .2, .1, .1)) ) ) df ## # A tibble: 20 × 5 ## x_cont x_cat y_cont y_cat y_count ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 9.44 0 10.5 0 5 ## 2 9.77 0 10.7 0 2 ## 3 11.6 0 12.6 0 5 ## 4 10.1 0 11.2 0 3 ## 5 10.1 0 11.2 0 4 ## 6 11.7 0 12.8 0 2 ## 7 10.5 0 11.5 0 3 ## 8 8.73 0 9.73 0 4 ## 9 9.31 0 10.3 0 1 ## 10 9.55 1 8.53 1 1 ## 11 11.2 0 12.2 0 3 ## 12 10.4 0 11.3 0 2 ## 13 10.4 1 9.43 0 1 ## 14 10.1 0 11.3 0 4 ## 15 9.44 0 10.6 0 1 ## 16 11.8 0 12.7 0 2 ## 17 10.5 0 11.5 0 2 ## 18 8.03 1 7.03 0 2 ## 19 10.7 1 9.61 0 5 ## 20 9.53 0 10.5 0 4 46.2.1 Linear regression Now that we have some simulated data, we will fit our models. We will start with linear models. 46.2.1.1 Continuous regressand continuous regressor glm( y_cont ~ x_cont, # Formula family = gaussian(link = &quot;identity&quot;), # Family/distribution/Link function data = df # Data ) ## ## Call: glm(formula = y_cont ~ x_cont, family = gaussian(link = &quot;identity&quot;), ## data = df) ## ## Coefficients: ## (Intercept) x_cont ## -1.430 1.202 ## ## Degrees of Freedom: 19 Total (i.e. Null); 18 Residual ## Null Deviance: 38.79 ## Residual Deviance: 12.82 AIC: 53.86 46.2.1.1.1 Interpretation Intercept: The mean value of y_cont when x_cont is equal to zero. x_cont: The average change in y_cont for each one-unit increase in x_cont. 46.2.1.2 Continuous regressand categorical regressor glm( y_cont ~ x_cat, # Formula family = gaussian(link = &quot;identity&quot;), # Family/distribution/Link function data = df # Data ) ## ## Call: glm(formula = y_cont ~ x_cat, family = gaussian(link = &quot;identity&quot;), ## data = df) ## ## Coefficients: ## (Intercept) x_cat ## 11.287 -2.636 ## ## Degrees of Freedom: 19 Total (i.e. Null); 18 Residual ## Null Deviance: 38.79 ## Residual Deviance: 16.55 AIC: 58.98 46.2.1.2.1 Interpretation Intercept: The mean value of y_cont when x_cat is equal to zero. x_cat: The change in the mean value of y_cont when x_cat changes from zero to one. 46.2.2 Logistic regression Now, let’s take a look at a couple of logistic models. 46.2.2.1 Categorical regressand continuous regressor glm( y_cat ~ x_cont, # Formula family = binomial(link = &quot;logit&quot;), # Family/distribution/Link function data = df # Data ) ## ## Call: glm(formula = y_cat ~ x_cont, family = binomial(link = &quot;logit&quot;), ## data = df) ## ## Coefficients: ## (Intercept) x_cont ## 3.9377 -0.6977 ## ## Degrees of Freedom: 19 Total (i.e. Null); 18 Residual ## Null Deviance: 7.941 ## Residual Deviance: 7.535 AIC: 11.54 46.2.2.1.1 Interpretation Intercept: The natural log of the odds of y_cat when x_cont is equal to zero. x_cont: The average change in the natural log of the odds of y_cont for each one-unit increase in x_cont. 46.2.2.2 Categorical regressand categorical regressor glm( y_cat ~ x_cat, # Formula family = binomial(link = &quot;logit&quot;), # Family/distribution/Link function data = df # Data ) ## ## Call: glm(formula = y_cat ~ x_cat, family = binomial(link = &quot;logit&quot;), ## data = df) ## ## Coefficients: ## (Intercept) x_cat ## -21.57 20.47 ## ## Degrees of Freedom: 19 Total (i.e. Null); 18 Residual ## Null Deviance: 7.941 ## Residual Deviance: 4.499 AIC: 8.499 46.2.2.2.1 Interpretation Intercept: The natural log of the odds of y_cat when x_cat is equal to zero. x_catt: The average change in the natural log of the odds of y_cont when x_cat changes from zero to one. 46.2.3 Poisson regression Finally, let’s take a look at a couple of Poisson models. 46.2.3.1 Count regressand continuous regressor glm( y_count ~ x_cont, # Formula family = poisson(link = &quot;log&quot;), # Family/distribution/Link function data = df # Data ) ## ## Call: glm(formula = y_count ~ x_cont, family = poisson(link = &quot;log&quot;), ## data = df) ## ## Coefficients: ## (Intercept) x_cont ## 0.44666 0.05734 ## ## Degrees of Freedom: 19 Total (i.e. Null); 18 Residual ## Null Deviance: 13.74 ## Residual Deviance: 13.57 AIC: 73.74 46.2.3.1.1 Interpretation Intercept: The natural log of the mean of y_count when x_cont is equal to zero. x_cont: The average change in the log of the mean of y_cont for each one-unit increase in x_cont. 46.2.3.2 Count regressand categorical regressor glm( y_count ~ x_cat, # Formula family = poisson(link = &quot;log&quot;), # Family/distribution/Link function data = df # Data ) ## ## Call: glm(formula = y_count ~ x_cat, family = poisson(link = &quot;log&quot;), ## data = df) ## ## Coefficients: ## (Intercept) x_cat ## 1.0776 -0.2666 ## ## Degrees of Freedom: 19 Total (i.e. Null); 18 Residual ## Null Deviance: 13.74 ## Residual Deviance: 13.17 AIC: 73.33 46.2.3.2.1 Interpretation Intercept: The natural log of the mean of y_count when x_cat is equal to zero. x_cat: The average change in the natural log of the mean of y_count when x_cat changes from zero to one. In the sections above, we got a glimpse into the syntax we will need to use to perform linear, logistic, and Poisson regression in R. However, the syntax alone is not very useful if we don’t understand what’s happening in a more intuitive way. The the sections that follow, that’s exactly what we will try to develop. 46.3 Family history and diabetes For this example, we want to regress diabetes (0=Negative, 1=Positive) on Family History (0=No, 1=Yes), to investigate the relationship between diabetes and family history of diabetes in our data. For dichotomous variables that take the values 0 and 1 only, the mean is equal to the proportion of 1’s. - 1, 1, 1, 0, 0 - Mean = (1 + 1 + 1 + 0 + 0) / 5 = 0.6 - Proportion of 1s = 3/5 = 0.6 diabetes &lt;- tidyr::expand_grid( family_history = c(1, 0), diabetes = c(1, 0) ) %&gt;% mutate(count = c(58, 196, 63, 381)) %&gt;% tidyr::uncount(count) diabetes ## # A tibble: 698 × 2 ## family_history diabetes ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 ## 2 1 1 ## 3 1 1 ## 4 1 1 ## 5 1 1 ## 6 1 1 ## 7 1 1 ## 8 1 1 ## 9 1 1 ## 10 1 1 ## # ℹ 688 more rows Remember, in the frequentist view, the regression of a variable Y on another variable X is the function that describes how the average (mean) value of Y changes across population subgroups defined by values of X.2 The mean value of diabetes across subgroups defined by family history are: diabetes %&gt;% group_by(family_history) %&gt;% mean_table(diabetes) ## # A tibble: 2 × 11 ## response_var group_var group_cat n mean sd sem lcl ucl min max ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 diabetes family_history 0 444 0.14 0.35 0.0166 0.11 0.17 0 1 ## 2 diabetes family_history 1 254 0.23 0.42 0.0264 0.18 0.28 0 1 diabetes %&gt;% freq_table(family_history, diabetes) %&gt;% select(row_var:n, percent_row) ## # A tibble: 4 × 6 ## row_var row_cat col_var col_cat n percent_row ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 family_history 0 diabetes 0 381 85.8 ## 2 family_history 0 diabetes 1 63 14.2 ## 3 family_history 1 diabetes 0 196 77.2 ## 4 family_history 1 diabetes 1 58 22.8 Take a look at the proportion of people with family history of diabetes who have diabetes and the proportion of people without family history of diabetes who have diabetes. You should notice that it is the same as the subgroup means. This illustrates an important property: “When the regressand Y is a binary indicator (0, 1) variable, E(Y|X=x) is called a binary regression, and this regression simplifies in a very useful manner. Specifically, when Y can be only 0 or 1 the average E(Y|X=x) equals the proportion of population members who have Y=1 among those who have X=x.” Now, let’s fit a linear regression model to this data. The rest of this part of the book will discuss the use of the models in practice. References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
