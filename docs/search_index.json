[
["index.html", "R for Epidemiology Welcome", " R for Epidemiology Brad Cannell 2020-06-10 Welcome Welcome to R for Epidemiology! This electronic book was originally created to accompany my Introduction to R Programming for Epidemiologic Research course at the University of Texas Health Science Center School of Public Health. However, I hope it will be useful to anyone who is interested in R and epidemiology. This book was created by Brad Cannell and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. Other Reading You may also be interested in: Hands-on Programming with R by Garrett Grolemund. This book is designed to provide a friendly introduction to the R language. R for Data Science by Garrett Grolemund and Hadley Wickham. This book is designed to teach readers how to do data science with R. Statistical Inference via Data Science: A ModernDive inot R and the Tidyverse. This book is designed to be a gentle introduction to the practice of analyzing data and answering questions using data the way data scientists, statisticians, data journalists, and other researchers would. Reproducable Research with R and RStudio by Christopher Gandrud. This book gives you tools for data gathering, analysis, and presentation of results so that you can create dynamic and highly reproducible research. Advanced R by Hadley Wickham. This book is designed primarily for R users who want to improve their programming skills and understanding of the language. "],
["about-the-author.html", "About the Author", " About the Author Brad Cannell, PhD, MPH Associate Professor Department of Epidemiology, Human Genetics and Environmental Sciences University of Texas Health Science Center School of Public Health Dr. Cannell received his PhD in Epidemiology, and Graduate Certificate in Gerontology, in 2013 from the University of Florida. He received his MPH with a concentration in Epidemiology from the University of Louisville in 2009, and his BA in Political Science and Marketing from the University of North Texas in 2005. During his doctoral studies, he was a Graduate Research Assistant for the Florida Office on Disability and Health, an affiliated scholar with the Claude D. Pepper Older Americans Independence Center, and a student-inducted member of the Delta Omega Honorary Society in Public Health. In 2016, Dr. Cannell received a Graduate Certificate in Predictive Analytics from the University of Maryland University College, and a Certificate in Big Data and Social Analytics from the Massachusetts Institute of Technology. He previously held professional staff positions in the Louisville Metro Health Department and the Northern Kentucky Independent District Health Department. He spent three years as a project epidemiologist for the Florida Office on Disability and Health at the University of Florida. He also served as an Environmental Science Officer in the United States Army Reserves from 2009 to 2013. Dr. Cannell‚Äôs research is broadly focused on healthy aging and health-related quality of life. Specifically, he has published research focusing on preservation of physical and cognitive function, living and aging with disability, and understanding and preventing elder mistreatment. Additionally, he has a strong background and training in epidemiologic methods and predictive analytics. He has been principal or co-investigator on multiple trials and observational studies in community and healthcare settings. He is currently the principal investigator on multiple data-driven federally funded projects that utilize technological solutions to public health issues in novel ways. Contact Email Brad Cannell Brad Cannell‚Äôs LinkedIn Profile Brad Cannell‚Äôs Github page "],
["introduction.html", "Introduction", " Introduction Goals I‚Äôm going to start the introduction by writing down some basic goals that underlie the construction and content of this book. I‚Äôm writing this for you, the reader, but also to hold myself accountable as I write. So, feel free to read if you are interested or skip ahead if you aren‚Äôt. The goals of this book are: To teach you how to use R and RStudio as tools for applied epidemiology. It is not to turn you into a computer scientist or a hard-core R programmer. Therefore, some readers who are experienced programmers may catch some technical inaccuracies on what I consider to be the fine points of what R is doing ‚Äúunder the hood.‚Äù To make this writing as accessible and practically useful as possible without stripping out all of the complexity that makes doing epidemiology in real life a challenge. In other words, I‚Äôm going to try to give you all the tools you need to do epidemiology in ‚Äúreal world‚Äù (as opposed to ideal) conditions without providing a whole bunch of extraneous (often theoretical) stuff that detracts from doing. Having said that, I will strive to add links to the other (often theoretical) stuff for readers who are interested. To teach you to accomplish common tasks, rather than teach you to use functions. In many R courses and texts, I‚Äôve noticed a focus on learning all the things a function, or set of related functions, can do. It‚Äôs then up to you, the reader, to sift through all of these capabilities and decided which, if any, of the things that can be done will accomplish the tasks that you are actually trying to accomplish. Instead, I will strive to start with the end in mind. What is the task we are actually trying to accomplish? What are some functions/methods I could use to accomplish that task? What are the strengths and limitations of each? To start each concept with the end result and then deconstruct how we arrived at that result, where possible. I find that it is easier for me to understand new concepts when learning them as a component of a final product. To learn concepts with data instead of (or alongside) mathematical formulas and text descriptions, where possible. I find that it is easier for me to understand new concepts by seeing them in action. Text conventions used in this book Bold text is used to highlight important terms, URLs, email addresses, file names, and file extensions. Highlighted inline code is used to emphasize small sections of R code, program elements such as variable or function names, databases, data types, environment variables, statements, and keywords. "],
["installing-r-and-rstudio.html", "1 Installing R and RStudio 1.1 Download and install on a Mac 1.2 Download and install on a PC", " 1 Installing R and RStudio Before we can do any programming with R, we first have to download it to your computer. Fortunately, R is free, easy to install, and runs on all major operating systems (i.e., Mac and Windows). However, R by itself is not nearly as easy to use as when we combine it with another program called RStudio. Fortunately, RStudio is also free and will also run on all major operating systems. At this point, you may be wondering what R is, what RStudio is, and how they are related. We will answer those questions in the near future. However, in the interest of keeping things brief and simple, I‚Äôm not going to get into them right now. Instead, all you have to worry about is getting the R programming language and the RStudio IDE (IDE is short for interactive development environment) downloaded and installed on your computer. The steps involved are slightly different depending on whether you are using a Mac or a PC (i.e., Windows). Therefore, please feel free to use the navigation panel on the left-hand side of the screen to navigate directly to the instructions that you need for your computer. üóíSide Note: In this chapter, I cover how to download and install R and RStudio on both Mac and PC. However, I personally use a Mac; therefore, the screenshots in all following chapters will be from a Mac. The good news is that RStudio operates almost identically on Mac and PC. Step 1: Regardless of which operating system you are using, please make sure your computer is on, properly functioning, connected to the internet, and has enough space on your hard drive to save R and RStudio. 1.1 Download and install on a Mac Step 2: Navigate to the Comprehensive R Archive Network (CRAN), which is located at https://cran.r-project.org/. Step 3: Click on Download R for (Mac) OS X. Step 4: Click on the link for the latest version of R. As you are reading this, the newest version may be different than the version you see in this picture, but the location of the newest version should be roughly in the same place ‚Äì the middle of the screen under ‚ÄúLatest release:‚Äù. After clicking the link, R should start to download to your computer automatically. Step 5: Locate the package file you just downloaded and double click it. Unless you‚Äôve changed your download settings, this file will probably be in your ‚Äúdownloads‚Äù folder. That is the default location for most web browsers. After you locate the file, just double click it. Step 6: A dialogue box will open and ask you to make some decisions about how and where you want to install R on your computer. I typically just click ‚Äúcontinue‚Äù at every step without changing any of the default options. If R installed properly, you should now see it in your applications folder. Step 7: Now we need to install the RStudio IDE. To do this, navigate to the RStudio desktop download website, which is located at https://rstudio.com/products/rstudio/download/. On that page, click the download button under RStudio Desktop Open Source License Free. Step 8: Download the most current version for Mac. Step 9: Again, locate the dmg file you just downloaded and double click it. Unless you‚Äôve changed your download settings, this file should be in the same location as the R package file you already downloaded. Step 10: A new finder window should automatically pop up that looks like the one you see here. Click on the RStudio icon and drag it into the Applications folder. You should now see RStudio in your Applications folder. Double click the icon to open RStudio. If this warning pops up, just click Open. The RStudio IDE should open and look something like the window you see here. If so, you are good to go! üéâ 1.2 Download and install on a PC Step 2: Navigate to the Comprehensive R Archive Network (CRAN), which is located at https://cran.r-project.org/. Step 3: Click on Download R for Windows. Step 4: Click on the base link. Step 5: Click on the link for the latest version of R. As you are reading this, the newest version may be different than the version you see in this picture, but the location of the newest version should be roughly the same. After clicking, R should start to download to your computer. Step 6: Locate the installation file you just downloaded and double click it. Unless you‚Äôve changed your download settings, this file will probably be in your downloads folder. That is the default location for most web browsers. Step 7: A dialogue box will open that asks you to make some decisions about how and where you want to install R on your computer. I typically just click ‚ÄúNext‚Äù at every step without changing any of the default options. If R installed properly, you should now see it in the Windows start menu. Step 8: Now we need to install the RStudio IDE. To do this, navigate to the RStudio desktop download website, which is located at https://rstudio.com/products/rstudio/download/. On that page, click the download button under RStudio Desktop Open Source License Free. Step 9: Download the most current version for Windows. Step 10: Again, locate the installation file you just downloaded and double click it. Unless you‚Äôve changed your download settings, this file should be in the same location as the R installation file you already downloaded. Step 11: Another dialogue box will open and ask you to make some decisions about how and where you want to install RStudio on your computer. I typically just click ‚ÄúNext‚Äù at every step without changing any of the default options. When RStudio is finished installing, you should see RStudio in the Windows start menu. Click the icon to open RStudio. The RStudio IDE should open and look something like the window you see here. If so, you are good to go! üéâ "],
["what-is-r.html", "2 What is R? 2.1 What is data? 2.2 What is R?", " 2 What is R? At this point in the book, you should have installed R and RStudio on your computer, but you may be thinking to yourself, ‚ÄúI don‚Äôt even know what R is.‚Äù Well, in this chapter you‚Äôll find out. I‚Äôll start with an overview of the R language, and then briefly touch on its capabilities and uses. You‚Äôll also see a complete R program and some complete documents generated by R programs. In this book you‚Äôll learn how to create similar programs and documents, and by the end of the book you‚Äôll be able to write your own R programs and present your results in the form of an issue brief written for general audiences who may or may not have public health expertise. But, before we discuss R let‚Äôs discuss something even more basic ‚Äì data. Here‚Äôs a question for you: What is data? 2.1 What is data? Data is information about objects (e.g., people, places, schools) and observable phenomenon (e.g., weather, temperatures, and disease symptoms) that is recorded and stored somehow as a collection of symbols, numbers, and letters. So, data is just information that has be ‚Äúwritten‚Äù down. Here we have a table, which is a common way of organizing data. In R, we will typically refer to these tables as data frames. Each box is a data frame is called a cell. Moving from left to right across the data frame are columns. Columns are also sometimes referred to as variables. In this book, we will often use the terms columns and variables interchangeably. Each column in a data frame has one, and only one, type. For now, know that the type tells us what kind of data is contained in a column and what we can do with that data. You may have already noticed that 3 of the columns in the table we‚Äôve been looking at contain numbers and 1 of the columns contains words. These columns will have different types in R and we can do different things with them based on their type. For example, we could ask R to tell us what the average value of the numbers in the height column are, but it wouldn‚Äôt make sense to ask R to tell us the average value of the words in the Gender column. We will talk more about many of the different column types exist in R later in this book. The information contained in the first cell of each column is called the column name (or variable) name. R gives us a lot of flexibility in terms of what we can name our columns, but there are a few rules. Column names can contain letters, numbers and the dot (.) or underscore (_) characters. Additionally, they can begin with a letter or a dot ‚Äì as long as the dot is not followed by a number. So, a name like ‚Äú.2cats‚Äù is not allowed. Finally, R has some reserved words that you are not allowed to use for column names. These include: ‚Äúif‚Äù, ‚Äúelse‚Äù, ‚Äúrepeat‚Äù, ‚Äúwhile‚Äù, ‚Äúfunction‚Äù, ‚Äúfor‚Äù, ‚Äúin‚Äù, ‚Äúnext‚Äù, and ‚Äúbreak‚Äù. Moving from top to bottom across the table are rows, which are sometimes referred to as records. Finally, the contents of each cell are called values. You should now be up to speed on some basic terminology used by R, as well as other analytic, database, and spreadsheet programs. These terms will be used repeatedly throughout the course. 2.2 What is R? So, what is R? Well, R is an open source statistical programming language that was created in the 1990‚Äôs specifically for data analysis. We will talk more about what open source means later, but for now, just think of R as an easy (relatively üòÇ) way to ask your computer to do math and statistics for you. More specifically, by the end of this book you will be able to independently use R to access data, manage data, analyze data, and present the results of your analysis. Let‚Äôs quickly take a closer look at each of these. 2.2.1 Accessing data So, what do we mean by ‚Äúaccess data‚Äù? Well, individuals and organizations store their data using different computer programs that use different file types. Some common examples that you may come across in epidemiology are database files, spreadsheets, raw data files, and SAS data sets. No matter how the data is stored, you can‚Äôt do anything with it until you can get it into R, in a form that R can use, and in a location that you can reach. In other words, accessing your data. Therefore, among our first tasks in this course will be to access data. 2.2.2 Managing data This isn‚Äôt very specific, but managing data is all the things you may have to do to your data to get it ready for analysis. You may also hear people refer to this process as data wrangling or data munging. Some specific examples of data management tasks include: Validating and cleaning data. In other words, dealing with potential errors in the data. Subsetting data. For example, using only some of the columns or some of the rows. Creating new variables. For example, creating a BMI variable in a data frame that was sent to you with height and weight columns. Combining data frames. For example, combining sociodemographic data about study participants with data collected in the field during an intervention. You may sometimes hear people refer to the 80/20 rule in reference to data management. This ‚Äúrule‚Äù says that in a typical data analysis project, roughly 80% of your time will be spent on data management and only 20% will be spent on the analysis itself. I can‚Äôt provide you with any empirical evidence (i.e., data) to back this claim up. But, as a person who has been involved in many projects that involve the collection and analysis of data, I can tell you anecdotally that this ‚Äùrule‚Äù is probably pretty close to being accurate in most cases. Additionally, it‚Äôs been my experience that most students of epidemiology are required to take one or more classes that emphasize methods for analyzing data; however, almost none of them have taken a course that emphasizes data management! Therefore, because data management is such a large component of most projects that involve the collection and analysis of data, and because most readers will have already been exposed to data analysis to a much greater extent than data management, this course will heavily emphasize the latter. 2.2.3 Analyzing data As just discussed, this is probably the capability you most closely associate with R, and there is no doubt that R is a powerful tool for analyzing data. However, in this book we won‚Äôt go beyond using R to calculate basic descriptive statistics. For our purposes, descriptive statistics include: Measures of central tendency. For example, mean, median, and mode. Measures of dispersion. For example, variance and standard error. Measures for describing categorical variables. For example, counts and percentages. Describing data using graphs and charts. With R, we can describe our data using beautiful and informative graphs. 2.2.4 Presenting data And finally, the ultimate goal is typically to present your findings in some form or another. For example, a report, a website, or a journal article. With R you can present your results in many different formats with relative ease. In fact, this is one of my favorite things about R and RStudio. In this class you will learn how to take your text, tabular, or graphical results and then publish them in many different formats including Microsoft Word, html files that can be viewed in web browsers, and pdf documents. Let‚Äôs take a look at some examples. Microsoft Word documents. Click here to view an example report created for one of my research projects in Microsoft Word. PDF documents. Click here to view a data dictionary I created in PDF format. HTML files. Hypertext Markup Language (HTML) files are what you are looking at whenever you view a webpage. You can use are to create HTML files that others can view in their web browser. You can email them these files to view in their web browser, or you can make them available for others to view online just like any other website. Click here to view an example dashboard I created for one of my research projects. Web applications. You can even use R to create full-fledged web applications. View the RStudio website to see some examples. "],
["navigating-the-rstudio-interface.html", "3 Navigating the RStudio interface 3.1 The console 3.2 The environment pane 3.3 The files pane 3.4 The source pane 3.5 RStudio preferences", " 3 Navigating the RStudio interface You now have R and RStudio on your computer and you have some idea of what R and RStudio are. At this point, it is really common for people to open RStudio and get totally overwhelmed. ‚ÄúWhat am I looking at?‚Äù ‚ÄùWhat do I click first?‚Äù ‚ÄúWhere do I even start?‚Äù Don‚Äôt worry if these, or similar, thoughts have crossed your mind. You are in good company and we will start to clear some of them up in this chapter. When you first load RStudio you should see a screen that looks very similar to what you see in the picture below. 3.1 In the current view, you see three panes and each pane has multiple tabs. Don‚Äôt beat yourself up if this isn‚Äôt immediately obvious. I‚Äôll make it clearer soon. Figure 3.1: The default RStudio user interface. 3.1 The console The first pane we are going to talk about is the Console/Terminal/Jobs pane. 3.2 Figure 3.2: The R console. It‚Äôs called the Console/Terminal/Jobs pane because it has three tabs you can click on: Console, Terminal, and Jobs. However, we will mostly refer to it as the Console pane and we will mostly ignore the Terminal and Jobs tabs. We aren‚Äôt ignoring them because they aren‚Äôt useful; rather, we are ignoring them because using them isn‚Äôt essential for anything we discuss anytime soon, and I want to keep things as simple as possible. The console is the most basic way to interact with R. You can type a command to R into the console prompt (the prompt looks like ‚Äú&gt;‚Äù) and R will respond to what you type. For example, below I‚Äôve typed ‚Äú1 plus 1,‚Äù hit enter, and the R console returned the sum of the numbers 1 and 1. 3.3 Figure 3.3: Doing some addition in the R console. The number 1 you see in brackets before the 2 (i.e., [1]) is telling you that this line of results starts with the first result. That fact is obvious here because there is only one result. To make this idea clearer, let‚Äôs show you a result with multiple lines. Figure 3.4: Demonstrating a function that returns multiple results. In the screenshot above we see a couple new things demonstrated. 3.4 First, as promised, we have more than one line of results (or output). The first line of results starts with a 1 in brackets (i.e., [1]), which indicates that this line of results starts with the first result. In this case the first result is the number 2. The second line of results starts with a 29 in brackets (i.e., [29]), which indicates that this line of results starts with the twenty-ninth result. In this case the twenty-ninth result is the number 58. If you count the numbers in the first line, there should be 28 ‚Äì results 1 through 28. I also want to make it clear that ‚Äú1‚Äù and ‚Äú29‚Äù are NOT results themselves. They are just helping us count the number of results per line. The second new thing here that you may have noticed is our use of a function. Functions are a BIG DEAL in R. So much so that R is called a functional language. You don‚Äôt really need to know all the details of what that means; however, you should know that, in general, everything you do in R you will do with a function. By contrast, everything you create in R will be an object. If you wanted to make an analogy between the R language and the English language, functions are verbs ‚Äì they do things ‚Äì and objects are nouns ‚Äì they are things. This may be confusion right now. Don‚Äôt worry. It will become clearer soon. Most functions in R begin with the function name followed by parentheses. For example, seq(), sum(), and mean(). Question: What is the name of the function we used in the example above? It‚Äôs the seq() function ‚Äì short for sequence. Inside the function, you may notice that there are three pairs of words, equal symbols, and numbers that are separated by commas. They are, from = 2, to = 100, and by = 2. In this case, from, to, and by are all arguments to the seq() function. I don‚Äôt know why they are called arguments, but as far as we are concerned, they just are. We will learn more about functions and arguments later, but for now just know that arguments give functions the information they need to give us the result we want. In this case, the seq() function gives us a sequence of numbers, but we have to give it information about where that sequence should start, where it should end, and how many steps should be in the middle. Here the sequence begins with the value we gave to the from argument (i.e., 2), ends with the value we gave to the to argument (i.e., 100), and increases at each step by the number we gave to the by argument (i.e., 2). So, 2, 4, 6, 8 ‚Ä¶ 100. While it‚Äôs convenient, let‚Äôs also learn some programming terminology: Arguments: Arguments always go inside the parentheses of a function and give the function the information it needs to give us the result we want. Pass: In programming lingo, you pass a value to a function argument. For example, in the function call seq(from = 2, to = 100, by = 2) we could say that we passed a value of 2 to the from argument, we passed a value of 100 to the to argument, and we passed a value of 2 to the by argument. Returns: Instead of saying, ‚Äúthe seq() function gives us a sequence of numbers‚Ä¶‚Äù we could say, ‚Äúthe seq() function returns a sequence of numbers‚Ä¶‚Äù In programming lingo, functions return one or more results. üóíSide Note: The seq() function isn‚Äôt particularly important or noteworthy. I essentially chose it at random to illustrate some key points. However, arguments, passing values, and return values are extremely important concepts and we will return to them many times. 3.2 The environment pane The second pane we are going to talk about is the Environment/History/Connections pane. 3.5 However, we will mostly refer to it as the Environment pane and we will mostly ignore the History and Connections tab. We aren‚Äôt ignoring them because they aren‚Äôt useful; rather, we are ignoring them because using them isn‚Äôt essential for anything we will discuss anytime soon, and I want to keep things as simple as possible. Figure 3.5: The environment pane. The Environment pane shows you all the objects that R can currently use for data management or analysis. In this picture, 3.5 our environment is empty. Let‚Äôs create an object and add it to our Environment. Figure 3.6: The vector x in the global environment. Here we see that we created a new object called x, which now appears in our Global Environment. 3.6 This gives us another great opportunity to discuss some new concepts. First, we created the x object in the Console by assigning the value 2 to the letter x. We did this by typing ‚Äúx‚Äù followed by a less than symbol (&lt;), a dash symbol (-), and the number 2. R is kind of unique in this way. I have never seen another programming language (although I‚Äôm sure they are out there) that uses &lt;- to assign values to variables. By the way, &lt;- is called the assignment operator (or assignment arrow), and ‚Äùassign‚Äù here means ‚Äúmake x contain 2‚Äù or ‚Äúput 2 inside x.‚Äù In many other languages you would write that as x = 2. But, for whatever reason, in R it is &lt;-. Unfortunately, &lt;- is more awkward to type than =. Fortunately, RStudio gives us a keyboard shortcut to make it easier. To type the assignment operator in RStudio, just hold down Option + - (dash keey) on a Mac or Alt + - (dash key) on a PC and RStudio will insert &lt;- complete with spaces on either side of the arrow. This may still seem awkward at first, but you will get used to it. üóíSide Note: A note about using the letter ‚Äúx‚Äù: By convention, the letter ‚Äúx‚Äù is a widely used variable name. You will see it used a lot in example documents and online. However, there is nothing special about the letter x. We could have just as easily used any other letter (a &lt;- 2), word (variable &lt;- 2), or descriptive name (my_favorite_number &lt;- 2) that is allowed by R. Second, you can see that our Global Environment now includes the object x, which has a value of 2. In this case, we would say that x is a numeric vector of length 1 (i.e., it has one value stored in it). We will talk more about vectors and vector types soon. For now, just notice that objects that you can manipulate or analyze in R will appear in your Global Environment. ‚ö†Ô∏èWarning: R is a case sensitive language. That means that upper case x (X) and lower case x (x) are different things to R. So, if you assign 2 to lower case x (x &lt;- 2). And then later ask R to tell what number you stored in upper case X, you will get an error (Error: object 'X' not found). 3.3 The files pane Next, let‚Äôs talk about the Files/Plots/Packages/Help/Viewer pane (that‚Äôs a mouthful). 3.7 Figure 3.7: The Files/Plots/Packages/Help/Viewer pane. Again, some of these tabs are more applicable for us than others. For us, the files tab and the help tab will probably be the most useful. You can think of the files tab as a mini Finder window (for Mac) or a mini File Explorer window (for PC). The help tab is also extremely useful once you get acclimated to it. Figure 3.8: The help tab. For example, in the screenshot above 3.8 we typed the seq into the search bar. The help pane then shows us a page of documentation for the seq() function. The documentation includes a brief description of what the function does, outlines all the arguments the seq() function recognizes, and, if you scroll down, gives examples of using the seq() function. Admittedly, this help documentation can seem a little like reading Greek (assuming you don‚Äôt speak Greek) at first. But, you will get more comfortable using it with practice. I hated the help documentation when I was learning R. Now, I use it all the time. 3.4 The source pane There is actually a fourth pane available in RStudio. If you click on the icon shown below you will get the following dropdown box with a list of files you can create. 3.9 Figure 3.9: Click the new source file icon. If you click any of these options, a new pane will appear. I will arbitrarily pick the first option ‚Äì R Script. Figure 3.10: New source file options. When I do, a new pane appears. It‚Äôs called the source pane. In this case, the source pane contains an untitled R Script. We won‚Äôt get into the details now because I don‚Äôt want to overwhelm you, but soon you will do the majority of your R programming in the source pane. Figure 3.11: A blank R script in the source pane. 3.5 RStudio preferences Finally, I‚Äôm going to recommend that you change a few settings in RStudio before we move on. Start by going to RStudio -&gt; Preferences (on Mac) 3.12 Figure 3.12: Select the preferences menu on Mac. Or start by going to Tools -&gt; Global Options (on Windows) 3.13 Figure 3.13: Select the global options menu on Windows. In the ‚ÄúGeneral‚Äù tab, I recommend unchecking the ‚ÄúRestore .Rdata into workspace at startup‚Äù checkbox. I also recommend setting the ‚ÄúSave workspace .Rdata on exit‚Äù dropdown to ‚ÄúNever.‚Äù Finally, I recommend unchecking the ‚ÄúAlways save history (even when not saving .Rdata)‚Äù checkbox. 3.14 Figure 3.14: General options tab. On the ‚ÄúAppearance‚Äù tab, I‚Äôm going to change my Editor Theme to Twilight. It‚Äôs not so much that I‚Äôm recommending you change yours ‚Äì this is entirely personal preference ‚Äì I‚Äôm just letting you know why my screenshots will look different from here on out. 3.15 Figure 3.15: Appearance tab. I‚Äôm sure you still have lots of questions at this point. That‚Äôs totally natural. However, I hope you now feel like you have some idea of what you are looking at when you open RStudio. Most of you will naturally get more comfortable with RStudio as we move through the book. For those of you who want more resources now, here are some suggestions. RStudio IDE cheatsheet ModernDive: What are R and RStudio? "],
["speaking-rs-language.html", "4 Speaking R‚Äôs language 4.1 R is a language 4.2 The R interpreter 4.3 Errors 4.4 Functions 4.5 Objects 4.6 Comments 4.7 Packages 4.8 Programming style", " 4 Speaking R‚Äôs language Students taking my R for epidemiology course often come into the course thinking it will be a math or statistics course. In reality, this course is probably much closer to a foreign language course. There is no doubt that we need a foundational understanding of math and statistics to understand the results we get from R, but R will take care of all of the complicated stuff for us. All we have to do is learn how to ask R to do what we want it to do. To some extent, this entire book is about learning to communicate with R. So, in this chapter we will introduce the R programming language from the 30,000-foot level. 4.1 R is a language In the same way that many people use the English language to communicate with each other, we will use the R programming language to communicate with R. Just like the English language, the R language comes complete with its own structure and vocabulary. Unfortunately, just like the English language, it also includes some weird exceptions and occasional miscommunications. We‚Äôve already seen a couple examples of commands written to R in the R programming language. Specifically: # Store the value 2 in the variable x x &lt;- 2 # Print the contents of x to the screen x ## [1] 2 and # Print an example number sequence to the screen seq(from = 2, to = 100, by = 2) ## [1] 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 ## [20] 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 ## [39] 78 80 82 84 86 88 90 92 94 96 98 100 üóíSide Note: The gray the boxes you see above are called R code chunks and I created them (and this entire book) using something called R markdown. Can you believe that you can write an entire book with R and RStudio? How cool is that? You will learn to use R markdown documents later in this book. R markdown is great because it allows you to mix R code with narrative text and multimedia content as I‚Äôve done throughout the page you‚Äôre currently looking at. This makes it really easy for us to add context and aesthetic appeal to our results. 4.2 The R interpreter Question: I keep talking about ‚Äúspeaking‚Äù to R, but when you speak to R using the R language, who are you actually speaking to? Well, you are speaking to something called the R interpreter. The R interpreter takes the commands we‚Äôve written in the R language, sends them to your computer to do the actual work (e.g., get the mean of a set of numbers), and then translates the results of that work back to us in a form that we humans can understand (e.g., the mean is 25.5). At this stage, one of the key concepts for you to understand about the R language is that is extremely literal! Understanding the literal nature of R is important because it will be the underlying cause of a lot of errors in your R code. 4.3 Errors No matter what I write next, you are going to get errors in your R code. I still get errors in my R code every single time I write R code. However, my hope is that this section will help you begin to understand why you are getting errors when you get them and provide us with a common language for discussing errors. So, what exactly do I mean when I say that the R interpreter is extremely literal? Well, in the Navigating RStudio chapter I already told you that R is a case sensitive language. Again, that means that upper case x (X) and lower-case x (x) are different things to R. So, if you assign 2 to lower case x (x &lt;- 2). And then later ask R to tell what number you stored in upper case X; you will get an error (Error: object 'X' not found). x &lt;- 2 X ## Error in eval(expr, envir, enclos): object &#39;X&#39; not found Specifically, this is an example of a logic error. Meaning, R understands what you are asking it to do ‚Äì you want it to print the contents of the upper-case X object to the screen. However, it can‚Äôt complete your request because you are asking it to do something that doesn‚Äôt logically make sense ‚Äì print the contents of a thing that doesn‚Äôt exist. Remember, R is literal, and it will not try to guess that you actually meant to ask it to print the contents of lower-case x. Another general type of error is known as a syntax error. In programming languages, syntax refers to the rules of the language. You can sort of think of this as the grammar of the language. In English, I could say something like, ‚Äúgiving dog water drink.‚Äù This sentence is grammatically completely incorrect; however, most of you would roughly be able to figure out what I‚Äôm asking you to do based on your life experience and knowledge of the situational context. The R interpreter, as awesome as it is, would not be able to make an assumption about what I want it to do. There would either be one, and only one, preprogrammed correct response to such a request, or the R interpreter would say, ‚ÄúI don‚Äôt know what you‚Äôre asking me to do.‚Äù When the R interpreter says, ‚ÄúI don‚Äôt know what you‚Äôre asking me to do,‚Äù you‚Äôve made a syntax error. Throughout the rest of the book, I will try to point out situations where R programmers often encounter errors and how you may be able to address them. The remainder of this chapter will discuss some key components of R‚Äôs syntax and the data structures (i.e., ways of storing data) that the R syntax interacts with. 4.4 Functions R is a functional programming language, which simply means that functions play a central role in the R language. But what are functions? Well, factories are a common analogy used to represent functions. In this analogy, arguments are raw material inputs that go into the factory. For example, steel and rubber. The function is the factory where all the work takes place ‚Äì converting raw materials into the desired output. Finally, the factory output represents the returned results. In this case, bicycles. Figure 4.1: A factory making bicycles. To make this concept more concrete, in the Navigating RStudio chapter we used the seq() function as a factory. Specifically, we wrote seq(from = 2, to = 100, by = 2). The inputs (arguments) were from, to, and by. The output (returned result) was a set of numbers that went from 2 to 100 by 2‚Äôs. Most functions, like the seq() function, will be a word or word part followed by parentheses. Other examples are the sum() function for addition and the mean() function to calculate the average value of a set of numbers. Figure 4.2: A function factory making numbers. 4.5 Objects In addition to functions, the R programming language also includes objects. In the Navigating RStudio chapter we created an object called x with a value of 2 using the x &lt;- 2 R code. In general, you can think of objects as anything that lives in your R global environment. Objects may be single variables (also called vectors in R) or entire data sets (also called data frames in R). Objects can be a confusing concept at first. I think it‚Äôs because it‚Äôs is hard to precisely define exactly what an object is. I‚Äôll say two things about this. First, you‚Äôre probably overthinking it. When we use R, we create and save stuff. We have to call that stuff something in order to talk about it or write books about it. Somebody decided we would call that stuff ‚Äúobjects.‚Äù The second thing I‚Äôll say is that this becomes much less abstract when we finally get to a place where you can really get your hands dirty doing some R programming. Figure 4.3: Creating the x object. Sometimes it can be useful to relate the R language to English grammar. That is, when you are writing R code you can roughly think of functions as verbs and objects as nouns. Just like nouns are things in the English language, and verbs do things in the English language, objects are things and functions do things in the R language. So, in the x &lt;- 2 command x is the object and &lt;- is the function. ‚ÄúWait! Didn‚Äôt you just tell us that functions will be a word followed by parentheses?‚Äù Fair question. Technically, I said, ‚ÄúMost functions will be a word, or word part, followed by parentheses.‚Äù Just like English, R has exceptions. All operators in R are also functions. Operators are symbols like +, -, =, and &lt;-. There are many more operators, but you will notice that they all do things. In this case, they add, subtract, and assign values to objects. 4.6 Comments And finally, there are comments. If our R code is a conversation we are having with the R interpreter, then comments are your inner thoughts taking place during the conversation. Comments don‚Äôt actually mean anything to R, but they will be extremely important for you. You actually already saw a couple examples of comments above. # Store the value 2 in the variable x x &lt;- 2 # Print the contents of x to the screen x ## [1] 2 In this code chunk, ‚Äú# Store the value 2 in the variable x‚Äù and ‚Äú# Print the contents of x to the screen‚Äù are both examples of comments. Notice that they both start with the pound or hash sign (#). The R interpreter will ignore anything on the current line that comes after the hash sign. A carriage return (new line) ends the comment. However, comments don‚Äôt have to be written on their own line. They can also be written on the same line as R code as long as put them after the R code, like this: x &lt;- 2 # Store the value 2 in the variable x x # Print the contents of x to the screen ## [1] 2 Most beginning R programmers underestimate the importance of comments. In the silly little examples above, the comments are not that useful. However, comments will become extremely important as you begin writing more complex programs. When working on projects, you will often need to share your programs with others. Reading R code without any context is really challenging ‚Äì even for experienced R programmers. Additionally, even if your collaborators can surmise what your R code is doing, they may have no idea why you are doing it. Therefore, your comments should tell others what your code does (if it isn‚Äôt completely obvious), and more importantly, what your code is trying to accomplish. Even if you aren‚Äôt sharing your code with others, you may need to come back and revise or reuse your code months or years down the line. You may be shocked at how foreign the code you wrote will seem months or years after you wrote it. Therefore, comments are just important for others, they are important for future you! 4.7 Packages In addition to being a functional programming language, R is also a type of programming language called an open source programming language. For our purposes, this has two big advantages. First, it means that R is FREE! Second, it means that smart people all around the world get to develop new packages for the R language that can do cutting edge and/or very niche things. That second advantage is probably really confusing if this is not a concept you are already familiar with. For example, when you install Microsoft Word on your computer all the code that makes that program work is owned and Maintained by the Microsoft corporation. If you need Word to do something that it doesn‚Äôt currently do, your only option is really to make a feature request on Microsoft‚Äôs website. R works a little differently. When you downloaded R from the CRAN website, you actually download something called Base R. Base R maintained by the R Core Team. However, anybody ‚Äì even you ‚Äì can write your own code (called packages) that add new functions to the R syntax. Like all functions, these new functions allow you to do things that you can‚Äôt do (or can‚Äôt do as easily) with Base R. An analogy that I really like here is used by Ismay and Kim in ModernDive. A good analogy for R packages is they are like apps you can download onto a mobile phone. So R is like a new mobile phone: while it has a certain amount of features when you use it for the first time, it doesn‚Äôt have everything. R packages are like the apps you can download onto your phone from Apple‚Äôs App Store or Android‚Äôs Google Play. 1 So, when you get a new smart phone it comes with apps for making phone calls, checking email, and sending text messages. But, what if you want to listen to music on Spotify? You may or may not be able to do that through your phone‚Äôs web browser, but it‚Äôs way more convenient and powerful to download and install the Spotify app. In this course, we will make extensive use of packages developed by people and teams outside of the R Core Team. In particular, we will use a number of related packages that are collectively known as the Tidyverse. One of the most popular packages in the tidyverse collection (and one of the most popular R packages overall) is called the dplyr package for data management. In the same way that you have to download and install Spotify on your mobile phone before you can use it, you have to download and install new R packages on your computer before you can use the functions they contain. Fortunately, R makes this really easy. For most packages, all you have to do is run the install.packages() function in the R console. For example, here is how you would install the dplyr package. # Make sure you remember to wrap the name of the package in single or double quotes. install.packages(&quot;dplyr&quot;) Over time, you will download and install a lot of different packages. All those packages with all of those new functions start to create a lot of overhead. Therefore, R doesn‚Äôt keep them loaded and available for use at all times. Instead, every time you open RStudio, you will have to explicitly tell R which packages you want to use. So, when you close RStudio and open it again, the only functions that you will be able to use are Base R functions. If you want to use functions from any other package (e.g., dplyr) you will have to tell R that you want to do so using the library() function. # No quotes needed here library(dplyr) Technically, loading the package with the library() function is not the only way to use a function from a package you‚Äôve downloaded. For example, the dplyr package contains a function called filter() that helps us keep or drop certain rows in a data frame. To use this function, we have to first download the dplyr package. Then we can use the filter function in one of two different ways. library(dplyr) filter(states_data, state == &quot;Texas&quot;) # Keeps only the rows from Texas The first way you already saw above. Load all the functions contained in the dplyr package using the library() function. Then use that function just like any other Base R function. The second way is something called the double colon syntax. To use the double colon syntax, you type the package name, two colons, and the name of the function you want to use from the package. Here is an example of the double colon syntax. dplyr::filter(states_data, state == &quot;Texas&quot;) # Keeps only the rows from Texas Most of the time you will load packages using the library() function. However, I wanted to show you the double colon syntax because you may come across it when you are reading R documentation and because there are times when it makes sense to use this syntax. 4.8 Programming style Finally, I want to discuss programming style. R can read any code you write as long as your write it using valid R syntax. However, R code can be much easier or harder for people (including you) to read depending on how it‚Äôs written. The coding best practices chapter of this book gives complete details on writing R code that is as easy as possible for people to read. So, please make sure to read it. It will make things so much easier for all of us! References "],
["lets-get-programming.html", "5 Let‚Äôs get programming 5.1 Simulating data 5.2 Vectors 5.3 Data frames 5.4 Missing data 5.5 Our first analysis 5.6 Some common errors 5.7 Summary", " 5 Let‚Äôs get programming In this chapter, we are going to tie together many of the concepts we‚Äôve learned so far, and you are going to create your first basic R program. Specifically, you are going to write a program that simulates some data and analyzes it. 5.1 Simulating data Data simulation can be really complicated, but it doesn‚Äôt have to be. It is simply the process of creating data as opposed to finding data in the wild. This can be really useful in several different ways. Simulating data is really useful for getting help with a problem you are trying to solve. Often, it isn‚Äôt feasible for you to send other people the actual data set you are working on when you encounter a problem you need help with. Sometimes, it may not even be legally allowed (i.e., for privacy reasons). Instead of sending them your entire data set, you can simulate a little data set that recreates the challenge you are trying to address without all the other complexity of the full data set. As a bonus, I have often found that I end up figuring out the solution to the problem I‚Äôm trying to solve as I recreate the problem in a simulated data set that I intended to share with others. Simulated data can also be useful for learning about and testing statistical assumptions. In epidemiology, we use statistics to draw conclusions about populations of people we are interested in based on samples of people drawn from the population. Because we don‚Äôt actually have data from all the people in the population, we have to make some assumptions about the population based on what we find in our sample. When we simulate data, we know the truth about our population because we created our population to have that truth. We can then use this simulated populations to play ‚Äúwhat if‚Äù games with our analysis. What if we only sampled half as many people? What if their heights aren‚Äôt actually normally distributed? What if we used a probit model instead of a logit model? Going through this process and answering these questions can help us understand how much, and under what circumstances, we can trust the answers we found in the real world. So, let‚Äôs go ahead and write a complete R program to simulate and analyze some data. As I said, it doesn‚Äôt have to be complicated. In fact, in just a few lines of R code below we simulate and analyze some data about a hypothetical class. class &lt;- data.frame( names = c(&quot;John&quot;, &quot;Sally&quot;, &quot;Brad&quot;, &quot;Anne&quot;), heights = c(68, 63, 71, 72) ) class ## names heights ## 1 John 68 ## 2 Sally 63 ## 3 Brad 71 ## 4 Anne 72 mean(class$heights) ## [1] 68.5 As you can see, this data frame contains the students‚Äô names and heights. We also use the mean() function to calculate the average height of the class. By the end of this chapter, you will understand all the elements of this R code and how to simulate your own data. 5.2 Vectors Vectors are the most fundamental data structure in R. Here, data structure means ‚Äúcontainer for our data.‚Äù There are other data structures as well; however, they are all built from vectors. That‚Äôs why I say vectors are the most fundamental data structure. Some of these other structures include matrices, lists, and data frames. In this book, we won‚Äôt use matrices or lists much at all, so you can forget about them for now. Instead, we will almost exclusively use data frames to hold and manipulate our data. However, because data frames are built from vectors, it can be useful to start by learning a little bit about them. Let‚Äôs create our first vector now. # Create an example vector names &lt;- c(&quot;John&quot;, &quot;Sally&quot;, &quot;Brad&quot;, &quot;Anne&quot;) # Print contents to the screen names ## [1] &quot;John&quot; &quot;Sally&quot; &quot;Brad&quot; &quot;Anne&quot; üëÜHere‚Äôs what we did above: We created a vector of names with the c() (short for combine) function. The vector contains four values: ‚ÄúJohn‚Äù, ‚ÄúSally‚Äù, ‚ÄúBrad‚Äù, and ‚ÄúAnne‚Äù. All of the values are character strings (i.e., words). We know this because all of the values are wrapped with quotation marks. Here we used double quotes above, but we could have also used single quotes. We cannot, however, mix double and single quotes for each character string. For example, c(\"John', ...) won‚Äôt work. We assigned that vector of character strings to the word names using the &lt;- function. R now recognizes names as an object that we can do things with. R programmers may refer to the names object as ‚Äúthe names object‚Äù, ‚Äúthe names vector‚Äù, or ‚Äúthe names variable‚Äù. For our purposes, these all mean the same thing. We printed the contents of the names object to the screen by typing the word ‚Äúnames‚Äù. R returns (shows us) the four character values (‚ÄúJohn‚Äù ‚ÄúSally‚Äù ‚ÄúBrad‚Äù ‚ÄúAnne‚Äù) on the computer screen. Try copying and pasting the code above into the RStudio console on your computer. You should notice the names vector appear in your global environment. You may also notice that the global environment pane gives you some additional information about this vector to the right of its name. Specifically, you should see chr [1:4] \"John\" \"Sally\" \"Brad\" \"Anne\". This is R telling us that names is a character vector (chr), with four values ([1:4]), and the first four values are \"John\" \"Sally\" \"Brad\" \"Anne\". 5.2.1 Vector types There are several different vector types, but each vector can have only one type. The type of the vector above was character. We can validate that with the typeof() function like so: typeof(names) ## [1] &quot;character&quot; The other vector types that we will use in this book are double, integer, and logical. Double vectors hold real numbers and integer vectors hold integers. Collectively, double vectors and integer vectors are known as numeric vectors. Logical vectors can only hold the values TRUE and FALSE. Here are some examples of each: 5.2.2 Double vectors # A numeric vector my_numbers &lt;- c(12.5, 13.98765, pi) my_numbers ## [1] 12.500000 13.987650 3.141593 typeof(my_numbers) ## [1] &quot;double&quot; 5.2.3 Integer vectors Creating integer vectors involves a weird little quirk of the R language. For some reason, and I have no idea why, we must type an ‚ÄúL‚Äù behind the number to make it an integer. # An integer vector - first attempt my_ints_1 &lt;- c(1, 2, 3) my_ints_1 ## [1] 1 2 3 typeof(my_ints_1) ## [1] &quot;double&quot; # An integer vector - second attempt # Must put &quot;L&quot; behind the number to make it an integer. No idea why they chose &quot;L&quot;. my_ints_2 &lt;- c(1L, 2L, 3L) my_ints_2 ## [1] 1 2 3 typeof(my_ints_2) ## [1] &quot;integer&quot; 5.2.4 Logical vectors # An integer vector # Type TRUE and FALSE in all caps my_logical &lt;- c(TRUE, FALSE, TRUE) my_logical ## [1] TRUE FALSE TRUE typeof(my_logical) ## [1] &quot;logical&quot; Rather than have an abstract discussion about the particulars of each of these vector types right now, I think it‚Äôs best to wait and learn more about them when they naturally arise in the context of a real challenge we are trying to solve with data. At this point, just having some vague idea that they exist is good enough. 5.3 Data frames Vectors are useful for storing a single characteristic where all the data is of the same type. However, in epidemiology, we typically want to store information about many different characteristics of whatever we happen to be studying. For example, we didn‚Äôt just want the names of the people in our class, we also wanted the heights. Of course, we can also store the heights in a vector like so: heights &lt;- c(68, 63, 71, 72) heights ## [1] 68 63 71 72 But this vector, in and of itself, doesn‚Äôt tell us which height goes with which person. When we want to create relationships between our vectors, we can use them to build a data frame. For example: # Create a vector of names names &lt;- c(&quot;John&quot;, &quot;Sally&quot;, &quot;Brad&quot;, &quot;Anne&quot;) # Create a vector of heights heights &lt;- c(68, 63, 71, 72) # Combine them into a data frame class &lt;- data.frame(names, heights) # Print the data frame to the screen class ## names heights ## 1 John 68 ## 2 Sally 63 ## 3 Brad 71 ## 4 Anne 72 üëÜHere‚Äôs what we did above: We created a data frame with the data.frame() function. The first argument we passed to the data.frame() function was a vector of names that we previously created. The second argument we passed to the data.frame() function was a vector of heights that we previously created. We assigned that data frame to the word class using the &lt;- function. R now recognizes class as an object that we can do things with. R programmers may refer to the names object as ‚Äúthe class object‚Äù or ‚Äúthe class data frame‚Äù. For our purposes, these all mean the same thing. We could also call it a data set, but that term isn‚Äôt used much in R circles. We printed the contents of the class object to the screen by typing the word ‚Äúclass‚Äù. R returns (shows us) the data frame on the computer screen. Try copying and pasting the code above into the RStudio console on your computer. You should notice the class data frame appear in your global environment. You may also notice that the global environment pane gives you some additional information about this data frame to the right of its name. Specifically, you should see 4 obs. of 2 variables. This is R telling us that class has four rows or observations (4 obs.) and two columns or variables (2 variables). If you click the little blue arrow to the left of the data frame‚Äôs name, you will see information about the individual vectors that make up the data frame. As a shortcut, instead of creating individual vectors and then combining them into a data frame as we‚Äôve done above, most R programmers will create the vectors (columns) directly inside of the data frame function like this: # Create the class data frame class &lt;- data.frame( names = c(&quot;John&quot;, &quot;Sally&quot;, &quot;Brad&quot;, &quot;Anne&quot;), heights = c(68, 63, 71, 72) ) # Closing parenthesis down here. # Print the data frame to the screen class ## names heights ## 1 John 68 ## 2 Sally 63 ## 3 Brad 71 ## 4 Anne 72 As you can see, both methods produce the exact same result. The second method, however, requires a little less typing and results in fewer objects cluttering up your global environment. What I mean by that is that the names and heights vectors won‚Äôt exist independently in your global environment. Rather, they will only exist as columns of the class data frame. You may have also noticed that when we created the names and heights vectors (columns) directly inside of the data.frame() function we used the equal sign (=) to assign values instead of the assignment arrow (&lt;-). This is just one of those quirky R exceptions we talked about in the chapter on speaking R‚Äôs language. In fact, = and &lt;- can be used interchangeably in R. It is only by convention that we usually use &lt;- for assigning values, but use = for assigning values to columns in data frames. I don‚Äôt know why this is the convention. If it were up to me, we wouldn‚Äôt do this. We would just pick = or &lt;- and use it in all cases where we want to assign values. But, it isn‚Äôt up to me and I gave up on trying to fight it a long time ago. Your R programming life will be easier if you just learn to assign values this way ‚Äì even if it‚Äôs dumb. ü§∑ ‚ö†Ô∏èWarning: By definition, all columns in a data frame must have the same length (i.e., number of rows). That means that each vector you create when building your data frame must have the same number of values in it. For example, the class data frame above has four names and four heights. If we had only entered three heights, we would have gotten the following error: Error in data.frame(names = c(\"John\", \"Sally\", \"Brad\", \"Anne\"), heights = c(68, : arguments imply differing number of rows: 4, 3 5.4 Missing data As indicated in the warning above, all columns in our data frames have to have the same length. So what do we do when we are truly missing information in some of our observations? For example, how do we create the class data frame if we are missing Anne‚Äôs height for some reason? In R, we represent missing data with an NA. For example: # Create the class data frame data.frame( names = c(&quot;John&quot;, &quot;Sally&quot;, &quot;Brad&quot;, &quot;Anne&quot;), heights = c(68, 63, 71, NA) # Now we are missing Anne&#39;s height ) ## names heights ## 1 John 68 ## 2 Sally 63 ## 3 Brad 71 ## 4 Anne NA ‚ö†Ô∏èWarning: Make sure you capitalize NA and don‚Äôt use any spaces or quotation marks. Also, make sure you use NA instead of writing \"Missing\" or something like that. By default, R considers NA to be a logical-type value (as opposed to character or numeric). for example: typeof(NA) ## [1] &quot;logical&quot; However, you can tell R to make NA a different type by using one of the more specific forms of NA. For example: typeof(NA_character_) ## [1] &quot;character&quot; typeof(NA_integer_) ## [1] &quot;integer&quot; typeof(NA_real_) ## [1] &quot;double&quot; Most of the time, you won‚Äôt have to worry about doing this because R will take care of converting NA for you. What do I mean by that? Well, remember that every vector can have only one type. So, when you add an NA (logical by default) to a vector with double values as we did above (i.e., c(68, 63, 71, NA)), that would cause you to have three double values and one logical value in the same vector, which is not allowed. Therefore, R will automatically convert the NA to NA_real_ for you behind the scenes. This is a concept known as ‚Äútype coercion‚Äù and you can read more about it here if you are interested. As I said, most of the time you don‚Äôt have to worry about type coercion ‚Äì it will happen automatically. But, sometimes it doesn‚Äôt and it will cause R to give you an error. I mostly encounter this when using the if_else() and case_when() functions, which we will discuss later. 5.5 Our first analysis Congratulations on your new R programming skills. üéâ You can now create vectors and data frames. This is no small thing. Basically, everything else we do in this book will start with vectors and data frames. Having said that, just creating data frames may not seem super exciting. So, let‚Äôs round out this chapter with a basic descriptive analysis of the data we simulated. Specifically, let‚Äôs find the average height of the class. You will find that in R there are almost always many different ways to accomplish a given task. Sometimes, choosing one over another is simply a matter of preference. Other times, one method is clearly more efficient and/or accurate than another. This is a point that will come up over and over in this book. Let‚Äôs use our desire to find the mean height of the class as an example. 5.5.1 Manual calculation of the mean For starters, we can add up all the heights and divide by the total number of heights to find the mean. (68 + 63 + 71 + 72) / 4 ## [1] 68.5 üëÜHere‚Äôs what we did above: We used the addition operator (+) to add up all the heights. We used the division operator (/) to divide the sum of all the heights by 4 - the number of individual heights we added together. We used parentheses to enforce the correct order of operations (i.e., make R do addition before division). This works, but why might it not be the best approach? Well, for starters, manually typing in the heights is error prone. We can easily accidently press the wrong key. Luckily, we already have the heights stored as a column in the class data frame. We can access or refer to a single column in a data frame using the dollar sign notation. 5.5.2 Dollar sign notation class$heights ## [1] 68 63 71 72 üëÜHere‚Äôs what we did above: We used the dollar sign notation to access the heights column in the class data frame. Dollar sign notation is just the data frame name, followed by the dollar sign, followed by the column name. 5.5.3 Bracket notation Further, we can use bracket notation to access each value in a vector. I think it‚Äôs easier to demonstrate bracket notation than it is to describe it. For example, we could access the third value in the names vector like this: # Create the heights vector heights &lt;- c(68, 63, 71, 72) # Bracket notation # Access the third element in the heights vector with bracket notation heights[3] ## [1] 71 Remember, that data frame columns are also vectors. So, we can combine the dollar sign notation and bracket notation, to access each individual value of the height column in the class data frame. This will help us get around the problem of typing each individual height value. For example: # First way to calculate the mean # (68 + 63 + 71 + 72) / 4 # Second way. Use dollar sign notation and bracket notation so that we don&#39;t # have to type individual heights (class$heights[1] + class$heights[2] + class$heights[3] + class$heights[4]) / 4 ## [1] 68.5 5.5.4 The sum function The second method is better in the sense that we no longer have to worry about mistyping the heights. However, who wants to type class$heights[...] over and over? What if we had a hundred numbers? What if we had a thousand numbers? This wouldn‚Äôt work. Luckily, there is a function that adds all the numbers contained in a numeric vector ‚Äì the sum() function. Let‚Äôs take a look: # Create the heights vector heights &lt;- c(68, 63, 71, 72) # Add together all the individual heights with the sum function sum(heights) ## [1] 274 Remember, that data frame columns are also vectors. So, we can combine the dollar sign notation and sum() function, to add up all the individual heights in the heights column of the class data frame. It looks like this: # First way to calculate the mean # (68 + 63 + 71 + 72) / 4 # Second way. Use dollar sign notation and bracket notation so that we don&#39;t # have to type individual heights # (class$heights[1] + class$heights[2] + class$heights[3] + class$heights[4]) / 4 # Third way. Use dollar sign notation and sum function so that we don&#39;t have # to type as much sum(class$heights) / 4 ## [1] 68.5 üëÜHere‚Äôs what we did above: We passed the numeric vector heights from the class data frame to the sum() function using dollar sign notation. The sum() function returned the total value of all the heights added together. We divided the total value of the heights by four ‚Äì the number of individual heights. 5.5.5 Nesting functions !! Before we move on, I want to point out something that is actually kind of a big deal. In the third method above, we didn‚Äôt manually add up all the individual heights - R did this calculation for us. Further, we didn‚Äôt store the sum of the individual heights somewhere and then divide that stored value by 4. Heck, we didn‚Äôt even see what the sum of the individual heights were. Instead, the returned value from the sum function (274) was used directly in the next calculation (/ 4) by R without us seeing the result. In other words, (68 + 63 + 71 + 72) / 4, 274 / 4, and sum(class$heights) / 4 are all exactly the same thing to R. However, the third method (sum(class$heights) / 4) is much more scalable (i.e., adding a lot more numbers doesn‚Äôt make this any harder to do) and much less error prone. Just to be clear, the BIG DEAL is that we now know that the values returned by functions can be directly passed to other functions in exactly the same way as if we typed the values ourselves. This concept, functions passing values to other functions is known as nesting functions. It‚Äôs called nesting functions because we can put functions inside of other functions. ‚ÄúBut, Brad, there‚Äôs only one function in the command sum(class$heights) / 4 ‚Äì the sum() function.‚Äù Really? Is there? Remember when I said that operators are also functions in R? Well, the division operator is a function. And, like all functions it can be written with parentheses like this: # Writing the division operator as a function with parentheses `/`(8, 4) ## [1] 2 üëÜHere‚Äôs what we did above: We wrote the division operator in its more function-looking form. Because the division operator isn‚Äôt a letter, we had to wrap it in backticks (`). The backtick key is on the top left corner of your keyboard near the escape key (esc). The first agurment we passed to the division function was the dividend (The number we want to divide). The second argument we passed to the division function was the divisor (The number we want to divide by). So, the following two commands mean exactly the same thing to R: 8 / 4 `/`(8, 4) And if we use this second form of the division operator, we can clearly see that one function is nested inside another function. `/`(sum(class$heights), 4) ## [1] 68.5 üëÜHere‚Äôs what we did above: We calculated the mean height of the class. The first argument we passed to the division function was the returned value from the sum() function. The second argument we passed to the division function was the divisor (4). This is kind of mind-blowing stuff the first time you encounter it. ü§Ø I wouldn‚Äôt blame you if you are feeling overwhelmed or confused. The main points to take away from this section are: Everything we do in R, we will do with functions. Even operators are functions, and they can be written in a form that looks function-like; however, we will almost never actually write them in that way. Functions can be nested. This is huge because it allows us to directly pass returned values to other functions. Nesting functions in this way allows us to do very complex operations in a scalable way and without storing a bunch of unneeded values that are created in the intermediate steps of the operation. The downside of nesting functions is that it can make our code difficult to read - especially when we nest many functions. Fortunately, we will learn to use the pipe operator (%&gt;%) in the workflow basics part of this book. Once you get used to pipes, they will make nested functions much easier to read. Now, let‚Äôs get back to our analysis‚Ä¶ 5.5.6 The length function I think most of us would agree that the third method we learned for calculating the mean height is preferable to the first two methods for most situations. However, the third method still requires us to know how many individual heights are in the heights column (i.e., 4). Luckily, there is a function that tells us how many individual values are contained in a vector ‚Äì the length() function. Let‚Äôs take a look: # Create the heights vector heights &lt;- c(68, 63, 71, 72) # Return the number of individual values in heights length(heights) ## [1] 4 Remember, that data frame columns are also vectors. So, we can combine the dollar sign notation and length() function to automatically calculate the number of values in the heights column of the class data frame. It looks like this: # First way to calculate the mean # (68 + 63 + 71 + 72) / 4 # Second way. Use dollar sign notation and bracket notation so that we don&#39;t # have to type individual heights # (class$heights[1] + class$heights[2] + class$heights[3] + class$heights[4]) / 4 # Third way. Use dollar sign notation and sum function so that we don&#39;t have # to type as much # sum(class$heights) / 4 # Fourth way. Use dollar sign notation with the sum function and the length # function sum(class$heights) / length(class$heights) ## [1] 68.5 üëÜHere‚Äôs what we did above: We passed the numeric vector heights from the class data frame to the sum() function using dollar sign notation. The sum() function returned the total value of all the heights added together. We passed the numeric vector heights from the class data frame to the length() function using dollar sign notation. The length() function returned the total number of values in the heights column. We divided the total value of the heights by the total number of values in the heights column. 5.5.7 The mean function The fourth method above is definitely the best method yet. However, this need to find the mean value of a numeric vector is so common that someone had the sense to create a function that takes care of all the above steps for us ‚Äì the mean() function. And as you probably saw coming, we can use the mean function like so: # First way to calculate the mean # (68 + 63 + 71 + 72) / 4 # Second way. Use dollar sign notation and bracket notation so that we don&#39;t # have to type individual heights # (class$heights[1] + class$heights[2] + class$heights[3] + class$heights[4]) / 4 # Third way. Use dollar sign notation and sum function so that we don&#39;t have # to type as much # sum(class$heights) / 4 # Fourth way. Use dollar sign notation with the sum function and the length # function # sum(class$heights) / length(class$heights) # Fifth way. Use dollar sign notation with the mean function mean(class$heights) ## [1] 68.5 Congratulations again! You completed your first analysis using R! 5.6 Some common errors Before we move on, I want to briefly discuss a couple common errors that will frustrate many of you early in your R journey. You may have noticed that I went out of my way to differentiate between the heights vector and the heights column in the class data frame. As annoying as that may have been, I did it for a reason. The heights vector and the heights column in the class data frame are two separate things to the R interpreter, and you have to be very specific about which one you are referring to. To make this more concrete, let‚Äôs add a weight column to our class data frame. class$weight &lt;- c(160, 170, 180, 190) üëÜHere‚Äôs what we did above: We created a new column in our data frame ‚Äì weight ‚Äì using dollar sign notation. Now, let‚Äôs find the mean weight of the students in our class. mean(weight) ## Error in mean(weight): object &#39;weight&#39; not found Uh, oh! What happened? Why is R saying that weight doesn‚Äôt exist? We clearly created it above, right? Wrong. We didn‚Äôt create an object called weight in the code chunk above. We created a column called weight in the object called class in the code chunk above. Those are different things to R. If we want to get the mean of weight we have to tell R that weight is a column in class like so: mean(class$weight) ## [1] 175 A related issue can arise when you have an object and a column with the same name but different values. For example: # An object called scores scores &lt;- c(5, 9, 3) # A colummn in the class data frame called scores class$scores &lt;- c(95, 97, 93, 100) If you ask R for the mean of scores, R will give you an answer. mean(scores) ## [1] 5.666667 However, if you wanted the mean of the scores column in the class data frame, this won‚Äôt be the correct answer. Hopefully, you already know how to get the correct answer, which is: mean(class$scores) ## [1] 96.25 Again, the scores object and the scores column of the class object are different things to R. 5.7 Summary Wow! We covered a lot in this first part of the book on getting started with R and RStudio. Don‚Äôt feel bad if your head is swimming. It‚Äôs a lot to take-in. However, you should feel proud of the fact that you can already do some legitimately useful things with R. Namely, simulate and analyze data. In the next part of this book, we are going to discuss some tools and best practices that will make it easier and more efficient for you to write and share your R code. After that, we will move on to tackling more advanced programming and data analysis challenges. "],
["r-scripts.html", "6 R scripts 6.1 Creating R scripts", " 6 R scripts Up to this point, I‚Äôve only showed you how to submit your R code to R in the console. 6.1 Figure 6.1: Submitting R code in the console. Submitting code directly to the console in this way works well for quick little tasks and snippets of code. But, writing longer R programs this way has some drawbacks that are probably already obvious to you. Namely, your code isn‚Äôt saved anywhere. And, because it isn‚Äôt saved anywhere, you can‚Äôt modify it, use it again later, or share it with others. Technically, the statements above are not entirely true. When you submit code to the console, it is copied to RStudio‚Äôs History pane and from there you can save, modify, and share with others (see figure 6.2). But, this method is much less convenient, and provides you with far fewer whistles and bells than the other methods we‚Äôll discuss in this book. Figure 6.2: Console commands copied to the History pane. Those of you who have worked with other statistical programs before may be familiar with the idea of writing, modifying, saving, and sharing code scripts. SAS calls these code scripts ‚ÄúSAS programs‚Äù, Stata calls them ‚ÄúDO files‚Äù, and SPSS calls them ‚ÄúSPSS syntax files‚Äù. If you haven‚Äôt created code scripts before, don‚Äôt worry. There really isn‚Äôt much to it. In R, the most basic type of code script is simply called an R script. An R script is just a plain text file that contains R code and comments. R script files end with the file extension .R. Before I dive into giving you any more details about R scripts, I want to say that I‚Äôm actually going to discourage you from using them for most of what we do in this book. Instead, I‚Äôm going to encourage you to use R markdown files for the majority of your interactive coding, and for preparing your final products for end users. The next chapter is all about R markdown files. However, I‚Äôm starting with R scripts because: They are simpler than R markdown files, so they are a good place to start. Some of what I discuss below will also apply to R markdown files. R scripts are a better choice than R markdown files in some situations (e.g., writing R packages, creating Shiny apps). Some people just prefer using R scripts. With all that said, the screenshot below is of an example R script: Figure 6.3: Example R script. Click here to download the R script As you can see, I‚Äôve called out a couple key elements of the R script to discuss. 6.3 First, instead of just jumping into writing R code, lines 1-5 contain a header that I‚Äôve created with comments. Because I‚Äôve created it with comments, the R interpreter will ignore it. But, it will help other people you collaborate with (including future you) figure out what this script does. Therefore, I suggest that your header includes at least the following elements: A brief description of what the R script does. The author(s) who wrote the R script. Important dates. For example, the date it was originally created and the date it was last modified. You can usually get these dates from your computer‚Äôs operating system, but they aren‚Äôt always accurate. Second, you may notice that I also used comments to create something I‚Äôm calling decorations on lines 1, 5, and 17. Like all comments, they are ignored by the R interpreter. But, they help create visual separation between distinct sections of your R code, which makes your code easier for humans to read. I tend to use the equal sign (# ====) for separating major sections and the dash (# ----) for separating minor sections; although, ‚Äúmajor‚Äù and ‚Äúminor‚Äù are admittedly subjective. I haven‚Äôt explicitly highlighted it in the screenshot above, but it‚Äôs probably worth pointing out the use of line breaks (i.e., returns) in the code as well. This is much easier to read‚Ä¶ # Load packages library(dplyr) # Load data data(&quot;mtcars&quot;) # I&#39;m not sure what&#39;s in the mtcars data. I&#39;m printing it below to take a look mtcars ## Data analysis # ---------------------------------------------------------------------------- # Below, we calculate the average mpg across all cars in the mtcars data frame. mean(mtcars$mpg) # Here, we also plot mpg against displacement. plot(mtcars$mpg, mtcars$disp) than this‚Ä¶ # Load packages library(dplyr) # Load data data(&quot;mtcars&quot;) # I&#39;m not sure what&#39;s in the mtcars data. I&#39;m printing it below to take a look mtcars ## Data analysis # ---------------------------------------------------------------------------- # Below, we calculate the average mpg across all cars in the mtcars data frame. mean(mtcars$mpg) # Here, we also plot mpg against displacement. plot(mtcars$mpg, mtcars$disp) Third, it‚Äôs considered a best practice to keep each line of code to 80 characters (including spaces) or less. There‚Äôs a little box at the bottom left corner of your R script that will tell you what row your cursor is currently in and how many characters into that row your cursor is currently at (starting at 1, not 0). Figure 6.4: Cursor location. For example, 20:3 corresponds to having your cursor between the ‚Äúe‚Äù and the ‚Äúa‚Äù in mean(mtcars$mpg) in the example script above. 6.4 Fourth, it‚Äôs also considered a best practice to load any packages that your R code will use at the very top of your R script (lines 7 &amp; 8). 6.3 Doing so will make it much easier for others (including future you) to see what packages your R code needs to work properly right from the start. 6.1 Creating R scripts To create your own R scripts, click on the icon shown below 6.5 and you will get a dropdown box with a list of files you can create. 6.6 Figure 6.5: Click the new source file icon. Click the very first option ‚Äì R Script. Figure 6.6: New source file options. When you do, a new untitled R Script will appear in the source pane. Figure 6.7: A blank R script in the source pane. And that‚Äôs pretty much it. Everything else in figure 6.3 is just R code and comments about the R code. But, you can now easily save, modify, and share this code with others. In the next chapter, we are going to learn how to write R code in R markdown files, where we can add a ton of whistles and bells to this simple R script. "],
["r-markdown.html", "7 R markdown 7.1 What is R markdown? 7.2 Why use R markdown? 7.3 Create an R markdown file 7.4 YAML headers 7.5 R code chunks 7.6 Markdown", " 7 R markdown In the chapter on R Scripts, you learned how to create R scripts ‚Äì plain text files that contain R code and comments. These R scripts are kind of a big deal because they give us a simple and effective tool for saving, modifying, and sharing our R code. If it weren‚Äôt for the existence of R markdown files, we would probably do all of the coding in this book using R scripts. However, R markdown files do exist and they are AWESOME! So, I‚Äôm actually going to suggest that you use them instead of R scripts the majority of the time. It‚Äôs actually kind of difficult for me to describe what an R markdown file is if you‚Äôve never seen or heard of one before. Therefore, I‚Äôm going to start with an example and work backwards from there. Figure 7.1 below is an R markdown file. It includes the exact same R code and comments as the example we saw in the chapter on creating R scripts. 6.3 Figure 7.1: Example R markdown file. Click here to download the R markdown file Notice that the results are embedded directly in the R markdown file immediately below the R code (e.g., between lines 19 and 20)! Once rendered, this R markdown file creates the HTML file you see below in figure 7.2. HTML files are what websites are made out of, and I‚Äôll walk you through how to create them from R markdown files later in this chapter. Figure 7.2: Preview of HTML file created from an R markdown file. Click here to download the HTML Notebook file. Notice how everything is nicely formatted and easy to read! When you create R markdown files on your computer, the rendered HTML file is saved in the same folder by default. 7.3 Figure 7.3: HTML Notebook file and R markdown file on MacOS. In the figure above, the HTML Notebook file is highlighted in blue and ends with the .nb.html file extension. The R markdown file is below the HTML Notebook file and ends with the .Rmd file extension. Both of these files can be modified, saved, and shared with others. 7.1 What is R markdown? There is literally an entire book about R markdown (and it‚Äôs worth reading at some point). 2 Therefore, I‚Äôm only going to hit some of the highlights in this chapter. As a starting point, you can think of R markdown files as being a mix of R scripts, the R console, and a Microsoft Word or Google Doc document. I say this because: The R code that you would otherwise write in R scripts is written in R code chunks when you use R markdown files. In figure 7.1 there are R code chunks at lines 8 to 10, 12 to 14, 16 to 19, 25 to 27, and 31 to 33. Instead of having to flip back and forth between your source pane and your console (or viewer) pane in RStudio, the results from your R code are embedded directly in the R markdown file, right alongside the code that generated them. In figure 7.1 there are embedded results between lines 19 and 20, between lines 27 and 28, and between lines 33 and 34. When creating a document in Microsoft Word or Google Docs, you may format text headers to help organize your document, you may format your text to emphasize certain words, you may add tables to help organize concepts or data, you may add links to other resources, and you may add pictures or charts to help you clearly communicate ideas to yourself or others. Similarly, R markdown files allow you to surround your R code with formatted text, tables, links, pictures, and charts directly in your document. Even when I don‚Äôt share my R markdown files with anyone else, I find that the added functionality described above really helps me organize my data analysis more effectively, and helps me understand what I was doing if I come back to the analysis at some point in the future. But, R markdown really shines when I do want to share my analysis or results with others. To get an idea of what I‚Äôm talking about, please take a look at the R markdown gallery and view some of the amazing things you can do with R markdown. As you can see, these R markdown files mix R code with other kinds of text and images to create documents, websites, presentations, and more. 7.2 Why use R markdown? At this point, you may be thinking ‚ÄúOk, that R markdown gallery has some cool stuff, but this also looks complicated. Why shouldn‚Äôt I just use a basic R script for the little R program I‚Äôm writing?‚Äù If that‚Äôs what you‚Äôre thinking, you have a valid point. R markdown files are slightly more complicated than basic R scripts. However, after reading in the sections below, I think you will find that getting started with R markdown doesn‚Äôt have to be super complicated, and the benefits provided make the initial investment in learning R markdown worth your time. 7.3 Create an R markdown file RStudio makes it very easy to create your own R markdown file. The process is actually really similar to the process we used to create an R script. Start by clicking on the icon shown below. 7.4 Figure 7.4: Click the new source file icon. As before, you‚Äôll be presented with a dropdown box that lists a bunch of different file types that you can create. This time, we‚Äôll click R Notebook instead of R script. 7.5 Figure 7.5: New source file options. At this point you may have noticed that right below R Notebook in the dropdown menu is an option that says R Markdown... and you may be confused about why we aren‚Äôt choosing that option. Great observation! As I said at the beginning of this chapter, R markdown files have a ton of functionality. Usually, when things have a ton of functionality (e.g., phones, cars, computers) it comes at a cost. That cost is complexity. R markdown files are no exception. They are awesome, but it can take some time to learn how to take advantage of all they have to offer. R Notebooks ARE R markdown files, but they have some default settings that make it quick and easy for us to jump right into using them for doing some interactive R coding. üóíSide Note: When I say ‚Äúinteractive R coding‚Äù I mean, type some R code, submit, see the result, type some more R code, submit it, see the result‚Ä¶ After you click the R Notebook option in the dropdown menu, a new untitled R Notebook file will appear in the source pane. This R Notebook will even include some example text and code meant to help get you started. We are typically going to erase all the example stuff and write our own text and code, but for now I will use it to highlight some key components of R markdown files. 7.6 Figure 7.6: A blank R script in the source pane. First, notice lines 1 through 4 in the example above. These lines make up something called the YAML header (pronounced yamel). You don‚Äôt need to know what YAML means, but you do need to know that this is one of the defining features of all R markdown files. Essentially, The YAML header turns plain text files into R markdown files. We‚Äôll talk more about the details of the YAML header soon. Second, notice lines 10 through 12. These lines make up something called an R code chunk. Code chunks in R markdown files always start with three backticks ( ` ) and a pair of curly braces ({}), and they always end with three more backticks. We know that this code chunk contains R code because of the ‚Äúr‚Äù inside of the curly braces. You can also create code chunks that will run other languages (e.g., python), but we won‚Äôt do that in this book. In this book, we will exclusively use the R language. You can think of each R code chunk as a mini R script. We‚Äôll talk more about the details of code chunks soon. Third, notice lines 6, 8, 14, and 18. These lines contain text instructions to help you use R Notebooks, but in a real analysis you would use formatted text like this to add context around the analysis in the code chunks. For now, you can think of this as being very similar to the comments we wrote in our R scripts. However, this text is actually something called markdown, which allows us to do lots of cool things that the comments in our R scripts aren‚Äôt able to do. For example, line 6 has a link to a website embedded in it, and lines 8, 14, 16, and 18 all include text that is being formatted (the orange text wrapped in asterisks). In this case, the text is being italicized. And that is all you have to do to create an basic R Notebook. Next, I‚Äôm going to give you a few more details about each of the key components of the R Notebook that I briefly introduced above. 7.4 YAML headers As I said before, the YAML header is really what makes an R markdown file an R markdown file. The YAML header always begins and ends with dash-dash-dash (---) typed on its own line (1 &amp; 4 above). 7.6 The stuff written inside the YAML header generally falls into two categories: Stuff about the R markdown file itself. For example, the YAML header we saw above gives that R markdown file a title. The title is added to the file by adding the title keyword, followed by a colon (:), followed by a character string wrapped in quotes. Other examples include author and date. Stuff that tells R how to process the R markdown file. What do I mean by that? Well, remember the R markdown gallery you saw earlier? That gallery includes Word documents, PDF documents, websites, and more. But all of those different document types started as an R markdown file similar to the one in figure 7.6. R will create a PDF or a Word document or a website from the R markdown file based on the instructions you give it inside the YAML header. For example, the YAML header we saw above tells R to create an HTML Notebook from that R markdown file. This output type is selected by adding the output keyword, followed by a colon (:), followed by the html_notebook keyword. What does an HTML Notebook look like? Well, if you hit the Preview button in RStudio: Figure 7.7: RStudio‚Äôs preview button. Only visible when an R Notebook is open. R will ask you to save your R markdown file. After you save it, R will automatically create (or render) a new HTML Notebook file and save it in the same location where your R markdown file is saved. Additionally, a little browser window will pop up and give you a preview of what the rendered HTML Notebook looks like. 7.8 Figure 7.8: An HTML Notebook created using an R markdown file. Notice how all the formatting that was applied when R rendered the HTML Notebook file. For example, the title ‚Äì ‚ÄúR Notebook‚Äù ‚Äì is in big bold letters at the top of the screen, the words ‚ÄúR Markdown‚Äù in the first line of text are now a clickable link to another website, and the word ‚ÄúRun‚Äù in the second line of text is now italicized. I can imagine that this section may seem a little confusing to some of you right now. If so, don‚Äôt worry. You don‚Äôt really need to understand the YAML header right now. Remember, when you create a new R Notebook file in the manner I described above, the YAML header is already there. You will probably want to change the title, but that‚Äôs about it. 7.5 R code chunks As I said above, R code chunks always start out with three backticks ( ` ) and a pair of curly braces with an ‚Äúr‚Äù in them ({r}), and they always end with three more backticks. Typing that over and over can be tedious, so RStudio provides a keyboard shortcut for inserting R code chunks into your R markdown files. On Mac type option + command + i. On Windows type control + alt + i Inside your code chunk, you can type anything that you would otherwise type in the console or in an R script. You can then click the little green arrow in the top right corner of the code chunk to submit it to R and see the result. 7.9 Figure 7.9: The results of an R code chunk embedded in an R Notebook. 7.6 Markdown Many of you have probably heard of HTML and CSS before. HTML stands for hypertext markup language and CSS stands for cascading style sheets. Together, HTML and CSS are used to create and style every website you‚Äôve ever seen. Remember that R Notebooks created from our R markdown files are HTML files. They will open in any web browser and behave just like any other website. Therefore, you can manipulate and style them using HTML and CSS just like any other website. However, it takes a lot of time and effort to learn HTML and CSS. So, markdown was created as an easier-to-use alternative. Think of it as HTML and CSS lite. It can‚Äôt fully replace HTML and CSS, but it is much easier to learn, and you can use it to do many of the main things you would want to do with HTML and CSS. For example, in figures 7.6 and 7.8 you saw that wrapping your text with single asterisks (*) italicizes that text, and that using a combination of brackets and parentheses [Text](Link) can turn your text into a clickable link. There are a ton of other things you can do with markdown, and I recommend checking out RStudio‚Äôs R markdown cheat sheet if you‚Äôre interested in learning more. You can download it (any many other cheat sheets) here. The cheat sheet is a little bit busy and may feel overwhelming at first. So, I suggest starting with the section called ‚ÄúPandoc‚Äôs Markdown‚Äù on the second page of the cheat sheet. Just play around with some of the formatting options and get a feel for what they do. Having said that, it‚Äôs totally fine if you don‚Äôt care to try to tackle learning markdown syntax right now. You don‚Äôt really need markdown to follow along with the rest of the book. However, I still suggest using R Notebook files for writing, saving, modifying, and sharing your R code ‚Äì even if you don‚Äôt plan to format them with markdown syntax. References "],
["r-projects.html", "8 R projects", " 8 R projects In previous chapters of this book, you learned how to use R scripts and R markdown files to create, modify, save, and share your R code and results. In many real-world projects, you will end up creating multiple R scripts and/or R markdown files. You may even have other files (e.g., images or data) that you want to store alongside your R code files. Over time, keeping up with all of these files can become cumbersome. One tool for helping you organize and manage collections of files is to organize them with R project files. Because nothing we will do in this book really requires creating projects, I‚Äôm not going to discuss them further at this point. It would probably just end up confusing some of you unnecessarily. However, I wanted to mention that R project files exist in case you ever end up needing them someday for your real-world data analysis projects. I recommend that interested readers start with Grolemund and Wickham‚Äôs chapter on projects in R for Data Science. 3 References "],
["coding-best-practices.html", "9 Coding best practices 9.1 General principles 9.2 Code comments 9.3 Style guidelines", " 9 Coding best practices At this point in the book, we‚Äôve talked a little bit about what R is. We‚Äôve also talked about the RStudio IDE and took a quick tour around its four main panes. Finally, we wrote our first little R program, which simulated and analyzed some data about a hypothetical class. Writing and executing this R program officially made you an R programmer. üèÜ However, you should know that not all R code is equally ‚Äúgood‚Äù ‚Äì even when it‚Äôs equally valid. What do I mean that? Well, we already discussed the R interpreter and R syntax in the chapter on speaking R‚Äôs language. Any code that uses R syntax that the R interpreter can understand is valid R code. But, is the R interpreter the only one reading your R code? No way! In epidemiology, we collaborate with others all the time! That collaboration is going to be much more efficient and enjoyable when there is good communication ‚Äì including R code that is easy to read and understand. Further, you will often need to read and/or reuse code you wrote weeks, months, or years after you wrote it. You may be amazed at how quickly you forget what you did and/or why you did it that way. Therefore, in addition to writing valid R code, this chapter is about writing ‚Äúgood‚Äù R code ‚Äì code that easily and efficiently communicates ideas to humans. Of course, ‚Äúgood code‚Äù is inevitably somewhat subjective. Reasonable people can have a difference of opinion about the best way to write code that is easy to read and understand. Additionally, reasonable people can have a difference of opinion about when code is ‚Äúgood enough.‚Äù For these reasons, I‚Äôm going to offer several ‚Äúsuggestions‚Äù about writing good R code below, but only two general principles, which I believe most R programmers would agree with. 9.1 General principles Comment your code. Whether you intend to share your code with other people or not, make sure to write lots of comments about what you are trying to accomplish in each section of your code and why. Use a style consistently. I‚Äôm going to suggest several guidelines for styling your R code below, but you may find that you prefer to style your R code in a different way. Whether you adopt my suggested style or not, please find or create a style that works for you and your collaborators and use it consistently. 9.2 Code comments There isn‚Äôt a lot of specific advice that I can give here because comments are so idiosyncratic to the task at hand. So, I think the best I can do at this point is to offer a few examples for you to think about. 9.2.1 Defining key variables As we will discuss below, variables should have names that are concise, yet informative. However, the data you receive in the real world will not always include informative variable names. Even when someone has given the variables informative names, there may still be contextual information about the variables that is important to understand for data management and analysis. Some data sets will come with something called a codebook or data dictionary. These are text files that contain information about the data set that are intended to provide you with some of that more detailed information. For example, the survey questions that were used to capture the values in each variable or what category each value in a categorical variable represents. However, real data sets don‚Äôt always come with a data dictionary, and even when they do, it can be convenient to have some of that contextual information close at hand, right next to your code. Therefore, I will sometimes comment my code with information about variables that are important for the analysis at hand. Here is an example from an administrative data set I was using for an analysis: * **Case number definition** - Case / investigation number. * **Intake stage definition** - An ID number assigned to the Intake. Each Intake (Report) has its own number. A case may have more than one intake. For example, case # 12345 has two intakes associated with it, 9 days apart, each with their own ID number. Each of the two intakes associated with this case have multiple allegations. * **Intake start definition** - An intake is the submission or receipt of a report - a phone call or web-based. The Intake Start Date refers to the date the staff member opens a new record to begin recording the report. 9.2.2 What this code is trying to accomplish Sometimes, it is obvious what a section of code literally does. but not so obvious why you‚Äôre doing it. I often try to write some comments around my code about what it‚Äôs trying to ultimately accomplish and why. For example: ## Standardize character strings # Because we will merge this data with other data sets in the future based on # character strings (e.g., name), we need to go ahead and standardize their # formats here. This will prevent mismatches during the merges. Specifically, # we: # 1. Transform all characters to lower case # 2. Remove any special characters (e.g., hyphens, periods) # 3. Remove trailing spaces (e.g., &quot;John Smith &quot;) # 4. Remove double spaces (e.g., &quot;John Smith&quot;) vars &lt;- quos(full_name, first_name, middle_name, last_name, county, address, city) client_data &lt;- client_data %&gt;% mutate_at(vars(!!! vars), tolower) %&gt;% mutate_at(vars(!!! vars), stringr::str_replace_all, &quot;[^a-zA-Z\\\\d\\\\s]&quot;, &quot; &quot;) %&gt;% mutate_at(vars(!!! vars), stringr::str_replace, &quot;[[:blank:]]$&quot;, &quot;&quot;) %&gt;% mutate_at(vars(!!! vars), stringr::str_replace_all, &quot;[[:blank:]]{2,}&quot;, &quot; &quot;) rm(vars) 9.2.3 Why I chose this particular strategy In addition to writing comments about why I did something, I sometimes write comments about why I did it instead of something else. Doing this can save you from having to relearn lessons you‚Äôve already learned through trial and error but forgot. For example: ### Create exact match dummy variables * We reshape the data from long to wide to create these variables because it significantly decreases computation time compared to doing this as a group_by operation on the long data. 9.3 Style guidelines UsInG c_o_n_s_i_s_t_e_n_t STYLE i.s. import-ant! Good coding style is like using correct punctuation. You can manage without it, but it sure makes things easier to read. As with styles of punctuation, there are many possible variations‚Ä¶ Good style is important because while your code only has one author, it‚Äôll usually have multiple readers. This is especially true when you‚Äôre writing code with others. In that case, it‚Äôs a good idea to agree on a common style up-front. Since no style is strictly better than another, working with others may mean that you‚Äôll need to sacrifice some preferred aspects of your style. 4 Below, I outline the style that I and my collaborators typically use when writing R code for a research project. It generally follows the Tidyverse style guide, which I strongly suggest you read. Outside of my class, you don‚Äôt have to use my style, but you really should find or create a style that works for you and your collaborators and use it consistently. 9.3.1 Object (variable) names In addition to the object naming guidance given in the Tidyverse style guide, I suggest the following object naming conventions. 9.3.2 Use names that are informative Using names that are informative and easy to remember will make life easier for everyone who uses your data ‚Äì including you! # Uninformative names - Don&#39;t do this x1 var1 # Informative names employed married education 9.3.2.1 Use names that are concise You want names to be informative, but you don‚Äôt want them to be overly verbose. Really long names create more work for you and more opportunities for typos. In fact, I recommend using a single word when you can. # Write out entire name of the study the data comes from - Don&#39;t do this womens_health_initiative # Write out an acronym for the study the data comes from - assuming everyone # will be familiar with this acronym - Do this whi 9.3.2.2 Use all lowercase letters Remember, R is case-sensitive, which means that myStudyData and mystudydata are different things to R. Capitalizing letters in your file name just creates additional details to remember and potentially mess up. Just keep it simple and stick with lowercase letters. # All upper case - so aggressive - Don&#39;t use MYSTUDYDATA # Camel case - Don&#39;t use myStudyData # All lowercase - Use my_study_data 9.3.2.3 Separate multiple words with underscores. Sometimes you really just need to use multiple words to name your object. In those cases, I suggested separating words with an underscore. # Multiple words running together - Hard to read - Don&#39;t use mycancerdata # Camel case - easier to read, but more to remember and mess up - Don&#39;t use myCancerData # Separate with periods - easier to read, but doesn&#39;t translate well to many # other languages. For example, SAS won&#39;t accept variable names with # periods - Don&#39;t use my.cancer.data # Separate with underscores - Use my_cancer_data 9.3.2.4 Prefix the names of similar variables When you have multiple related variables, it‚Äôs good practice to start their variable names with the same word. It makes these related variables easier to find and work with in the future if we need to do something with all of them at once. We can sort our variable names alphabetically to easily find find them. Additionally, we can use variable selectors like starts_with(\"name\") to perform some operation on all of them at once. # Don&#39;t use first_name last_name middle_name # Use name_first name_last name_middle # Don&#39;t use street city state # Use address_street address_city address_state 9.3.3 File Names All the variable naming suggestons above also apply to file names. However, I make a few additional suggestions specific to file names below. 9.3.3.1 Managing multiple files in projects When you are doing data management and analysis for real-world projects you will typically need to break the code up into multiple files. If you don‚Äôt, the code often becomes really difficult to read and manage. Having said that, finding the code you are looking for when there are 10, 20, or more separate files isn‚Äôt much fun either. Therefore, I suggest the following (or similar) file naming conventions be used in your projects. Separate data cleaning and data analysis into separate files (typically, .R or .Rmd). Data cleaning files should be prefixed with the word ‚Äúdata‚Äù and named as follows data_[order number]_[purpose] # Examples data_01_import.Rmd data_02_clean.Rmd data_03_process_for_regression.Rmd Analysis files that do not directly create a table or figure should be prefixed with the word ‚Äúanalysis‚Äù and named as follows analysis_[order number]_[brief summary of content] # Examples analysis_01_exploratory.Rmd analysis_02_regression.Rmd Analysis files that DO directly create a table or figure should be prefixed with the word ‚Äútable‚Äù or ‚Äúfig‚Äù respectively and named as follows table_[brief summary of content] or fig_[brief summary of content] # Examples table_network_characteristics.Rmd fig_reporting_patterns.Rmd üóíSide Note: I sometimes do data manipulation (create variables, subset data, reshape data) in an analysis file if that analysis (or table or chart) is the only analysis that uses the modified data. Otherwise, I do the modifications in a separate data cleaning file. Images Should typically be exported as png (especially when they are intended for use HTML files). Should typically be saved in a separate ‚Äúimg‚Äù folder under the project home directory. Should be given a descriptive name. Example: histogram_heights.png, NOT fig_02.png. -I have found that the following image sizes typically work pretty well for my projects. 1920 x 1080 for HTML 770 x 360 for Word Word and PDF output files I typically save them in a separate ‚Äúdocs‚Äù folder under the project home directory Whenever possible, I try to set the Word or PDF file name to match the name of the R file that it was created in. Example: first_quarter_report.Rmd creates docs/first_quarter_report.pdf Exported data files (i.e., RDS, RData, CSV, Excel, etc.) I typically save them in a separate ‚Äúdata‚Äù folder under the project home directory. Whenever possible, I try to set the Word or PDF file name to match the name of the R file that it was created in. Example: data_03_texas_only.Rmd creates data/data_03_texas_only.csv References "],
["using-pipes.html", "10 Using pipes 10.1 What are pipes? 10.2 How do pipes work? 10.3 Final thought on pipes", " 10 Using pipes 10.1 What are pipes? ü§î What are pipes? This %&gt;% is the pipe operator. The pipe operator is not part of base R. So, you will need to install and load a package to use it. There is actually more than one package that you can use. I recommend that you install and load the dplyr package. You can install the dplyr package by copying and pasting the following command in your R console install.packages(\"dplyr\") You can load the dplyr package by copying and pasting the following command in your R console library(dplyr) ü§î What does the pipe operator do? In my opinion, the pipe operator makes your R code much easier to read and understand. ü§î How does it do that? It makes your R code easier to read and understand by allowing you to view your nested functions in the order you want them to execute, as opposed to viewing them literally nested inside of each other. You were first introduced to nesting functions in the Let‚Äôs get programming chapter. Recall that functions return values, and the R language allows us to directly pass those returned values into other functions for further calculations. We referred to this as nesting functions and said it was a big deal because it allows us to do very complex operations in a scalable way and without storing a bunch of unneeded values. In that chapter, we also discussed a potential downside of nesting functions. Namely, our R code can become really difficult to read when we start nesting lots of functions inside one another. Pipes allow us to retain the benefits of nesting functions without making our code really difficult to read. At this point, I think it‚Äôs best to show you an example. In the code below we want to generate a sequence of numbers, then we want to calculate the log of each of the numbers, and then find the mean of the logged values. # Performing an operation using a series of steps. my_numbers &lt;- seq(from = 2, to = 100, by = 2) my_numbers_logged &lt;- log(my_numbers) mean_my_numbers_logged &lt;- mean(my_numbers_logged) mean_my_numbers_logged ## [1] 3.662703 üëÜHere‚Äôs what we did above: We created a vector of numbers called my_numbers using the seq() function. Then we used the log() function to create a new vector of numbers called my_numbers_logged, which contains the log values of the numbers in my_numbers. Then we used the mean() function to create a new vector called mean_my_numbers_logged, which contains the mean of the log values in my_numbers_logged. Finally, we printed the value of mean_my_numbers_logged to the screen to view. The obvious first question here is, ‚Äúwhy would I ever want to do that?‚Äù Good question! You probably won‚Äôt ever want to do what we just did in the code chunk above, but we haven‚Äôt learned many functions for working with real data yet and I don‚Äôt want to distract you with a bunch of new functions right now. Instead, I want to demonstrate what pipes do. So, we‚Äôre stuck with this silly example. üëç What‚Äôs nice about the code above? I would argue that it is pretty easy to read because each line does one thing and it follows a series of steps in logical order ‚Äì create the numbers, log the numbers, get the mean. üëé What could be better about the code above? All we really wanted was the mean value of the logged numbers (i.e., mean_my_numbers_logged); however, on our way to getting mean_my_numbers_logged we also created two other objects that we don‚Äôt care about ‚Äì my_numbers and my_numbers_logged. It took us time to do the extra typing required to create those objects, and those objects are now cluttering up our global environment. It may not seem like that big of a deal here, but in a real data analysis project these things can really add up. Next, let‚Äôs try nesting these functions instead: # Performing an operation using nested functions. mean_my_numbers_logged &lt;- mean(log(seq(from = 2, to = 100, by = 2))) mean_my_numbers_logged ## [1] 3.662703 üëÜHere‚Äôs what we did above: We created a vector of numbers called mean_my_numbers_logged by nesting the seq() function inside of the log() function and nesting the log() function inside of the mean() function. Then, we printed the value of mean_my_numbers_logged to the screen to view. üëç What‚Äôs nice about the code above? It is certainly more efficient than the sequential step method we used at first. We went from using 4 lines of code to using 2 lines of code, and we didn‚Äôt generate any unneeded objects. üëé What could be better about the code above? Many people would say that this code is harder to read than than the the sequential step method we used at first. This is primarily due to the fact that each line no longer does one thing, and the code no longer follows a sequence of steps from start to finish. For example, the final operation we want to do is calculate the mean, but the mean() function is the first function we use in the code. Finally, let‚Äôs try see what this code looks like when we use pipes: # Performing an operation using pipes. mean_my_numbers_logged &lt;- seq(from = 2, to = 100, by = 2) %&gt;% log() %&gt;% mean() mean_my_numbers_logged ## [1] 3.662703 üëÜHere‚Äôs what we did above: We created a vector of numbers called mean_my_numbers_logged by passing the result of the seq() function directly to the log() function using the pipe operator, and passing the result of the the log() function directly to the mean() function using the pipe operator. Then, we printed the value of mean_my_numbers_logged to the screen to view. üëè As you can see, by using pipes we were able to retain the benefits of performing the operation in a series of steps (i.e., each line of code does one thing and they follow in sequential order) and the benefits of nesting functions (i.e., more efficient code). The utility of the pipe operator may not be immediately apparent to you based on this very simple example. So, next I‚Äôm going to show you a little snippet of code from one of my research projects. In the code chunk that follows, the operation I‚Äôm trying to perform on my data is written in two different ways ‚Äì without pipes and with pipes. It‚Äôs very unlikely that you will know what this code does, but that isn‚Äôt really the point. Just try to get a sense of which version is easier for you to read. # Nest functions without pipes responses &lt;- select(ungroup(filter(group_by(filter(merged_data, !is.na(incident_number)), incident_number), row_number() == 1)), date_entered, detect_data, validation) # Nest functions with pipes responses &lt;- merged_data %&gt;% filter(!is.na(incident_number)) %&gt;% group_by(incident_number) %&gt;% filter(row_number() == 1) %&gt;% ungroup() %&gt;% select(date_entered, detect_data, validation) What do you think? Even without know what this code does, do you feel like one version is easier to read than the other? 10.2 How do pipes work? Perhaps I‚Äôve convinced you that pipes are generally useful. But, it may not be totally obvious to you how to use them. They are actually really simple. Start by thinking about pipes as having a left side and a right side. Figure 10.1: Pipes have a left side and a right side. The thing on the right side of the pipe operator should always be a function. Figure 10.2: A function should always be to the right of the pipe operator. The thing on the left side of the pipe operator can be a function or an object. Figure 10.3: A function or an object can be to the left of the pipe operator. All the pipe operator does is take the thing on the left side and pass it to the first argument of the function on the right side. Figure 10.4: Pipe the left side to the first argument of the function on the right side. It‚Äôs a really simple concept, but it can also cause people a lot of confusion at first. So, let‚Äôs take look at a couple more concrete examples. Below we pass a vector of numbers to the to the mean() function, which returns the mean value of those numbers to us. mean(c(2, 4, 6, 8)) ## [1] 5 We can also use a pipe to pass that vector of numbers to the mean() function. c(2, 4, 6, 8) %&gt;% mean() ## [1] 5 So, the R interpreter took the thing on the left side of the pipe operator, stuck it into the first argument of the function on the right side of the pipe operator, and then executed the function. In this case, the mean() function doesn‚Äôt require any other arguments, so we don‚Äôt have to write anything else inside of the mean() function‚Äôs parentheses. When we see c(2, 4, 6, 8) %&gt;% mean(), R sees mean(c(2, 4, 6, 8)) Here‚Äôs one more example. Pretty soon we will learn how to use the filter() function from the dplyr package to keep only a subset of rows from our data frame. Let‚Äôs start by simulating some data: # Simulate some data height_and_weight &lt;- data.frame( id = c(&quot;001&quot;, &quot;002&quot;, &quot;003&quot;, &quot;004&quot;, &quot;005&quot;), sex = c(&quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;), ht_in = c(71, 69, 64, 65, 73), wt_lbs = c(190, 176, 130, 154, 173) ) height_and_weight ## id sex ht_in wt_lbs ## 1 001 Male 71 190 ## 2 002 Male 69 176 ## 3 003 Female 64 130 ## 4 004 Female 65 154 ## 5 005 Male 73 173 In order to work, the filter() function requires us to pass two values to it. The first value is the name of the data frame object with the rows we want to subset. The second is the condition used to subset the rows. Let‚Äôs say that we want to do a subgroup analysis using only the females in our data frame. We could use the filter() function like so: # First value = data frame name (height_and_weight) # Second value = condition for keeping rows (when the value of sex is Female) filter(height_and_weight, sex == &quot;Female&quot;) ## id sex ht_in wt_lbs ## 1 003 Female 64 130 ## 2 004 Female 65 154 üëÜHere‚Äôs what we did above: We kept only the rows from the data frame called height_and_weight that had a value of Female for the variable called sex using dplyr‚Äôs filter() function. We can also use a pipe to pass the height_and_weight data frame to the filter() function. # First value = data frame name (height_and_weight) # Second value = condition for keeping rows (when the value of sex is Female) height_and_weight %&gt;% filter(sex == &quot;Female&quot;) ## id sex ht_in wt_lbs ## 1 003 Female 64 130 ## 2 004 Female 65 154 As you can see, we get the exact same result. So, the R interpreter took the thing on the left side of the pipe operator, stuck it into the first argument of the function on the right side of the pipe operator, and then executed the function. In this case, the filter() function needs a value supplied to two arguments in order work. So, we wrote sex == \"Female\" inside of the filter() function‚Äôs parentheses. When we see height_and_weight %&gt;% filter(sex == \"Female\"), R sees filter(height_and_weight, sex == \"Female\"). üóíSide Note: This pattern ‚Äì a data frame piped into a function, which is usually then piped into one or more additional functions is something that you will see over and over in this book. Don‚Äôt worry too much about how the filter() function works. That isn‚Äôt the point here. The two main takeaways so far are: Pipes make your code easier to read once you get used to them. The R interpreter knows how to automatically take whatever is on the left side of the pipe operator and make it the value that gets passed to the first argument of the function on the right side of the pipe operator. 10.2.1 Keyboard shortcut Typing %&gt;% over and over can be tedious! Thankfully, RStudio provides a keyboard shortcut for inserting the pipe operator into your R code. On Mac type shift + command + m. On Windows type shift + control + m It may not seem totally intuitive at first, but this shortcut is really handy once you get used to it. 10.2.2 Pipe style As with all the code we write, style is an important consideration. I generally agree with the recommendations given in the Tidyverse style guide. In particular, I tend to use pipes in such a way that each line of my code does one, and only one, thing. For example: # Each line does one thing - Use responses &lt;- merged_data %&gt;% filter(!is.na(incident_number)) %&gt;% group_by(incident_number) %&gt;% filter(row_number() == 1) %&gt;% ungroup() %&gt;% select(date_entered, detect_data, validation) # Some lines do more than one thing - Don&#39;t use responses &lt;- merged_data %&gt;% filter(!is.na(incident_number)) %&gt;% group_by(incident_number) %&gt;% filter(row_number() == 1) %&gt;% ungroup() %&gt;% select(date_entered, detect_data, validation) As previously stated, there is a certain amount of subjectivity in what constitutes ‚Äúgood‚Äù style. But, I will once again reiterate that it is important to adopt some style and use it consistently. 10.3 Final thought on pipes I think it‚Äôs important to note that not everyone in the R programming community is a fan of using pipes. I hope I‚Äôve made a compelling case for why I use pipes, but I acknowledge that it is ultimately a preference, and that using pipes is not the best choice in all circumstances. Ultimately, whether or not you choose to use the pipe operator is up to you; however, I will be using them extensively throughout the remainder of this book. "],
["introduction-to-descriptive-analysis.html", "11 Introduction to descriptive analysis 11.1 What is descriptive analysis and why would we do it? 11.2 What kind of descriptive analysis should we perform?", " 11 Introduction to descriptive analysis 11.1 What is descriptive analysis and why would we do it? So, we have all this data that tells us all this information about different traits or characteristics of the people for whom the data was collected. For example, if we collected data about the students in this course, we may have information about how tall you are, about what kind of insurance you have, and about what your favorite color is. student_id height_in insurance color 1001 64.96 private blue 1002 67.93 other yellow 1003 84.03 none red But, unless you‚Äôre a celebrity, or under investigation for some reason, it‚Äôs unlikely that many people outside of your friends and family care to know any of this information about you, per se. Usually they want to know this information about the typical person in the population, or subpopulation, to which you belong. Or, they want to know more about the relationship between people who are like you in some way and some outcome that they are interested in. For example: We typically aren‚Äôt interested in knowing that student 1002 (above) is 67.93 inches tall. We are typically more interested in knowing things like the average height of the class ‚Äì 72.31. Before we can make any inferences or draw any conclusions, we must (or at least should) begin by conducting descriptive analysis of our data. This is also sometimes referred to as exploratory analysis. There are at least three reasons why we want to start with a descriptive analysis: We can use descriptive analysis to uncover errors in our data. It helps us understand the distribution of values in our variables. Descriptive analysis serve as a starting point for understanding relationships between our variables. 11.2 What kind of descriptive analysis should we perform? When conducting descriptive analysis, the method you choose will depend on the type of data you‚Äôre analyzing. At the most basic level, variables can be described as numerical or categorical. Numeric variables can then be further divided into continuous and discrete - the distinction being whether the variable can take on a continuum of values, or only set of certain values. Categorical variables can be subdivided into ordinal or nominal variables - depending on whether or not the categories can logically be ordered in a meaningful way. Finally, for all types, and subtypes, of variables there are both numerical and graphical methods we can use for descriptive analysis. In the exercises that follow you will be introduced to measures of frequency, measures of central tendency, and measures of dispersion. Then, you‚Äôll learn various methods for estimating and interpreting these measures using R. "],
["numerical-descriptions-of-categorical-variables.html", "12 Numerical descriptions of categorical variables 12.1 Height and Weight Data 12.2 Calculating frequencies 12.3 Calculating percentages 12.4 Missing data 12.5 Formatting results", " 12 Numerical descriptions of categorical variables We‚Äôll begin our discussion of descriptive statistics in the categorical half of our flow chart. Specifically, we‚Äôll start by numerically describing categorical variables. As a reminder, categorical variables are variables whose values fit into categories. Some examples of categorical variables commonly seen in public health data are: sex, race or ethnicity, and level of educational attainment. Notice that there is no inherent numeric value to any of these categories. Having said that, we can, and often will, assign a numeric value to each category using R. The two most common numerical descriptions of categorical variables are probably the frequency count (you will often hear this referred to as simply the frequency, the count, or the n) and the proportion or percentage (the percentage is just the proportion multiplied by 100). The count is simply the number of observations, in this case people, which fall into each possible category. The proportion is just the count divided by the total number of observations. In this example, 2 people out of 5 people (.40 or 40%) are in the Asian race category. Now let‚Äôs go over how to calculate frequency counts and percentages using R. 12.1 Height and Weight Data Below, we‚Äôre going to learn to do descriptive analysis in R by experimenting with some simulated data that contains several people‚Äôs sex, height, and weight. You can follow along with this lesson by copying and pasting the code chunks below in your R session. # Load the dplyr package. We will need several of dpylr&#39;s functions in the # code below. library(dplyr) # Simulate some data height_and_weight_20 &lt;- data.frame( id = c(&quot;001&quot;, &quot;002&quot;, &quot;003&quot;, &quot;004&quot;, &quot;005&quot;, &quot;006&quot;, &quot;007&quot;, &quot;008&quot;, &quot;009&quot;, &quot;010&quot;, &quot;011&quot;, &quot;012&quot;, &quot;013&quot;, &quot;014&quot;, &quot;015&quot;, &quot;016&quot;, &quot;017&quot;, &quot;018&quot;, &quot;019&quot;, &quot;020&quot;), sex = c(&quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &#39;Female&#39;), ht_in = c(71, 69, 64, 65, 73, 69, 68, 73, 71, 66, 71, 69, 66, 68, 75, 69, 66, 65, 65, 65), wt_lbs = c(190, 176, 130, 154, 173, 182, 140, 185, 157, 155, 213, 151, 147, 196, 212, 190, 194, 176, 176, 102) ) 12.1.1 View the data Let‚Äôs start our analysis by taking a quick look at our data‚Ä¶ height_and_weight_20 ## id sex ht_in wt_lbs ## 1 001 Male 71 190 ## 2 002 Male 69 176 ## 3 003 Female 64 130 ## 4 004 Female 65 154 ## 5 005 Male 73 173 ## 6 006 Male 69 182 ## 7 007 Female 68 140 ## 8 008 Male 73 185 ## 9 009 Female 71 157 ## 10 010 Male 66 155 ## 11 011 Male 71 213 ## 12 012 Female 69 151 ## 13 013 Female 66 147 ## 14 014 Female 68 196 ## 15 015 Male 75 212 ## 16 016 Female 69 190 ## 17 017 Female 66 194 ## 18 018 Female 65 176 ## 19 019 Female 65 176 ## 20 020 Female 65 102 üëÜHere‚Äôs what we did above: Simulated some data that we can use to practice categorical data analysis. We viewed the data and found that it has 4 variables (columns) and 20 observations (rows). Also notice that you can use the ‚ÄúNext‚Äù button at the bottom right corner of the printed data frame to view rows 11 through 20 if you are viewing this data in RStudio. 12.2 Calculating frequencies Now that we‚Äôre able to easily view our data, let‚Äôs return to the original purpose of this demonstration ‚Äì calculating frequencies and proportions. At this point, I suspect that few of you would have any trouble telling me that the frequency of females in this data is 12 and the frequency of males in this data is 8. It‚Äôs pretty easy to just count the number of females and males in this small data set with only 20 rows. Further, if I asked you what proportion of this sample is female, most of you would still be able to easily tell me 12/20 = 0.6, or 60%. But, what if we had 100 observations or 1,000,000 observations? You‚Äôd get sick of counting pretty quickly. Fortunately, you don‚Äôt have to! Let R do it for you! As is always the case with R, there are multiple ways we can calculate the statistics that we‚Äôre interested in. For example, we can use the base R table function like this: 12.2.1 The base R table function table(height_and_weight_20$sex) ## ## Female Male ## 12 8 Additionally, we can use the CrossTable function from the gmodels package, which gives us a little more information by default. 12.2.2 The gmodels CrossTable function # Like all packages, you will have to install gmodels before you can use the # CrossTable function. gmodels::CrossTable(height_and_weight_20$sex) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 20 ## ## ## | Female | Male | ## |-----------|-----------| ## | 12 | 8 | ## | 0.600 | 0.400 | ## |-----------|-----------| ## ## ## ## 12.2.3 The tidyverse way The final way I‚Äôm going to discuss here is the tidyverse way, which is my preference. We will have to write a little additional code, but the end result will be more flexible, more readable, and will return our statistics to us in a data frame that we can save and use for further analysis. Let‚Äôs walk through this step by step‚Ä¶ üóíSide Note: You should already be familiar with the pipe operator (%&gt;%), but if it doesn‚Äôt look familiar to you, you can learn more about it in Using pipes. Don‚Äôt forget, if you are using RStudio, you can use the keyboard shortcut shift + command + m (Mac) or shift + control + m (Windows) to insert the pipe operator. First, we don‚Äôt want to view the individual values in our data frame. Instead, we want to condense those values into summary statistics. This is a job for the summarise() function. height_and_weight_20 %&gt;% summarise() ## data frame with 0 columns and 1 row As you can see, summarise() doesn‚Äôt do anything interesting on its own. We need to tell it what kind of summary information we want. We can use the n() function to count rows. By, default, it will count all the rows in the data frame. For example: height_and_weight_20 %&gt;% summarise(n()) ## n() ## 1 20 üëÜHere‚Äôs what we did above: We passed our entire data frame to the summarise() function and asked it to count the number of rows in the data frame. The result we get is a new data frame with 1 column (named n()) and one row with the value 20 (the number of rows in the original data frame). This is a great start. However, we really want to count the number of rows that have the value ‚Äúfemale‚Äù for sex, and then separately count the number of rows that have the value ‚Äúmale‚Äù for sex. Said another way, we want to break our data frame up into smaller data frames ‚Äì one for each value of sex ‚Äì and then count the rows. This is exactly what dplyr‚Äôs group_by() function does. height_and_weight_20 %&gt;% group_by(sex) %&gt;% summarise(n()) ## # A tibble: 2 x 2 ## sex `n()` ## &lt;chr&gt; &lt;int&gt; ## 1 Female 12 ## 2 Male 8 And, that‚Äôs what we want. üóíSide Note: dplyr‚Äôs group_by() function operationalizes the Split - Apply - Combine strategy for data analysis. That sounds sort of fancy, but all it really means is that we split our data frame up into smaller data frames, apply our calculation separately to each smaller data frame, and then combine those individual results back together as a single result. So, in the example above, the height_and_weight_20 data frame was split into two separate little data frames (i.e., one for females and one for males), then the summarise() and n() functions counted the number of rows in each of the two smaller data frames (i.e., 12 and 8 respectively), and finally combined those individual results into a single data frame, which was printed to the screen for us to view. However, it will be awkward to work with a variable named n() in the future. Let‚Äôs go ahead and assign it a different name. We can assign it any valid name we want. Some names that might make sense are n, frequency, or count. I‚Äôm going to go ahead an just name it n without the parentheses. height_and_weight_20 %&gt;% group_by(sex) %&gt;% summarise(n = n()) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## sex n ## &lt;chr&gt; &lt;int&gt; ## 1 Female 12 ## 2 Male 8 üëÜHere‚Äôs what we did above: We added n = to our summarise function (summarise(n = n())) so that our count column in the resulting data frame would be named n instead of n(). Finally, estimating categorical frequencies like this is such a common operation that dplyr has a shortcut for it. We can use the count function to get the same result that we got above. height_and_weight_20 %&gt;% count(sex) ## sex n ## 1 Female 12 ## 2 Male 8 12.3 Calculating percentages In addition to frequencies, we will often be interested in calculating percentages for categorical variables. As always, there are many was to accomplish this in R. From here on out, I‚Äôm going to primarily use tidyverse functions. In this case, the proportion of people in our data who are female can be calculated as the number who are female (12) divided by the total number of people in the data. We already know that there are 20 people in the data, so we could calculate proportions like so: height_and_weight_20 %&gt;% count(sex) %&gt;% mutate(prop = n / 20) ## sex n prop ## 1 Female 12 0.6 ## 2 Male 8 0.4 üëÜHere‚Äôs what we did above: Because the count() function returns a data frame just like any other data frame, we can manipulate it in the same ways we can manipulate any other data frame. So, we used dplyr‚Äôs mutate() function to create a new variable in the data frame named prop. Again, we could have given it any valid name. We will talk more about mutate() in the data management part of the book. For now, just know that it is the primary function we will use for adding new columns (variables) to our data frames. Then we set the value of prop to be equal to the value of n divided by 20. This works, but it would be better to have R calculate the total number of observations for the denominator (20) than for us to manually type it in. In this case, we can do that with the sum() function. height_and_weight_20 %&gt;% count(sex) %&gt;% mutate(prop = n / sum(n)) ## sex n prop ## 1 Female 12 0.6 ## 2 Male 8 0.4 üëÜHere‚Äôs what we did above: Instead of manually typing in the total count for our denominator (20), we had R calculate it for us using the sum() function. The sum() function added together all the values of the variable n (i.e., 12 + 8 = 20). Finally, we just need to multiply our proportion by 100 to convert it to a percentage. height_and_weight_20 %&gt;% count(sex) %&gt;% mutate(percent = n / sum(n) * 100) ## sex n percent ## 1 Female 12 60 ## 2 Male 8 40 üëÜHere‚Äôs what we did above: Changed the name of the variable we are creating from prop to percent. But, we could have given it any valid name. Multiplied the proportion by 100 to convert it to a percentage. 12.4 Missing data In the real world, you will frequently encounter data that has missing values. Let‚Äôs quickly take a look at an example by adding some missing values to our data frame. height_and_weight_20 &lt;- height_and_weight_20 %&gt;% mutate(sex = replace(sex, c(2, 9), NA)) %&gt;% print() ## id sex ht_in wt_lbs ## 1 001 Male 71 190 ## 2 002 &lt;NA&gt; 69 176 ## 3 003 Female 64 130 ## 4 004 Female 65 154 ## 5 005 Male 73 173 ## 6 006 Male 69 182 ## 7 007 Female 68 140 ## 8 008 Male 73 185 ## 9 009 &lt;NA&gt; 71 157 ## 10 010 Male 66 155 ## 11 011 Male 71 213 ## 12 012 Female 69 151 ## 13 013 Female 66 147 ## 14 014 Female 68 196 ## 15 015 Male 75 212 ## 16 016 Female 69 190 ## 17 017 Female 66 194 ## 18 018 Female 65 176 ## 19 019 Female 65 176 ## 20 020 Female 65 102 üëÜHere‚Äôs what we did above: Replaced the 2nd and 9th value of sex with NA (missing) using the replace() function. Now let‚Äôs see how our code from above handles this height_and_weight_20 %&gt;% count(sex) %&gt;% mutate(percent = n / sum(n) * 100) ## sex n percent ## 1 Female 11 55 ## 2 Male 7 35 ## 3 &lt;NA&gt; 2 10 As you can see, we are now treating missing as if it were a category of sex. Sometimes this will be the result you want. However, often you will want the n and percent of non-missing values for your categorical variable. This is sometimes referred to as a complete case analysis. There‚Äôs a couple of different ways we can handle this. I will simply filter out rows with a missing value for sex with dplyr‚Äôs filter() function. height_and_weight_20 %&gt;% filter(!is.na(sex)) %&gt;% count(sex) %&gt;% mutate(percent = n / sum(n) * 100) ## sex n percent ## 1 Female 11 61.11111 ## 2 Male 7 38.88889 üëÜHere‚Äôs what we did above: We used filter() to keep only the rows that have a non-missing value for sex. In the R language, we use the is.na() function to tell the R interpreter to identify NA (missing) values in a vector. We cannot use something like sex == NA to identify NA values, which is sometimes confusing for people who are coming to R from other statistical languages. In the R language, ! is the NOT operator. It sort of means ‚Äúdo the opposite.‚Äù So, filter() tells R which rows of a data frame to keep and is.na(sex) tells R to find rows with an NA value for the variable sex. Together, filter(is.na(sex)) would tell R to keep rows with an NA value for the variable sex. Adding the NOT operator ! tells R to do the opposite ‚Äì keep rows that do NOT have an NA value for the variable sex. We used our code from above to calculate the n and percent of non-missing values of sex. 12.5 Formatting results Notice that now our percentages are being displayed with 5 digits to the right of the decimal. If we wanted to present our findings somewhere (e.g., a journal article or a report for our employer) we would almost never want to display this many digits. Let‚Äôs get R to round these numbers for us. height_and_weight_20 %&gt;% filter(!is.na(sex)) %&gt;% count(sex) %&gt;% mutate(percent = (n / sum(n) * 100) %&gt;% round(2)) ## sex n percent ## 1 Female 11 61.11 ## 2 Male 7 38.89 üëÜHere‚Äôs what we did above: We passed the calculated percentage values (n / sum(n) * 100) to the round() function to round our percentages to 2 decimal places. Notice that we had to wrap n / sum(n) * 100 in parentheses in order to pass it to the round() function with a pipe. We could have alternatively written our R code this way: mutate(percent = round(n / sum(n) * 100, 2)). üèÜ Congratulations! You now know how to use R to do some basic descriptive analysis of individual categorical variables. "],
["measures-of-central-tendency.html", "13 Measures of central tendency 13.1 Calculate the mean 13.2 Calculate the median 13.3 Calculate the mode 13.4 Compare mean, median, and mode 13.5 Data checking 13.6 Properties of mean, median, and mode", " 13 Measures of central tendency In previous sections you‚Äôve seen methods for describing individual categorical variables. Now we‚Äôll switch over to numerically describing numerical variables. In epidemiology, we often want to describe the ‚Äútypical‚Äù person in a population with respect to some characteristic that is recorded as a numerical variable ‚Äì like height or weight. The most basic, and probably most commonly used, way to do so is with a measure of central tendency. In this chapter we‚Äôll discuss three measures of central tendency: The mean The median The mode Now, this is not a statistics course. But, I‚Äôm going to briefly discuss these measures, and some of their characteristics, below to make sure that we‚Äôre all on the same page when we discuss the interpretation of our results. The mean When we talk about the typical, or ‚Äúaverage‚Äù, value of some variable measured on a continuous scale, we are usually talking about the mean value of that variable. To be even more specific, we are usually talking about the arithmetic mean value. This value has some favorable characteristics that make it a good description of central tendency. üëç For starters it‚Äôs simple. Most people are familiar with the mean, and at the very least, have some intuitive sense of what it means (no pun intended). üëç In addition, there can be only one mean value for any set of values. However, there are a couple of potentially problematic characteristics of the mean as well: üëé It‚Äôs susceptible to extreme values in your data. In other words, a couple of people with very atypical values for the characteristic you are interested in can drastically alter the value of the mean, and your estimate for the typical person in your population of interest along with it. üëé Additionally, it‚Äôs very possible to calculate a mean value that is not actually observed anywhere in your data. üóíSide Note: The sample mean is often referred to as \\(\\bar{x}\\), which pronounced ‚Äúx bar.‚Äù The median The median is probably the second most commonly used measure of central tendency. Like the mean, it‚Äôs computationally simple and relatively straightforward to understand. üëç There can be one, and only one, median. üëç And, its value may also be unobserved in the data.üëé However, unlike the mean, it‚Äôs relatively resistant to extreme values. üëç In fact, when the median is used as the measure of central tendency, it‚Äôs often because the person conducting the analysis suspects that extreme values in the data are likely to distort the mean. The mode And finally, we have the mode, or the value that is most often observed in the data. It doesn‚Äôt get much simpler than that. üëç But, unlike the mean and the median, there can be more than one mode for a given set of values. In fact, there can even be no mode if all the values are observed the exact same number of times.üëé However, if there is a mode, by definition it‚Äôs observed in the data.üëç Now that we are all on the same page with respect to the fundamentals of central tendency, let‚Äôs take a look at how to calculate these measures using R. 13.1 Calculate the mean Calculating the mean is really straightforward. We can just use base R‚Äôs built-in mean() function. # Load the tibble package for the tribble function library(tibble) # Load the dplyr package. We will need several of dpylr&#39;s functions in the # code below. library(dplyr) # Simulate some data height_and_weight_20 &lt;- tribble( ~id, ~sex, ~ht_in, ~wt_lbs, &quot;001&quot;, &quot;Male&quot;, 71, 190, &quot;002&quot;, &quot;Male&quot;, 69, 177, &quot;003&quot;, &quot;Female&quot;, 64, 130, &quot;004&quot;, &quot;Female&quot;, 65, 153, &quot;005&quot;, NA, 73, 173, &quot;006&quot;, &quot;Male&quot;, 69, 182, &quot;007&quot;, &quot;Female&quot;, 68, 186, &quot;008&quot;, NA, 73, 185, &quot;009&quot;, &quot;Female&quot;, 71, 157, &quot;010&quot;, &quot;Male&quot;, 66, 155, &quot;011&quot;, &quot;Male&quot;, 71, 213, &quot;012&quot;, &quot;Female&quot;, 69, 151, &quot;013&quot;, &quot;Female&quot;, 66, 147, &quot;014&quot;, &quot;Female&quot;, 68, 196, &quot;015&quot;, &quot;Male&quot;, 75, 212, &quot;016&quot;, &quot;Female&quot;, 69, 19000, &quot;017&quot;, &quot;Female&quot;, 66, 194, &quot;018&quot;, &quot;Female&quot;, 65, 176, &quot;019&quot;, &quot;Female&quot;, 65, 176, &quot;020&quot;, &quot;Female&quot;, 65, 102 ) üëÜ Here‚Äôs what we did above: We loaded the tibble package so that we could use its tribble() function. We used the tribble() function to simulate some data ‚Äì heights and weights for 20 hypothetical students. The tribble() function creates something called a tibble. A tibble is the tidyverse version of a data frame. In fact, it is a data frame, but with some additional functionality. You can use the link to read more about it if you‚Äôd like. We used the tribble() function instead of the data.frame() function to create our data frame above because we can use the tribble() function to create our data frames in rows (like you see above) instead of columns with the c() function. Using the tribble() function to create a data frame isn‚Äôt any better or worse than using the data.frame() function. I just wanted you to be aware that it exists and is sometimes useful. mean(height_and_weight_20$ht_in) ## [1] 68.4 üëÜ Here‚Äôs what we did above: We used base R‚Äôs mean() function to calculate the mean of the column ‚Äúht_in‚Äù from the data frame ‚Äúheight_and_weight_20‚Äù. Note: if you just type mean(ht_in) you will get an error. That‚Äôs because R will look for an object called ‚Äúht_in‚Äù in the global environment. However, we didn‚Äôt create an object called ‚Äúht_in‚Äù. We created an object (in this case a data frame) called ‚Äúheight_and_weight_20‚Äù. That object has a column in it called ‚Äúht_in‚Äù. So, we must specifically tell R to look for the ‚Äúht_in‚Äù column in the data frame ‚Äúheight_and_weight_20‚Äù. Using base R, we can do that in one of two ways: height_and_weight_20$ht_in or height_and_weight_20[[\"ht_in\"]]. 13.2 Calculate the median Similar to above, we can use base R‚Äôs median() function to calculate the median. median(height_and_weight_20$ht_in) ## [1] 68.5 üëÜ Here‚Äôs what we did above: We used base R‚Äôs median() function to calculate the modian of the column ‚Äúht_in‚Äù from the data frame ‚Äúheight_and_weight_20‚Äù. 13.3 Calculate the mode Base R does not have a built-in mode() function. Well, it actually does have a mode() function, but for some reason that function does not return the mode value(s) of a set of numbers. Instead, the mode() function gets or sets the type or storage mode of an object. For example: mode(height_and_weight_20$ht_in) ## [1] &quot;numeric&quot; This is clearly not what we are looking for. So, how do we find the mode value(s)? Well, we are going to have to build our own mode function. Later in the book, I‚Äôll return to this function and walk you through how I built it one step at a time. For now, just copy and paste the code into R on your computer. Keep in mind, as is almost always the case with R, my way of writing this function is only one of multiple possible ways. mode_val &lt;- function(x) { # Count the number of occurrences for each value of x value_counts &lt;- table(x) # Get the maximum number of times any value is observed max_count &lt;- max(value_counts) # Create and index vector that identifies the positions that correspond to # count values that are the same as the maximum count value: TRUE if so # and false otherwise index &lt;- value_counts == max_count # Use the index vector to get all values that are observed the same number # of times as the maximum number of times that any value is observed unique_values &lt;- names(value_counts) result &lt;- unique_values[index] # If result is the same length as value counts that means that every value # occured the same number of times. If every value occurred the same number # of times, then there is no mode no_mode &lt;- length(value_counts) == length(result) # If there is no mode then change the value of result to NA if (no_mode) { result &lt;- NA } # Return result result } mode_val(height_and_weight_20$ht_in) ## [1] &quot;65&quot; &quot;69&quot; üëÜ Here‚Äôs what we did above: We created our own function, mode_val(), that takes a vector (or data frame column) as a value to its ‚Äúx‚Äù argument and returns the mode value(s) of that vector. We can also see that the function works as expected when there is more than one mode value. In this case, ‚Äú65‚Äù and ‚Äú69‚Äù each occur 4 times in the column ‚Äúht_in‚Äù. 13.4 Compare mean, median, and mode Now that you know how to calculate the mean, median, and mode, let‚Äôs compare these three measures of central tendency. This is a good opportunity to demonstrate some of the different characteristics of each that we spoke about earlier. height_and_weight_20 %&gt;% summarise( min_weight = min(wt_lbs), mean_weight = mean(wt_lbs), median_weight = median(wt_lbs), mode_weight = mode_val(wt_lbs) %&gt;% as.double(), max_weight = max(wt_lbs) ) ## # A tibble: 1 x 5 ## min_weight mean_weight median_weight mode_weight max_weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 102 1113. 176. 176 19000 üëÜ Here‚Äôs what we did above: We used the mean() function, median() function, and our mode_val() function inside of dpylr‚Äôs summarise() function to find the mean, median, and mode values of the column ‚Äúwt_lbs‚Äù in the ‚Äúheight_and_weight_20‚Äù data frame. We also used the as.double() function to convert the value returned by mode_val() ‚Äì ‚Äú176‚Äù ‚Äì from a character string to a numeric double. This isn‚Äôt strictly necessary, but I think it looks better. Finally, we used base R‚Äôs min() and max() functions to view the lowest and highest weights in our sample. 13.5 Data checking Do you see any red flags üö©as you scan the results? Do you really think a mean weight of 1,113 pounds sounds reasonable? This should definitely be a red flag for you. Now move your gaze three columns to the right and notice that the maximum value of weight is 19,000 lbs ‚Äì an impossible value for a study in human populations. In this case the real weight was supposed to be 190 pounds, but the person entering the data accidently got a little trigger-happy with the zero key. This is an example of what I meant when I said ‚ÄúWe can use descriptive analysis to uncover errors in our data‚Äù in the Introduction to descriptive analysis chapter. Often times, for various reasons, some observations for a given variable take on values that don‚Äôt make sense. Starting by calculating some basic descriptive statistics for each variable is one approach you can use to try to figure out if you have values in your data that don‚Äôt make sense. In this case we can just go back and fix our data, but what if we didn‚Äôt know this value was an error? What if it were a value that was technically possible, but very unlikely? Well, we can‚Äôt just go changing values in our data. It‚Äôs unethical, and in some cases illegal. Below, we discuss the how the properties of the median and mode can come in handy in situations such as this. 13.6 Properties of mean, median, and mode Despite the fact that this impossibly extreme value is in our data, the median and mode estimates are reasonable estimates of the typical person‚Äôs weight in this sample. This is what I meant when I said that the median and mode were more ‚Äúresistant to extreme values‚Äù than the mean. You may also notice that no person in our sample had an actual weight of 1,112.75 (the mean) or even 176.5 (the median). This is what I meant above when I said that the mean and median values are ‚Äúnot necessarily observed in the data.‚Äù In this case, the mode value (176) is also a more reasonable estimate of the average person‚Äôs weight than the mean. And unlike the mean and the median, participants 18 and 19 actually weigh 176 pounds. I‚Äôm not saying that the mode is always the best measure of central tendency to use. However, I am saying that you can often learn useful information from your data by calculating and comparing these relatively simple descriptive statistics on each of your numeric variables. "],
["measures-of-dispersion.html", "14 Measures of dispersion 14.1 Comparing distributions", " 14 Measures of dispersion In the chapter on measures of central tendency, we found the minimum value, mean value, median value, mode value, and maximum value of the weight variable in our hypothetical sample of students. We‚Äôll go ahead start this lesson by rerunning that analysis below, but this time we will analyze heights instead of weights. # Load the tibble package for the tribble function library(tibble) # Load the dplyr package. We will need several of dpylr&#39;s functions in the # code below. library(dplyr) # Simulate some data height_and_weight_20 &lt;- tribble( ~id, ~sex, ~ht_in, ~wt_lbs, &quot;001&quot;, &quot;Male&quot;, 71, 190, &quot;002&quot;, &quot;Male&quot;, 69, 177, &quot;003&quot;, &quot;Female&quot;, 64, 130, &quot;004&quot;, &quot;Female&quot;, 65, 153, &quot;005&quot;, NA, 73, 173, &quot;006&quot;, &quot;Male&quot;, 69, 182, &quot;007&quot;, &quot;Female&quot;, 68, 186, &quot;008&quot;, NA, 73, 185, &quot;009&quot;, &quot;Female&quot;, 71, 157, &quot;010&quot;, &quot;Male&quot;, 66, 155, &quot;011&quot;, &quot;Male&quot;, 71, 213, &quot;012&quot;, &quot;Female&quot;, 69, 151, &quot;013&quot;, &quot;Female&quot;, 66, 147, &quot;014&quot;, &quot;Female&quot;, 68, 196, &quot;015&quot;, &quot;Male&quot;, 75, 212, &quot;016&quot;, &quot;Female&quot;, 69, 19000, &quot;017&quot;, &quot;Female&quot;, 66, 194, &quot;018&quot;, &quot;Female&quot;, 65, 176, &quot;019&quot;, &quot;Female&quot;, 65, 176, &quot;020&quot;, &quot;Female&quot;, 65, 102 ) # Recreate our mode function mode_val &lt;- function(x) { value_counts &lt;- table(x) result &lt;- names(value_counts)[value_counts == max(value_counts)] if (length(value_counts) == length(result)) { result &lt;- NA } result } height_and_weight_20 %&gt;% summarise( min_height = min(ht_in), mean_height = mean(ht_in), median_height = median(ht_in), mode_height = mode_val(ht_in) %&gt;% paste(collapse = &quot; , &quot;), max_height = max(ht_in) ) ## # A tibble: 1 x 5 ## min_height mean_height median_height mode_height max_height ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 64 68.4 68.5 65 , 69 75 üóíSide Note: To get both mode height values to display in the output above I used the paste() function with the collapse argument set to \" , \" (notices the spaces). This forces R to display our mode values as a character string. The downside is that the ‚Äúmode_height‚Äù variable no longer has any numeric value to R ‚Äì it‚Äôs simply a character string. However, this isn‚Äôt a problem for us. We won‚Äôt be using the mode in this lesson ‚Äì and it is rarely used in practice. Keep in mind that our interest is in describing the ‚Äútypical‚Äù or ‚Äúaverage‚Äù person in our sample. The results of our analysis above tells us that the average person who answered the height question in our hypothetical class was: 68.4 inches. This information gets us reasonably close to understanding the typical height of the students in our hypothetical class. But remember, our average person does not necessarily have the same height as any actual person in our class. So a natural extension of our original question is: ‚Äúhow much like the average person, are the other people in class.‚Äù For example, is everyone in class 68.4 inches? Or are there differences in everyone‚Äôs height, with the average person‚Äôs height always having a value in the middle of everyone else‚Äôs? The measures used to answer this question are called measures of dispersion, which we can say is the amount of difference between people in the class, or more generally, the amount of variability in the data. Three common measures of dispersion used are the: Range Variance Standard Deviation Range The range is simply the difference between the maximum and minimum value in the data. height_and_weight_20 %&gt;% summarise( min_height = min(ht_in), mean_height = mean(ht_in), max_height = max(ht_in), range = max_height - min_height ) ## # A tibble: 1 x 4 ## min_height mean_height max_height range ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 64 68.4 75 11 In this case, the range is 11. The range can be useful because it tells us how much difference there is between the tallest person in our class and the shortest person in our class ‚Äì 11 inches. However, it doesn‚Äôt tell us how close to 68.4 inches ‚Äúmost‚Äù people in the class are. In other words, are most people in the class out at the edges of the range of values in the data? Or are people ‚Äúevenly distributed‚Äù across the range of heights for the class? Or something else entirely? Variance The variance is a measure of dispersion that is slightly more complicated to calculate, although not much, but gives us a number we can use to quantify the dispersion of heights around the mean. To do this, let‚Äôs work through a simple example that only includes six observations: 3 people who are 58 inches tall and 3 people who are 78 inches tall. In this sample of six people from our population the average height is 68 inches. Next, let‚Äôs draw an imaginary line straight up from the mean. Then, let‚Äôs measure the difference, or distance, between each person‚Äôs height and the mean height. Then we square the differences. Then we add up all the squared differences. And finally, we divide by n, the number of non-missing observations, minus 1. In this case n equals six, so n-1 equals five. üóíSide Note: The sample variance is often written as \\(s^2\\). üóíSide Note: If the 6 observations here represented our entire population of interest, then we could simply divide by n instead of n-1. Getting R to do this math for us is really straightforward. We simply use base R‚Äôs var() function. var(c(rep(58, 3), rep(78, 3))) ## [1] 120 üëÜ Here‚Äôs what we did above: We created a numeric vector of heights using the c() function. Instead of typing c(58, 58, 58, 78, 78, 78) I chose to use the rep() function. rep(58, 3) is equivalent to typing c(58, 58, 58) and rep(78, 3) is equivalent to typing c(78, 78, 78). We passed this numeric vector to the var() function and R returned the variance ‚Äì 120 So, 600 divided by 5 equals 120. Therefore, the sample variance in this case is 120. However, because the variance is expressed in squared units, instead of the original units, it isn‚Äôt necessarily intuitive to interpret. Standard deviation If we take the square root of the variance, we get the standard deviation. üóíSide Note: The sample standard deviation is often written as \\(s\\). The standard deviation is 10.95 inches, which is much easier to interpret, and compare with other samples. Now that we know the sample standard deviation, we can use it to describe a value‚Äôs distance from the mean. Additionally, when our data is approximately normally distributed, then the percentage of values within each standard deviation from the mean follow the rules displayed in this table: That is, about 68% of all the observations fall within one standard deviation of the mean (that is, 10.95 inches). About 95% of all observations are within 2 standard deviations of the mean (that is, 10.95 * 2 = 21.9 inches), and about 99.9% of all observations are within 3 standard deviations of the mean (that is, 10.95 * 3 = 32.85 inches). Don‚Äôt forget that these percentage rules apply to values around the mean. In other words, half the values will be greater than the mean and half the values will be lower than the mean. You will often see this graphically illustrated with a ‚Äúnormal curve‚Äù or ‚Äúbell curve.‚Äù Unfortunately, the current data is nowhere near normally distributed and does not make for a good example of this rule. 14.1 Comparing distributions Now that you understand what the different measures of distribution are and how they are calculated, let‚Äôs further develop your ‚Äúfeel‚Äù for interpreting them. I like to do this by comparing different simulated distributions. sim_data &lt;- tibble( all_68 = rep(68, 20), half_58_78 = c(rep(58, 10), rep(78, 10)), even_58_78 = seq(from = 58, to = 78, length.out = 20), half_48_88 = c(rep(48, 10), rep(88, 10)), even_48_88 = seq(from = 48, to = 88, length.out = 20) ) sim_data ## # A tibble: 20 x 5 ## all_68 half_58_78 even_58_78 half_48_88 even_48_88 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 68 58 58 48 48 ## 2 68 58 59.1 48 50.1 ## 3 68 58 60.1 48 52.2 ## 4 68 58 61.2 48 54.3 ## 5 68 58 62.2 48 56.4 ## 6 68 58 63.3 48 58.5 ## 7 68 58 64.3 48 60.6 ## 8 68 58 65.4 48 62.7 ## 9 68 58 66.4 48 64.8 ## 10 68 58 67.5 48 66.9 ## 11 68 78 68.5 88 69.1 ## 12 68 78 69.6 88 71.2 ## 13 68 78 70.6 88 73.3 ## 14 68 78 71.7 88 75.4 ## 15 68 78 72.7 88 77.5 ## 16 68 78 73.8 88 79.6 ## 17 68 78 74.8 88 81.7 ## 18 68 78 75.9 88 83.8 ## 19 68 78 76.9 88 85.9 ## 20 68 78 78 88 88 üëÜ Here‚Äôs what we did above: We created a data frame with 5 simulated distributions: all_68 has a value of 68 repeated 20 times half_58_78 is made up of the values 58 and 78, each repeated 10 times (similar to our example above) even_58_78 is 20 evenly distributed numbers between 58 and 78 half_48_88 is made up of the values 48 and 88, each repeated 10 times even_48_88 is 20 evenly distributed numbers between 48 and 88 I can use this simulated data to quickly demonstrate a couple of these concepts for you. Let‚Äôs use R to calculate and compare the mean, variance, and standard deviation of each variable. tibble( Column = names(sim_data), Mean = purrr::map_dbl(sim_data, mean), Variance = purrr::map_dbl(sim_data, var), SD = purrr::map_dbl(sim_data, sd) ) ## # A tibble: 5 x 4 ## Column Mean Variance SD ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 all_68 68 0 0 ## 2 half_58_78 68 105. 10.3 ## 3 even_58_78 68 38.8 6.23 ## 4 half_48_88 68 421. 20.5 ## 5 even_48_88 68 155. 12.5 üëÜ Here‚Äôs what we did above: We created a data frame to hold some summary statistics about each column in the ‚Äúsim_data‚Äù data frame. We used the map_dbl() function from the purrr package to iterate over each column in the data. Don‚Äôt worry too much about this right now. We will talk more about iteration and the purrr package later in the book. So, for all the columns the mean is 68 inches. And that makes sense, right? We set the middle value and/or most commonly occurring value to be 68 inches for each of these variables. However, the variance and standard deviation are quite different. For the column ‚Äúall_68‚Äù the variance and standard deviation are both zero. If you think about it, this should make perfect sense: all the values are 68 ‚Äì they don‚Äôt vary ‚Äì and each observations distance from the mean (68) is zero. When comparing the rest of the columns notice that all of them have a non-zero variance. This is because not all people have the same value in that column ‚Äì they vary. Additionally, we can see very clearly that variance (and standard deviation) are affected by at least two things: First is the distribution of values across the range of possible values. For example, half_58_78 and half_48_88 have a larger variance than even_58_78 and even_48_88 because all the values are clustered at the min and max - far away from the mean. The second property of the data that is clearly influencing variance is the width of the range of values included in the distribution. For example, even_48_88 has a larger variance and standard deviation than even_58_78, even though both are evenly distributed across the range of possible values. The reason is because the range of possible values is larger, and therefore the range of distances from the mean is larger too. In summary, although the variance and standard deviation don‚Äôt always have a really intuitive meaning all by themselves, we can get some useful information by comparing them. Generally speaking, the variance is larger when values are clustered at very low or very high values away from the mean, or when values are spread across a wider range. "],
["describing-the-relationship-between-a-continuous-outcome-and-a-continuous-predictor.html", "15 Describing the relationship between a continuous outcome and a continuous predictor 15.1 Pearson Correlation Coefficient", " 15 Describing the relationship between a continuous outcome and a continuous predictor Before covering anything new, let‚Äôs quickly review the importance and utility of descriptive analysis. We can use descriptive analysis to uncover errors in our data Descriptive analysis helps us understand the distribution of values in our variables Descriptive analysis serves as a starting point for understanding relationships between our variables In the first few lessons on descriptive analysis we covered performing univariate analysis. That is, analyzing a single numerical or a single categorical variable. In this module, we‚Äôll learn methods for describing relationships between two variables. This is also called bivariate analysis. For example, we may be interested in knowing if there is a relationship between heart rate and exercise. If so, we may ask ourselves if heart rate differs, on average, by daily minutes of exercise. And, we could answer that question with the using a bivariate descriptive analysis. Before performing any such bivariate descriptive analysis, you should ask yourself what types of variables you will analyze. We‚Äôve already discussed the difference between numerical variables and categorical variables, but we will also need to decide whether each variable is an outcome or a predictor. Outcome variable: The variable whose value we are attempting to predict, estimate, or determine is the outcome variable. The outcome variable may also be referred to as the dependent variable or the response variable. Predictor variable: The variable that we think will determine, or at least help us predict, the value of the outcome variable is called the predictor variable. The predictor variable may also be referred to as the independent variable or the explanatory variable. So, think back to our interest in whether or not heart rate differs by daily minutes of exercise. In this scenario, which variable is the predictor and which is the outcome? In this scenario daily minutes of exercise is the predictor and heart rate is the outcome. Heart rate is the variable we‚Äôre interested in predicting or understanding, and exercise is a variable that we think helps to predict or explain heart rate. In this first chapter on bivariate analysis, we will learn a simple method for describing the relationship between a continuous outcome variable and a continuous predictor variable ‚Äì the Pearson Correlation Coefficient. 15.1 Pearson Correlation Coefficient Pearson‚Äôs Correlation Coefficient is a parametric measure of the linear relationship between two numerical variables. It‚Äôs also referred to as rho (pronounced like ‚Äúrow‚Äù) and can be written shorthand as a lowercase \\(r\\). The Pearson Correlation Coefficient can take on values between -1 and 1, including zero. A value of 0 indicates that there is no linear correlation between the two variables. A negative value indicates that there is a negative linear correlation between the two variables. In other words, as the value of x increases, the value of y decreases. Or, as the value of x decreases, the value of y increases. A positive value indicates that there is a positive linear correlation between the two variables. As the value of x increases, the value of y increases. Or as the value of x decreases, the value of y decreases. ‚ö†Ô∏èWarning: When the relationship between two variables is nonlinear, or when outliers are present, the correlation coefficient might incorrectly estimate the strength of the relationship. Plotting the data enables you to verify the linear relationship and to identify the potential outliers. 15.1.1 Calculating r In this first code chunk, we‚Äôre going to use some simple simulated data to develop an intuition about describing the relationship between two continuous variables. # Load the dplyr package library(dplyr) # Load the ggplot2 package library(ggplot2) set.seed(123) df &lt;- tibble( id = 1:20, x = sample(x = 0:100, size = 20, replace = TRUE), y = sample(x = 0:100, size = 20, replace = TRUE) ) df ## # A tibble: 20 x 3 ## id x y ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 30 71 ## 2 2 78 25 ## 3 3 50 6 ## 4 4 13 41 ## 5 5 66 8 ## 6 6 41 82 ## 7 7 49 35 ## 8 8 42 77 ## 9 9 100 80 ## 10 10 13 42 ## 11 11 24 75 ## 12 12 89 14 ## 13 13 90 31 ## 14 14 68 6 ## 15 15 90 8 ## 16 16 56 40 ## 17 17 91 73 ## 18 18 8 22 ## 19 19 92 26 ## 20 20 98 59 üëÜ Here‚Äôs what we did above: We created a data frame with 3 simulated variables ‚Äì id, x, and y. We used the sample() function to create x and y by sampling a number between 0 and 100 at random, 20 times. The replace = TRUE option tells R that the same number can be selected more than once. The set.seed() function is to ensure that I get the same random numbers every time I run the code chunk. There is nothing special about 0 and 100; they are totally arbitrary. But, because all of these values are chosen at random, we have no reason to believe that there should be any relationship between them. Accordingly, we should also expect the Pearson Correlation Coefficient to be 0 (or very close to it). In order to develop an intuition, let‚Äôs first plot this data, and get a feel for what it looks like. ggplot(df, aes(x, y)) + geom_point() + theme_bw() Above, we‚Äôve created a nice scatter plot using ggplot2(). But, how do we interpret it? Well, each dot corresponds to a person in our data at the point where their x value intersects with their y value. This is made clearer by adding a geom_text() layer to our plot. ggplot(df, aes(x, y)) + geom_point() + geom_text(aes(label = id), nudge_x = 1.5, nudge_y = 2) + theme_bw() üëÜ Here‚Äôs what we did above: We added a geom_text() layer to our plot in order to make it clear which person each dot represents. The nudge_x = 1.5 option moves our text (the id number) to the right 1.5 units. The nudge_y = 2 option moves our text 2 units up. We did this to make the id number easier to read. If we had not nudged them, they would have been placed directly on top of the points. For example, person 1 in our simulated data had an x value of 30 and a y value of 71. When you look at the plot above, does it look like person 1‚Äôs point is approximately at (x = 30, y = 71)? If we want to emphasize the point even further, we can plot a vertical line at x = 30 and a horizontal line at y = 71. Let‚Äôs do that below. ggplot(df, aes(x, y)) + geom_text(aes(label = id), nudge_x = 1.5, nudge_y = 2) + geom_vline(xintercept = 30, col = &quot;red&quot;, size = 0.25) + geom_hline(yintercept = 71, col = &quot;red&quot;, size = 0.25) + geom_point() + theme_bw() As you can see, the dot representing id 1 is at the intersection of these two lines. So, we know how to read the plot now, but we still don‚Äôt really know anything about the relationship between x and y. Remember, we want to be able to characterize x and y as having one of these 5 relationships: Looking again at our scatter plot, which relationship do you think x and y have? ggplot(df, aes(x, y)) + geom_point() + geom_text(aes(label = id), nudge_x = 1.5, nudge_y = 2) + geom_point(aes(x, y), data.frame(x = 100, y = 80), shape = 1, size = 16, col = &quot;red&quot;) + geom_point(aes(x, y), data.frame(x = 90, y = 8), shape = 1, size = 16, col = &quot;blue&quot;) + theme_bw() Well, if you look at id 9 above, x is a high number (100) and y is a high number (80). But if you look at id 15, x is a high number (90) and y is a low number (8). In other words, these dots are scattered all over the chart area. There doesn‚Äôt appear to be much of a pattern, trend, or relationship. And that‚Äôs exactly what we would expect from randomly generated data. Now that we know what this data looks like, and we intuitively feel as though x and y are unrelated, it would be nice to quantify our results in some way. And, that is precisely what the Pearson Correlation Coefficient does. cor.test(x = df$x, y = df$y) ## ## Pearson&#39;s product-moment correlation ## ## data: df$x and df$y ## t = -0.60281, df = 18, p-value = 0.5542 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.5490152 0.3218878 ## sample estimates: ## cor ## -0.1406703 üëÜ Here‚Äôs what we did above: By default, R‚Äôs cor.test() function gives us a list of information about the relationship between x and y. The very last number in the output (-0.1406703) is the Pearson Correlation Coefficient. The fact that this value is negative (between -1 and 0) tells us that x and y tend to vary in opposite directions. The numeric value (0.1406703) tells us something about the strength of the relationship between x and y. In this case, the relationship is not strong ‚Äì exactly what we expected. You will sometimes hear rules of thumb for interpreting the strength of \\(r\\) such as5: ¬±0.1 = Weak correlation ¬±0.3 = Medium correlation ¬±0.5 = Strong correlation Rules of thumb like this are useful as you are learning; however, you want to make sure you don‚Äôt become overly reliant on them. As you get more experience, you will want to start interpreting effect sizes in the context of your data and the specific research question at hand. The p-value (0.5542) tells us that we‚Äôd be pretty likely to get the result we got even if there really were no relationship between x and y ‚Äì assuming all other assumptions are satisfied and the sample was collected without bias. Taken together, the weak negative correlation and p-value tell us that there is not much ‚Äì if any ‚Äì relationship between x and y. Another way to say the same thing is, ‚Äúx and y are statistically independent.‚Äù 15.1.2 Correlation intuition To further bolster our intuition about these relationships, let‚Äôs look at a few positively and negatively correlated variables. # Positively correlated data tibble( x = 1:10, y = 100:109, r = cor(x, y) ) %&gt;% ggplot() + geom_point(aes(x, y)) + geom_text(aes(x = 2.5, y = 107.5, label = paste(&quot;r = &quot;, r)), col = &quot;blue&quot;) + theme_classic() Above, we created positively correlated data. In fact, this data is perfectly positively correlated. That is, every time the value of x increases, the value of y increases by a proportional amount. Now, instead of being randomly scattered around the plot area, the dots line up in a perfect, upward-sloping, diagonal line. I also, went ahead and added the correlation coefficient directly to the plot. As you can see, it is exactly 1. This is what you should expect from perfectly positively correlated data. How about this next data set? Now, every time x decreases by one, y decreases by one. Is this positively or negatively correlated data? df &lt;- tibble( x = 1:-8, y = 100:91 ) df ## # A tibble: 10 x 2 ## x y ## &lt;int&gt; &lt;int&gt; ## 1 1 100 ## 2 0 99 ## 3 -1 98 ## 4 -2 97 ## 5 -3 96 ## 6 -4 95 ## 7 -5 94 ## 8 -6 93 ## 9 -7 92 ## 10 -8 91 df %&gt;% mutate(r = cor(x, y)) %&gt;% ggplot() + geom_point(aes(x, y)) + geom_text(aes(x = -6, y = 98, label = paste(&quot;r = &quot;, r)), col = &quot;blue&quot;) + theme_classic() This is still perfectly positively correlated data. The values for x and y are still changing in the same direction proportionately. The fact that the direction is one of decreasing value makes no difference. One last simulated example here. This time, as x increases by one, y decreases by one. Let‚Äôs plot this data and calculate the Pearson Correlation Coefficient. tibble( x = 1:10, y = 100:91, r = cor(x, y) ) %&gt;% ggplot() + geom_point(aes(x, y)) + geom_text(aes(x = 7.5, y = 98, label = paste(&quot;r = &quot;, r)), col = &quot;blue&quot;) + theme_classic() This is what perfectly negatively correlated data looks like. The dots line up in a perfect, downward-sloping diagonal line, and when we check the value of rho, we see that it is exactly -1. Of course, as you may have suspected, in real life things are almost never this cut and dry. So, let‚Äôs investigate the relationship between continuous variables using more realistic data. In this example I‚Äôm using data from a class survey I actually conducted in the past: class &lt;- tibble( ht_in = c(70, 63, 62, 67, 67, 58, 64, 69, 65, 68, 63, 68, 69, 66, 67, 65, 64, 75, 67, 63, 60, 67, 64, 73, 62, 69, 67, 62, 68, 66, 66, 62, 64, 68, NA, 68, 70, 68, 68, 66, 71, 61, 62, 64, 64, 63, 67, 66, 69, 76, NA, 63, 64, 65, 65, 71, 66, 65, 65, 71, 64, 71, 60, 62, 61, 69, 66, NA), wt_lbs = c(216, 106, 145, 195, 143, 125, 138, 140, 158, 167, 145, 297, 146, 125, 111, 125, 130, 182, 170, 121, 98, 150, 132, 250, 137, 124, 186, 148, 134, 155, 122, 142, 110, 132, 188, 176, 188, 166, 136, 147, 178, 125, 102, 140, 139, 60, 147, 147, 141, 232, 186, 212, 110, 110, 115, 154, 140, 150, 130, NA, 171, 156, 92, 122, 102, 163, 141, NA) ) Next, I‚Äôm going to use a scatter plot to explore the relationship between height and weight in this data. ggplot(class, aes(ht_in, wt_lbs)) + geom_jitter() + theme_classic() ## Warning: Removed 4 rows containing missing values (geom_point). Quickly, what do you think? Will height and weight be positively correlated, negatively correlated, or not correlated? cor.test(class$ht_in, class$wt_lbs) ## ## Pearson&#39;s product-moment correlation ## ## data: class$ht_in and class$wt_lbs ## t = 5.7398, df = 62, p-value = 3.051e-07 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4013642 0.7292714 ## sample estimates: ## cor ## 0.5890576 The dots don‚Äôt line up in a perfectly upward ‚Äì or downward ‚Äì slope. But the general trend is still an upward slope. Additionally, we can see that height and weight are positively correlated because the value of the correlation coefficient is between 0 and positive 1 (0.5890576). By looking at the p-value (3.051e-07), we can also see that the probability of finding a correlation value this large or larger in our sample if the true value of the correlation coefficient in the population from which our sample was drawn is zero, is very small. That‚Äôs quite a mouthful, right? In more relatable terms, you can just think of it this way. In our data, as height increases weight tends to increase as well. Our p-value indicates that it‚Äôs pretty unlikely that we would get this result if there were truly no relationship in the population this sample was drawn from ‚Äì assuming it‚Äôs an unbiased sample. Quick detour: The p-value above is written in scientific notation, which you may not have seen before. I‚Äôll quickly show you how to basically disable scientific notation in R. options(scipen = 999) cor.test(class$ht_in, class$wt_lbs) ## ## Pearson&#39;s product-moment correlation ## ## data: class$ht_in and class$wt_lbs ## t = 5.7398, df = 62, p-value = 0.0000003051 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4013642 0.7292714 ## sample estimates: ## cor ## 0.5890576 üëÜ Here‚Äôs what we did above: We used the R global option options(scipen = 999) to display decimal numbers instead of scientific notation. Because this is a global option, it will remain in effect until you restart your R session. If you do restart your R session, you will have to run options(scipen = 999) again to disable scientific notation. Finally, wouldn‚Äôt it be nice if we could draw a line through this graph that sort of quickly summarizes this relationship (or lack thereof). Well, that is exactly what an Ordinary Least Squares (OLS) regression line does. To add a regression line to our plot, all we need to do is add a geom_smooth() layer to our scatterplot with the method argument set to lm. Let‚Äôs do that below and take a look. ggplot(class, aes(ht_in, wt_lbs)) + geom_smooth(method = &quot;lm&quot;) + geom_jitter() + theme_classic() ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 4 rows containing non-finite values (stat_smooth). ## Warning: Removed 4 rows containing missing values (geom_point). The exact calculation for deriving this line is beyond the scope of this chapter. In general, though, you can think of the line as cutting through the middle of all of your points and representing the average change in the y value given a one-unit change in the x value. So here, the upward slope indicates that, on average, as height (the x value) increases, so does weight (the y value). And that is completely consistent with our previous conclusions about the relationship between height and weight. References "],
["describing-the-relationship-between-a-continuous-outcome-and-a-categorical-predictor.html", "16 Describing the relationship between a continuous outcome and a categorical predictor 16.1 Single predictor and single outcome 16.2 Multiple predictors", " 16 Describing the relationship between a continuous outcome and a categorical predictor Up until now, we have only ever looked at the overall mean of a continuous variable. For example, the mean height for the entire class. However, we often want to estimate the means within levels, or categories, of another variable. For example, we may want to look at the mean height within gender. Said another way, we want to know the mean height for men and separately the mean height for women. More generally, in this lesson you will learn to perform bivariate analysis when the outcome is continuous and the predictor is categorical. Typically in a situation such as this, all we need to do is apply the analytic methods we‚Äôve already learned for a single continuous outcome, but apply them separately within levels of our categorical predictor variable. Below, we‚Äôll walk through doing so with R. To start with, we will again use our previously collected class survey data. library(dplyr) library(ggplot2) class &lt;- tibble( age = c(32, 30, 32, 29, 24, 38, 25, 24, 48, 29, 22, 29, 24, 28, 24, 25, 25, 22, 25, 24, 25, 24, 23, 24, 31, 24, 29, 24, 22, 23, 26, 23, 24, 25, 24, 33, 27, 25, 26, 26, 26, 26, 26, 27, 24, 43, 25, 24, 27, 28, 29, 24, 26, 28, 25, 24, 26, 24, 26, 31, 24, 26, 31, 34, 26, 25, 27, NA), age_group = c(2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, NA), gender = c(2, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, NA), ht_in = c(70, 63, 62, 67, 67, 58, 64, 69, 65, 68, 63, 68, 69, 66, 67, 65, 64, 75, 67, 63, 60, 67, 64, 73, 62, 69, 67, 62, 68, 66, 66, 62, 64, 68, NA, 68, 70, 68, 68, 66, 71, 61, 62, 64, 64, 63, 67, 66, 69, 76, NA, 63, 64, 65, 65, 71, 66, 65, 65, 71, 64, 71, 60, 62, 61, 69, 66, NA), wt_lbs = c(216, 106, 145, 195, 143, 125, 138, 140, 158, 167, 145, 297, 146, 125, 111, 125, 130, 182, 170, 121, 98, 150, 132, 250, 137, 124, 186, 148, 134, 155, 122, 142, 110, 132, 188, 176, 188, 166, 136, 147, 178, 125, 102, 140, 139, 60, 147, 147, 141, 232, 186, 212, 110, 110, 115, 154, 140, 150, 130, NA, 171, 156, 92, 122, 102, 163, 141, NA), bmi = c(30.99, 18.78, 26.52, 30.54, 22.39, 26.12, 23.69, 20.67, 26.29, 25.39, 25.68, 45.15, 21.56, 20.17, 17.38, 20.8, 22.31, 22.75, 26.62, 21.43, 19.14, 23.49, 22.66, 32.98, 25.05, 18.31, 29.13, 27.07, 20.37, 25.01, 19.69, 25.97, 18.88, 20.07, NA, 26.76, 26.97, 25.24, 20.68, 23.72, 24.82, 23.62, 18.65, 24.03, 23.86, 10.63, 23.02, 23.72, 20.82, 28.24, NA, 37.55, 18.88, 18.3, 19.13, 21.48, 22.59, 24.96, 21.63, NA, 29.35, 21.76, 17.97, 22.31, 19.27, 24.07, 22.76, NA), bmi_3cat = c(3, 1, 2, 3, 1, 2, 1, 1, 2, 2, 2, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, NA, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, NA, 3, 1, 1, 1, 1, 1, 1, 1, NA, 2, 1, 1, 1, 1, 1, 1, NA) ) %&gt;% mutate( age_group = factor(age_group, labels = c(&quot;Younger than 30&quot;, &quot;30 and Older&quot;)), gender = factor(gender, labels = c(&quot;Female&quot;, &quot;Male&quot;)), bmi_3cat = factor(bmi_3cat, labels = c(&quot;Normal&quot;, &quot;Overweight&quot;, &quot;Obese&quot;)) ) %&gt;% print() ## # A tibble: 68 x 7 ## age age_group gender ht_in wt_lbs bmi bmi_3cat ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 32 30 and Older Male 70 216 31.0 Obese ## 2 30 30 and Older Female 63 106 18.8 Normal ## 3 32 30 and Older Female 62 145 26.5 Overweight ## 4 29 Younger than 30 Male 67 195 30.5 Obese ## 5 24 Younger than 30 Female 67 143 22.4 Normal ## 6 38 30 and Older Female 58 125 26.1 Overweight ## 7 25 Younger than 30 Female 64 138 23.7 Normal ## 8 24 Younger than 30 Male 69 140 20.7 Normal ## 9 48 30 and Older Male 65 158 26.3 Overweight ## 10 29 Younger than 30 Male 68 167 25.4 Overweight ## # ‚Ä¶ with 58 more rows 16.1 Single predictor and single outcome We can describe our continuous outcome variables using the same methods we learned in previous lessons. However, this time we will use dplyr's group_by() function to calculate these statistics within subgroups of interests. For example: class_summary &lt;- class %&gt;% filter(!is.na(ht_in)) %&gt;% group_by(gender) %&gt;% summarise( n = n(), mean = mean(ht_in), `standard deviation` = sd(ht_in), min = min(ht_in), max = max(ht_in) ) %&gt;% print() ## # A tibble: 2 x 6 ## gender n mean `standard deviation` min max ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female 43 64.3 2.59 58 69 ## 2 Male 22 69.2 2.89 65 76 üëÜ Here‚Äôs what we did above: We used base R‚Äôs statistical functions inside dplyr's summarise() function to calculate the number of observations, mean, standard deviation, minimum value and maximum value of height within levels of gender. We used filter(!is.na(ht_in)) to remove all rows from the data that have a missing value for ‚Äúht_in‚Äù. If we had not done so, R would have returned a value of ‚ÄúNA‚Äù for mean, standard deviation, min, and max. Alternatively, we could have added the na.rm = TRUE option to each of the mean(), sd(), min(), and max() functions. We used group_by(gender) to calculate our statistics of interest separately within each category of the variable ‚Äúgender.‚Äù In this case, ‚ÄúFemale‚Äù and ‚ÄúMale.‚Äù You may notice that I used backticks around the variable name ‚Äústandard deviation‚Äù ‚Äì NOT single quotes. If you want to include a space in a variable name in R, you must surround it with backticks. In general, it‚Äôs a really bad idea to create variable names with spaces in them. I recommend only doing so in situations where you are using a data frame to display summary information, as we did above. Notice too that I saved our summary statistics table as data frame named ‚Äúclass_summary.‚Äù Doing so is sometimes useful, especially for plotting as we will see below. As you look over this table, you should have an idea of whether male or female students in the class appear to be taller on average, and whether male or female students in the class appear to have more dispersion around the mean value. Finally, let‚Äôs plot this data to get a feel for the relationship between gender and height graphically. class %&gt;% filter(!is.na(ht_in)) %&gt;% ggplot(aes(x = gender, y = ht_in)) + geom_jitter(aes(col = gender), width = 0.20) + geom_segment( aes(x = c(0.75, 1.75), y = mean, xend = c(1.25, 2.25), yend = mean, col = gender), size = 1.5, data = class_summary ) + scale_x_discrete(&quot;Gender&quot;) + scale_y_continuous(&quot;Height (Inches)&quot;) + scale_color_manual(values = c(&quot;#BC581A&quot;, &quot;#00519B&quot;)) + theme_classic() + theme(legend.position = &quot;none&quot;, axis.text.x = element_text(size = 12)) üëÜ Here‚Äôs what we did above: We used ggplot2 to plot each student‚Äôs height as well as the mean heights of female and male students respectively. The geom_jitter() function plots a point for each student‚Äôs height, and then makes slight random adjustments to the location of the points so that they are less likely to overlap. One of the great things about plotting our data like this is that we can quickly see if there are many more observations in one category than another. That information would be obscured if we were to use a box plot. The geom_segment() function creates the two horizontal lines at the mean values of height. Notice we used a different data frame ‚Äì class_summary ‚Äì using the data = class_summary argument to plot the mean values. We changed the x and y axis titles using the scale_x_discrete() and scale_y_continuous() functions. We changed the default ggplot colors to orange and blue (Go Gators! üêä) using the scale_color_manual() function. We simplified the plot using the theme_classic() function. theme(legend.position = \"none\", axis.text.x = element_text(size = 12)) removed the legend and increased the size of the x-axis labels a little bit. After checking both numerical and graphical descriptions of the relationship between gender and height we may conclude that male students were taller, on average, than female students. 16.2 Multiple predictors At times we may be interested in comparing continuous outcomes across levels of two or more categorical variables. As an example, perhaps we want to describe BMI by gender and age group. All we have to do is add age group to the group_by() function. class_summary &lt;- class %&gt;% filter(!is.na(bmi)) %&gt;% group_by(gender, age_group) %&gt;% summarise( n = n(), mean = mean(bmi), `standard deviation` = sd(bmi), min = min(bmi), max = max(bmi) ) %&gt;% print() ## # A tibble: 4 x 7 ## # Groups: gender [2] ## gender age_group n mean `standard deviation` min max ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female Younger than 30 35 23.1 5.41 17.4 45.2 ## 2 Female 30 and Older 8 21.8 5.67 10.6 26.8 ## 3 Male Younger than 30 19 24.6 3.69 19.7 33.0 ## 4 Male 30 and Older 2 28.6 3.32 26.3 31.0 And we can see these statistics for BMI within levels of gender separately for younger and older students. Males that are 30 and older report, on average, the highest BMI (28.6). Females age 30 and older report, on average, the lowest BMI (21.8). This is good information, but often when comparing groups a picture really is worth a thousand words. Let‚Äôs wrap up this chapter with one final plot. class %&gt;% filter(!is.na(bmi)) %&gt;% ggplot(aes(x = age_group, y = bmi)) + facet_wrap(vars(gender)) + geom_jitter(aes(col = age_group), width = 0.20) + geom_segment( aes(x = rep(c(0.75, 1.75), 2), y = mean, xend = rep(c(1.25, 2.25), 2), yend = mean, col = age_group), size = 1.5, data = class_summary ) + scale_x_discrete(&quot;Age Group&quot;) + scale_y_continuous(&quot;BMI&quot;) + scale_color_manual(values = c(&quot;#BC581A&quot;, &quot;#00519B&quot;)) + theme_classic() + theme(legend.position = &quot;none&quot;, axis.text.x = element_text(size = 10)) üëÜ Here‚Äôs what we did above: We used the same code for this plot that we used for the first height by gender plot. The only difference is that we added facet_wrap(vars(gender)) to plot males and females on separate plot panels. "],
["describing-the-relationship-between-a-categorical-outcome-and-a-categorical-predictor.html", "17 Describing the relationship between a categorical outcome and a categorical predictor 17.1 Comparing two variables", " 17 Describing the relationship between a categorical outcome and a categorical predictor Generally speaking, there is no good way to describe the relationship between a continuous predictor and a categorical outcome. So, when your outcome is categorical, the predictor must also be categorical. Therefore, any continuous predictor variables must be collapsed into categories before conducting bivariate analysis when your outcome is categorical. The best categories are those that have scientific or clinical meaning. For example, collapsing raw scores on a test of cognitive function into a categorical variable for cognitive impairment. The variable could be dichotomous (yes, no) or it could have multiple levels (no, mild cognitive impairment, dementia). Once your continuous variables are collapsed you‚Äôre ready to create n-way frequency tables that will allow you to describe the relationship between two or more categorical variables. To start with, we will once again use our previously collected class survey data. library(dplyr) library(ggplot2) class &lt;- tibble( age = c(32, 30, 32, 29, 24, 38, 25, 24, 48, 29, 22, 29, 24, 28, 24, 25, 25, 22, 25, 24, 25, 24, 23, 24, 31, 24, 29, 24, 22, 23, 26, 23, 24, 25, 24, 33, 27, 25, 26, 26, 26, 26, 26, 27, 24, 43, 25, 24, 27, 28, 29, 24, 26, 28, 25, 24, 26, 24, 26, 31, 24, 26, 31, 34, 26, 25, 27, NA), age_group = c(2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, NA), gender = c(2, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, NA), ht_in = c(70, 63, 62, 67, 67, 58, 64, 69, 65, 68, 63, 68, 69, 66, 67, 65, 64, 75, 67, 63, 60, 67, 64, 73, 62, 69, 67, 62, 68, 66, 66, 62, 64, 68, NA, 68, 70, 68, 68, 66, 71, 61, 62, 64, 64, 63, 67, 66, 69, 76, NA, 63, 64, 65, 65, 71, 66, 65, 65, 71, 64, 71, 60, 62, 61, 69, 66, NA), wt_lbs = c(216, 106, 145, 195, 143, 125, 138, 140, 158, 167, 145, 297, 146, 125, 111, 125, 130, 182, 170, 121, 98, 150, 132, 250, 137, 124, 186, 148, 134, 155, 122, 142, 110, 132, 188, 176, 188, 166, 136, 147, 178, 125, 102, 140, 139, 60, 147, 147, 141, 232, 186, 212, 110, 110, 115, 154, 140, 150, 130, NA, 171, 156, 92, 122, 102, 163, 141, NA), bmi = c(30.99, 18.78, 26.52, 30.54, 22.39, 26.12, 23.69, 20.67, 26.29, 25.39, 25.68, 45.15, 21.56, 20.17, 17.38, 20.8, 22.31, 22.75, 26.62, 21.43, 19.14, 23.49, 22.66, 32.98, 25.05, 18.31, 29.13, 27.07, 20.37, 25.01, 19.69, 25.97, 18.88, 20.07, NA, 26.76, 26.97, 25.24, 20.68, 23.72, 24.82, 23.62, 18.65, 24.03, 23.86, 10.63, 23.02, 23.72, 20.82, 28.24, NA, 37.55, 18.88, 18.3, 19.13, 21.48, 22.59, 24.96, 21.63, NA, 29.35, 21.76, 17.97, 22.31, 19.27, 24.07, 22.76, NA), bmi_3cat = c(3, 1, 2, 3, 1, 2, 1, 1, 2, 2, 2, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, NA, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, NA, 3, 1, 1, 1, 1, 1, 1, 1, NA, 2, 1, 1, 1, 1, 1, 1, NA), genhlth = c(2, 2, 3, 3, 2, 1, 2, 2, 2, 1, 3, 3, 1, 2, 2, 1, 2, NA, 3, 2, 3, 1, 2, 2, 2, 4, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 3, 3, 2, 1, 3, 3, 2, 2, 3, 3, 2, 3, 2, 2, 3, 5, 3, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 1, 2, 2, 1, 3), persdoc = c(1, 2, 2, 1, 2, 0, 0, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 2, 0, 0, 1, 1, 2, 1, 2, 0, 0, 2, 0, 0, 2, 2, 0, NA, 0, 0, 0, 2, 0, 2, NA, 0, 2, 1, 1, 1, 2, 2, 0, 0, 0, 1, 2, 1, 1, 0, 0, 0, NA) ) %&gt;% mutate( age_group = factor(age_group, labels = c(&quot;Younger than 30&quot;, &quot;30 and Older&quot;)), gender = factor(gender, labels = c(&quot;Female&quot;, &quot;Male&quot;)), bmi_3cat = factor(bmi_3cat, labels = c(&quot;Normal&quot;, &quot;Overweight&quot;, &quot;Obese&quot;)), genhlth = factor(genhlth, labels = c(&quot;Excellent&quot;, &quot;Very Good&quot;, &quot;Good&quot;, &quot;Fair&quot;, &quot;Poor&quot;)), persdoc = factor(persdoc, labels = c(&quot;No&quot;, &quot;Yes, only one&quot;, &quot;Yes, more than one&quot;)) ) %&gt;% print() ## # A tibble: 68 x 9 ## age age_group gender ht_in wt_lbs bmi bmi_3cat genhlth persdoc ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 32 30 and Older Male 70 216 31.0 Obese Very Go‚Ä¶ Yes, only o‚Ä¶ ## 2 30 30 and Older Female 63 106 18.8 Normal Very Go‚Ä¶ Yes, more t‚Ä¶ ## 3 32 30 and Older Female 62 145 26.5 Overweig‚Ä¶ Good Yes, more t‚Ä¶ ## 4 29 Younger than‚Ä¶ Male 67 195 30.5 Obese Good Yes, only o‚Ä¶ ## 5 24 Younger than‚Ä¶ Female 67 143 22.4 Normal Very Go‚Ä¶ Yes, more t‚Ä¶ ## 6 38 30 and Older Female 58 125 26.1 Overweig‚Ä¶ Excelle‚Ä¶ No ## 7 25 Younger than‚Ä¶ Female 64 138 23.7 Normal Very Go‚Ä¶ No ## 8 24 Younger than‚Ä¶ Male 69 140 20.7 Normal Very Go‚Ä¶ Yes, only o‚Ä¶ ## 9 48 30 and Older Male 65 158 26.3 Overweig‚Ä¶ Very Go‚Ä¶ Yes, more t‚Ä¶ ## 10 29 Younger than‚Ä¶ Male 68 167 25.4 Overweig‚Ä¶ Excelle‚Ä¶ No ## # ‚Ä¶ with 58 more rows 17.1 Comparing two variables We‚Äôve already used R to create one-way descriptive tables for categorical variables. One-way frequency tables can be interesting in their own right; however, most of the time we are interested in the relationships between two variables. For example, think about when we looked at mean height within levels of gender. This told us something about the relationship between height and gender. While far from definite, our little survey provides some evidence that women, on average, are shorter than men. Well, we can describe the relationship between two categorical variables as well. One way of doing so is with two-way frequency tables, which are also sometimes referred to as crosstabs or contingency tables. Let‚Äôs start by simply looking at an example. Below we use the same CrossTable() function that we used in the lesson on univariate analysis of categorical data. The only difference is that we pass two vectors to the function instead of one. The first variable will always form the rows, and the second variable will always form the columns. In other words, we can say that we are creating a two-way table of persdoc by genhealth. df &lt;- filter(class, !is.na(bmi_3cat)) # Drop rows with missing bmi gmodels::CrossTable(df$persdoc, df$genhlth) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 61 ## ## ## | df$genhlth ## df$persdoc | Excellent | Very Good | Good | Fair | Poor | Row Total | ## -------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## No | 4 | 9 | 8 | 0 | 0 | 21 | ## | 0.090 | 0.097 | 0.180 | 0.344 | 0.344 | | ## | 0.190 | 0.429 | 0.381 | 0.000 | 0.000 | 0.344 | ## | 0.400 | 0.310 | 0.400 | 0.000 | 0.000 | | ## | 0.066 | 0.148 | 0.131 | 0.000 | 0.000 | | ## -------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## Yes, only one | 4 | 12 | 6 | 1 | 0 | 23 | ## | 0.014 | 0.104 | 0.315 | 1.029 | 0.377 | | ## | 0.174 | 0.522 | 0.261 | 0.043 | 0.000 | 0.377 | ## | 0.400 | 0.414 | 0.300 | 1.000 | 0.000 | | ## | 0.066 | 0.197 | 0.098 | 0.016 | 0.000 | | ## -------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## Yes, more than one | 2 | 8 | 6 | 0 | 1 | 17 | ## | 0.222 | 0.001 | 0.033 | 0.279 | 1.867 | | ## | 0.118 | 0.471 | 0.353 | 0.000 | 0.059 | 0.279 | ## | 0.200 | 0.276 | 0.300 | 0.000 | 1.000 | | ## | 0.033 | 0.131 | 0.098 | 0.000 | 0.016 | | ## -------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## Column Total | 10 | 29 | 20 | 1 | 1 | 61 | ## | 0.164 | 0.475 | 0.328 | 0.016 | 0.016 | | ## -------------------|-----------|-----------|-----------|-----------|-----------|-----------| ## ## Ok, let‚Äôs walk through this output together‚Ä¶ Think of little box labeled ‚ÄúCell Contents‚Äù as a legend that tells you how to interpret the rest of the boxes. Reading from top to bottom, the first number you encounter in a box will be the frequency or count of observations (labeled N). The second number you encounter will be the chi-square contribution. Please ignore that number for now. The third number will be the row proportion. The fourth number will be the column proportion. And the fifth number will be the overall proportion. Reading the table of summary statistics from top to bottom, the row headers describe categories of persdoc, which are one, only one, and more than one. Reading from left to right, the column headers describe categories of genhealth, which are excellent, very good, good, fair, and poor. The bottom row gives the total frequency and proportion of observations that fall in each of the categories defined by the columns. For example, 10 students ‚Äì about 0.164 of the entire class ‚Äì reported being in excellent general health. The far-right column gives the total frequency and proportion of observations that fall in each of the categories defined by the rows. For example, 23 students ‚Äì about 0.377 of the entire class ‚Äì reported that they have exactly one person that they think of as their personal doctor or healthcare provider. And the bottom right corner gives the overall total frequency of observations in the table. Together, the last row, the far-right column, and the bottom right cell make up what are called the marginal totals because they are on the outer margin of the table. Next, let‚Äôs interpret the data contained in the first cell with data. The first number is the frequency. There are 4 students that do not have a personal doctor and report being in excellent health. The third number is the row proportion. The row this cell is in is the No row, which includes 21 students. Out of the 21 total students in the No row, 4 reported being in excellent health. 4 divided by 21 is 0.190. Said another way, 19% of students with no personal doctor reported being in excellent health. The fourth number is the column proportion. This cell is in the Excellent column. Of the 10 students in the Excellent column, 4 reported that they do not have a personal doctor. 4 out of 10 is 0.4. Said another way, 40% of students who report being in excellent health have no personal doctor. The last number is the overall proportion. So, 4 out of the 61 total students in this analysis have no personal doctor and report being in excellent health. Four out of 61 is 0.066. So, about 7% of all the students in the class have no personal doctor and are in excellent health. Now that you know how to read the table, I want to point out a couple subtleties that may not have jumped out at you above. The changing denominator. As we moved from the row proportion to the column proportion and then the overall proportion, all that changed was the denominator (the blue circle). And each time we did so we were describing the characteristics of a different group of people: (1) students without a personal doctor, (2) students in excellent general health, (3) all students ‚Äì regardless of personal doctor or general health. Language matters. Because we are actually describing the characteristics of different subgroups, the language we use to interpret our results is important. For example, when I interpreted the row proportion above I wrote, ‚Äú19% of students with no personal doctor reported being in excellent health.‚Äù This language implies that I‚Äôm describing the health (characteristic) of students with no personal doctor (subgroup). It would be completely incorrect to instead say, ‚Äú19% of students in excellent health have no personal doctor‚Äù or ‚Äú19% of students have no personal doctor.‚Äù Those are interpretations of the column percent and overall percent respectively. They are not interchangeable. "],
["creating-tables-with-r-and-microsoft-word.html", "18 Creating tables with R and Microsoft Word 18.1 Table 1 18.2 Opioid drug use 18.3 Table columns 18.4 Table rows 18.5 Make the table skeleton 18.6 Fill in column headers 18.7 Fill in row headers 18.8 Fill in data values 18.9 Fill in title 18.10 Fill in footnotes 18.11 Final formatting 18.12 Summary", " 18 Creating tables with R and Microsoft Word At this point, you should all know that it is generally a bad idea to submit raw R output as part of a report, presentation, or publication. You should also understand when it is most appropriate to use tables, as opposed to charts and graphs, to present your results. If not, please stop here and read Chapter 7 of Successful Scientific Writing, which discusses the ‚Äúwhy‚Äù behind much of what I will show you ‚Äúhow‚Äù to do in this chapter. 6 R for Epidemiology is predominantly a book about using R to manage, visualize, and analyze data in ways that are common in the field epidemiology. However, in most modern work/research environments it is difficult to escape the requirement to share your results in a Microsoft Word document. And often, because we are dealing with data, those results include tables of some sort. However, not all tables communicate your results equally well. In this chapter, I will walk you through the process of starting with some results you calculated in R and ending with a nicely formatted table in Microsoft Word. Specifically, we are going to create a Table 1. 18.1 Table 1 In epidemiology, medicine, and other disciplines, ‚ÄúTable 1‚Äù has a special meaning. Yes, it‚Äôs the first table shown to the reader of your article, report, or presentation, but the special meaning goes beyond that. In many disciplines, including epidemiology, when you speak to a colleague about their ‚ÄúTable 1‚Äù it is understood that you are speaking about a table that describes (statistically) the relevant characteristics of the sample being studied. Often, but not always, the sample being studied is made up of people, and the relevant descriptive characteristics about those people include sociodemographic and/or general health information. Therefore, it is important that you don‚Äôt label any of your tables as ‚ÄúTable 1‚Äù arbitrarily. Unless you have a really good reason to do otherwise, your Table 1 should always be a descriptive overview of your sample. Here is a list of other traits that should consider when creating your Table 1: All other formatting best practices that apply to scientific tables in general. This includes formatting requirements specific to wherever you are submitting your table (e.g., formatting requirements in the American Journal of Public Health). Table 1 is often, but not always, stratified into subgroups (i.e., descriptive results are presented separately for each subgroup of the study sample in a way that lends itself to each between-group comparisons). When Table 1 is stratified into subgroups, the variable that contains the subgroups is typically the primary exposure/predictor of interest in your study. 18.2 Opioid drug use As a motivating example, let‚Äôs say that we are working at the North Texas Regional Health Department and have been asked to create a report about drug use in our region. Our stakeholders are particularly interested in opioid drug use. To create this report, we will analyze data from a sample of 9,985 adults who were asked about their use of drugs. One of the first analyses that we did was a descripitive comparison of the sociodemographic characteristics of 3 subgroups of people in our data. Will will use these analyses to create our Table 1. üóíSide Note: This data includes over 9,000 rows, so I don‚Äôt show the code for creating a data frame from this data. In fact, I directly imported this data into R from a comma separated values (.csv) file. We haven‚Äôt yet learned how to import csv files, but you can view/download the data by clicking here if you‚Äôd like to check it out anyway. ## var cat n n_total percent se t_crit lcl ## 1 use_f Non-users 8315 9985 83.274912 0.3734986 1.960202 82.52992 ## 2 use_f Use other drugs 1532 9985 15.343015 0.3606903 1.960202 14.64925 ## 3 use_f Use opioid drugs 138 9985 1.382073 0.1168399 1.960202 1.17080 ## ucl ## 1 83.994296 ## 2 16.063453 ## 3 1.630841 ## var use n mean sd t_crit sem lcl ucl ## 1 age 0 8315 36.80173 9.997545 1.960249 0.10963828 36.58681 37.01665 ## 2 age 1 1532 21.98362 2.979511 1.961515 0.07612296 21.83431 22.13294 ## 3 age 2 138 17.34740 3.081049 1.977431 0.26227634 16.82877 17.86603 ## row_var row_cat col_var col_cat n n_row n_total percent_total se_total ## 1 use 0 female_f No 3077 8315 9985 30.8162243 0.46210382 ## 2 use 0 female_f Yes 5238 8315 9985 52.4586880 0.49979512 ## 3 use 1 female_f No 796 1532 9985 7.9719579 0.27107552 ## 4 use 1 female_f Yes 736 1532 9985 7.3710566 0.26150858 ## 5 use 2 female_f No 91 138 9985 0.9113671 0.09510564 ## 6 use 2 female_f Yes 47 138 9985 0.4707061 0.06850118 ## t_crit_total lcl_total ucl_total percent_row se_row t_crit_row lcl_row ## 1 1.960202 29.9178650 31.7293461 37.00541 0.5295162 1.960202 35.97359 ## 2 1.960202 51.4781679 53.4373162 62.99459 0.5295162 1.960202 61.95076 ## 3 1.960202 7.4565108 8.5197562 51.95822 1.2768770 1.960202 49.45247 ## 4 1.960202 6.8745702 7.9003577 48.04178 1.2768770 1.960202 45.54583 ## 5 1.960202 0.7426382 1.1179996 65.94203 4.0488366 1.960202 57.62323 ## 6 1.960202 0.3538217 0.6259604 34.05797 4.0488366 1.960202 26.61786 ## ucl_row ## 1 38.04924 ## 2 64.02641 ## 3 54.45417 ## 4 50.54753 ## 5 73.38214 ## 6 42.37677 ## row_var row_cat col_var col_cat n n_row n_total ## 1 use 0 edu_f Less than high school 3908 8315 9985 ## 2 use 0 edu_f High school 2494 8315 9985 ## 3 use 0 edu_f Some college 915 8315 9985 ## 4 use 0 edu_f College graduate 998 8315 9985 ## 5 use 1 edu_f Less than high school 322 1532 9985 ## 6 use 1 edu_f High school 567 1532 9985 ## 7 use 1 edu_f Some college 321 1532 9985 ## 8 use 1 edu_f College graduate 322 1532 9985 ## 9 use 2 edu_f Less than high school 36 138 9985 ## 10 use 2 edu_f High school 36 138 9985 ## 11 use 2 edu_f Some college 40 138 9985 ## 12 use 2 edu_f College graduate 26 138 9985 ## percent_total se_total t_crit_total lcl_total ucl_total percent_row ## 1 39.1387081 0.48845160 1.960202 38.1855341 40.1002400 46.99940 ## 2 24.9774662 0.43322925 1.960202 24.1379137 25.8362747 29.99399 ## 3 9.1637456 0.28874458 1.960202 8.6132458 9.7456775 11.00421 ## 4 9.9949925 0.30017346 1.960202 9.4217947 10.5989817 12.00241 ## 5 3.2248373 0.17680053 1.960202 2.8957068 3.5899942 21.01828 ## 6 5.6785178 0.23161705 1.960202 5.2411937 6.1499638 37.01044 ## 7 3.2148222 0.17653492 1.960202 2.8862151 3.5794637 20.95300 ## 8 3.2248373 0.17680053 1.960202 2.8957068 3.5899942 21.01828 ## 9 0.3605408 0.05998472 1.960202 0.2601621 0.4994549 26.08696 ## 10 0.3605408 0.05998472 1.960202 0.2601621 0.4994549 26.08696 ## 11 0.4006009 0.06321673 1.960202 0.2939655 0.5457064 28.98551 ## 12 0.2603906 0.05100282 1.960202 0.1773397 0.3821865 18.84058 ## se_row t_crit_row lcl_row ucl_row ## 1 0.5473707 1.960202 45.92799 48.07358 ## 2 0.5025506 1.960202 29.01822 30.98824 ## 3 0.3432094 1.960202 10.34925 11.69521 ## 4 0.3564220 1.960202 11.32112 12.71881 ## 5 1.0412961 1.960202 19.04975 23.13209 ## 6 1.2339820 1.960202 34.62587 39.46012 ## 7 1.0401075 1.960202 18.98696 23.06466 ## 8 1.0412961 1.960202 19.04975 23.13209 ## 9 3.7515606 1.960202 19.42163 34.07250 ## 10 3.7515606 1.960202 19.42163 34.07250 ## 11 3.8761776 1.960202 22.00774 37.12261 ## 12 3.3408449 1.960202 13.13952 26.26721 Above, we have the results of several different descriptive analyses we did in R. Remember that we never want to present raw R output. Perhaps you‚Äôve already thought to yourself, ‚Äúwow, these results are really overwhelming. I‚Äôm not sure what I‚Äôm even looking at.‚Äù Well, that‚Äôs exactly how many of the people in your audience will feel as well. In its current form, this information is really hard for us to process. We want to take some of the information from the output above and use it to create a Table 1 in Word that is much easier to read. Specifically, we want our final Table 1 to look like this: You may also click here to view/download the Word file that contains the Table 1. Now that you‚Äôve seen the end result, let‚Äôs learn how to make this Table 1 together, step-by-step. Go ahead and open Microsoft Word now if you want to follow along. 18.3 Table columns The first thing I typically do is figure out how many columns and rows my table will need. This is generally pretty straightforward; although, there are exceptions. For a basic Table 1 like the one we are creating above we need the following columns: One column for our row headers (i.e., the names and categories of the variables we are presenting in our analysis). One column for each subgroup that we will be describing in our table. In this case, there are 3 subgroups so we will need 3 additional columns. So, we will need 4 total columns. üóíSide Note: If you are going to describe the entire sample overall without stratifying it into subgroups then you would simply have 2 columns. One for the row headers and one for the values. 18.4 Table rows Next, we need to figure out how many rows our table will need. This is also pretty straightforward. Generally, we will need the following rows: One row for the title. Some people write their table titles outside (above or below) the actual table. I like to include the title directly in the top row of the table. That way, it moves with the table if the table gets moved around. One row for the column headers. The column headers generally include a label like ‚ÄúCharacteristic‚Äù for the row headers column and a descriptive label for each subgroup we are describing in our table. One row for each variable we will analyze in our analysis. In this example, we have three ‚Äì age, sex, and education. NOTE that we do NOT need a separate row for each category of each variable. One row for the footer. So, we will need 6 total rows. 18.5 Make the table skeleton Now that we know we need to create a table with 4 columns and 6 rows, lets go ahead and do that in Microsoft Word. We do so by clicking the Insert tab in the ribbon above our document. Then, we click the Table button and select the number of columns and rows we want. 18.6 Fill in column headers Now we have our table skeleton. The next thing I would typically do is fill in the column headers. Remember that our column headers look like this: Here are a couple of suggestions for filling in your column headers: Put your column headers in the second row of the empty table shell. The title will eventually go into the first row. I don‚Äôt add the title right away because it is typically long and will distort the table‚Äôs dimensions. Later, we will see how to horizontally merge table cells to remove this distortion, but we don‚Äôt want to do that now. Right now, we want to leave all the cells unmerged so that we can easily resize our columns. The first column header is generally a label for our row headers. Because the rows are typically characteristics of our sample, I almost always use the word ‚Äúcharacteristic‚Äù here. If you come up with a better word, please feel free to use it. The rest of the column headers are generally devoted to the subgroups we are describing. The subgroups should be ordered in a way that is meaningful. For example, by level of severity or chronological order. Typically, ordering in alphabetical order isn‚Äôt that meaningful. The subgroup labels should be informative and meaningful, but also succinct. This can sometimes be a challenge. I have seen terms like ‚ÄúValue‚Äù, ‚ÄúAll‚Äù, and ‚ÄúFull Sample‚Äù used when Table 1 was describing the entire sample overall rather than describing the sample by subgroups. 18.6.1 Group sample sizes You should always include the group sample size in the column header. They should typically be in the format ‚Äú(n = sample size)‚Äù and typed in the same cell as the label, but below the label (i.e., hit the return key). The group sample sizes can often provide important context to the statistics listed below in the table, and clue the reader into missing data issues. 18.6.2 Formatting column headers I generally bold my column headers, horizontally center them, and vertically align them to the bottom of the row. At this point, your table should look like this in Microsoft Word: 18.7 Fill in row headers The next thing I would typically do is fill in the row headers. Remember, that our row headers look like this: Here are a couple of suggestions for filling in your row headers: The variables should be organized in a way that is meaningful. In our example, we have only 3 sociodemographic variables. However, if we also had some variables about health status and some variables related to criminal history, then we would almost certainly want the variables that fit into each of these categories to be vertically arranged next to each other. Like the column headers, the row headers should be informative and meaningful, but also succinct. Again, this can sometimes be a challenge. In our example, we use ‚ÄúAge‚Äù, ‚ÄúSex‚Äù, and ‚ÄúEducation‚Äù. Something like ‚ÄúHighest level of formal education completed‚Äù would have also been informative and meaningful, but not succinct. Something like ‚ÄúQuestion 6‚Äù is succinct, but isn‚Äôt informative or meaningful at all. 18.7.1 Label statistics You should always tell the reader what kind statistics they are looking at ‚Äì don‚Äôt assume that they know. For example, the highlighted number in figure 18.1 are 36.8 and 10. What is 36.8? The mean, the median? The percentage of people who had a non-missing value for age? What is 10? The sample size? The standard error of the mean? An odds ratio? You know that 36.8 is a mean and 10 is the standard deviation because I identified what they were in row header. 18.2 When you label the statistics in the row headers as we‚Äôve done in our example, they should take the format you see in figure 18.2. That is, the variable name, followed by a comma, followed by the statistics used in that row. Also notice the use of parentheses. We used parentheses around the letters ‚Äúsd‚Äù (for standard deviation) because the numbers inside the parentheses in that row are standard deviations. So, the label used to identify the statistics should give the reader a blueprint for interpreting the statistics that matches the format of the statistics themselves. Figure 18.1: What are these numbers. Figure 18.2: Identifying statistics in the row header. The statistics can, and sometimes are, labeled in the column header instead of the row header. This can sometimes be a great idea. However, it can also be a source of confusion. For example, in the figure below, the column headers include labels (i.e., n (%)) for the statistics below. However, not all the statistics below are counts (n) and percentages! Even though the Age variable has it‚Äôs own separate statistics label in the row header, this is still generally a really bad idea! Therefore, I highly recommend only labeling your statistics in the column header when those labels are accurate for every value in the column. For example: 18.7.2 Formatting row headers Whenever possible, make sure that variable name and statistic identifier fit on one line (i.e., they don‚Äôt carryover into the line below). Always type the category labels for categorical variables in the same cell as the variable name. However, each category should have it‚Äôs own line (i.e., hit the return key). Whenever possible, make sure that each category label fits on one line (i.e., it doesn‚Äôt carryover into the line below). Indent each category label two spaces to the left of the variable name. Hit the return key once after the last category for each categorical variable. This creates a blank line that adds vertical separation between row headers and makes them easier to read. At this point, your table should look like this in Microsoft Word: 18.8 Fill in data values So, we have some statistics visible to us on the screen in RStudio. Somehow, we have to get those numbers over to our table in Microsoft Word. There are many different ways we can do this. I‚Äôm going to compare a few of those ways here. 18.8.1 Manually type values One option is to manually type the numbers into your word document. üëç If you are in a hurry, or if you just need to update a small handful of statistics in your table, then this option is super straightforward. However, there are at least two big problems with this method. üëé First, it is extremely error prone. Most people are very likely to type a wrong number or misplace a decimal here and there when they manually type statistics into their Word tables. üëé Second, it isn‚Äôt very scalable. What if you need to make very large tables with lots and lots of numbers? What if you update your data set and need to change every number in your Word table? This is not fun to do manually. 18.8.2 Copy and paste values Anther option is to copy and paste values from RStudio into Word. This option is similar to above, but instead of typing each value into your Word table, you highlight and copy the value in RStudio and paste it into Word. üëç If you are in a hurry, or if you just need to update a small handful of statistics in your table, then this option is also pretty straightforward. However, there are still issues associated with this method. üëé First, it is still somewhat error prone. It‚Äôs true that the numbers and decimal placements should always be correct when you copy and paste; however, you may be surprised by how often many people accidently paste the values into the wrong place or in the wrong order. üëé Second, I‚Äôve noticed that there are often weird formatting things that happen when I copy from RStudio and paste into Word. They are usually pretty easy to fix, but this is still a small bit of extra hassle. üëé Third, it isn‚Äôt very scalable. Again, if we need to make very large tables with lots and lots of numbers or update our data set and need to change every number in your Word table, this method is time-consuming and tedious. 18.8.3 Knit a Word document So far, we have only used the HTML Notebook output type for our R markdown files. However, it‚Äôs actually very easy have RStudio create a Word document from you R markdown files. We don‚Äôt have all the R knowledge we need to fully implement this method yet, so I don‚Äôt want to confuse you by going into the details here. But, I do want to mention that it is possible. üëç The main advantages of this method are that it is much less error prone and much more scalable than manually typing or copying and pasting values. üëé The main disadvantages are that it requires more work on the front end and still requires you to open Microsoft Word a do a good deal of formatting of the table(s). 18.8.4 flextable and officer A final option I‚Äôll mention is to create your table with the flextable and officer packages. This is my favorite option, but it is also definitely the most complicated. Again, I‚Äôm not going to go into the details here because they would likely just be confusing for most readers. üëç This method essentially overcomes all of the previous methods‚Äô limitations. It is the least error prone, it is extremely scalable, and it allows us to do basically all the formatting in R. With a push of a button we have a complete, perfectly formatted table output to a Word document. If we update our data, we just push the button again and we have a new perfectly formatted table. üëé The primary downside is that this method requires you to invest some time in learning these packages, and requires the greatest amount of writing code up front. If you just need to create a single small table that you will never update, this method is probably not worth the effort. However, if you absolutely need to make sure that your table has no errors, or if you will need to update your table on a regular basis, then this method is definitely worth learning. 18.8.5 Significant digits No matter which of the methods above you choose, you will almost never want to give your reader the level of precision that R will give you. For example, the first row of the R results below indicates that 83.274912% of our sample reported that they don‚Äôt use drugs. ## var cat n n_total percent se t_crit lcl ## 1 use_f Non-users 8315 9985 83.274912 0.3734986 1.960202 82.52992 ## 2 use_f Use other drugs 1532 9985 15.343015 0.3606903 1.960202 14.64925 ## 3 use_f Use opioid drugs 138 9985 1.382073 0.1168399 1.960202 1.17080 ## ucl ## 1 83.994296 ## 2 16.063453 ## 3 1.630841 Notice the level of precision there. R gives us the percentage out to 6 decimal places. If you fill your table with numbers like this, it will be much harder for your readers to digest your table and make comparisons between groups. It‚Äôs just the way our brains work. So, the logical next question is, ‚Äúhow many decimal places should I report?‚Äù Unfortunately, this is another one of those times that I have to give you an answer that may be a little unsatisfying. It is true that there are rules for significant figures (significant digits); however, the rules are not always helpful to students in my experience. Therefore, I‚Äôm going to share with you a few things I try to consider when deciding how many digits to present. I don‚Äôt recall ever presenting a level of precision greater than 3 decimal places in the research I‚Äôve been involved with. If you are working in physics or genetics and measuring really tiny things it may be totally valid to report 6, 8, or 10 digits to the right of the decimal. But, in epidemiology ‚Äì a population science ‚Äì this is rarely, if ever, useful. What is the overall message I am trying to communicate? That is the point of the table, right? I‚Äôm trying to clearly and honestly communicate information to my reader. In general, the simpler the numbers are to read and compare, the clearer the communication. So, I tend to error on the side of simplifying as much as possible. For example, in the R results below, we could say that 83.274912% of our sample reported that they don‚Äôt use drugs, 15.343015% reported that they use drugs other than opioids, and 1.382073% reported that they use opioid drugs. Is saying it that way really more useful than saying that ‚Äú83% of our sample reported that they don‚Äôt use drugs, 15% reported that they use drugs other than opioids, and 1% reported that they use opioid drugs‚Äù? Are we missing any actionable information by rounding our percentages to the nearest integer here? Are our overall conclusions about drug use any different? No, probably not. And, the rounded percentages are much easier to read, compare, and remember. Be consistent ‚Äì especially within a single table. I have experienced some rare occasions where it made sense to round one variable to 1 decimal place and another variable to 3 decimals places in the same table. But, circumstances like this are definitely the exception. Generally speaking, if you round one variable to 1 decimal place then you want to round them all to one decimal place. Like all other calculations we‚Äôve done in this book, I suggest you let R do the heavy lifting when it comes to rounding. In other words, have R round the values for you before you move them to Word. R is much less likely to make a rounding error than your are! You may recall that we learned how to round in the chapter on numerical descriptions of categorical variables. 18.8.6 Formatting data values Now that we have our appropriately rounded values in our table, we just need to do a little formatting before we move on. First, make sure to fix any fonts, font sizes, and/or background colors that may have been changed if you copied and pasted the values from RStudio into Word. Second, make sure the values line up horizontally with the correct variable names and category labels. Third, I tend to horizontally center all my values in their columns. At this point, your table should look like this in Microsoft Word: 18.9 Fill in title At this point in the process, I will typically go ahead and add the title to the first cell of my Word table. The title should always start with ‚ÄúTable #.‚Äù In our case, it will start with ‚ÄúTable 1.‚Äù In general, I use bold text for this part of the title. What comes next will change a little bit from table to table but is extremely important and worth putting some thought into. Remember, all tables and figures need to be able to stand on their own. What does that mean? It means that if I pick up your report and flip straight to the table, I should be able to understand what it‚Äôs about and how to read it without reading any of the other text in your report. The title is a critical part of making a table stand on its own. In general, your title should tell the reader what the table contains (e.g., sociodemographic characteristics) and who the table is about (e.g., results of the Texas Opioid Study). I will usually also add the size of the sample of people included in the table (e.g., n = 9985) and the year the data was collected (e.g., 2020). In different circumstances, more or less information may be needed. However, always ask yourself, ‚Äúcan this table stand on its own? Can most readers understand what‚Äôs going on in this table even if they didn‚Äôt read the full report?‚Äù At this point, your table should look like this in Microsoft Word: Don‚Äôt worry about your title being all bunched up in the corner. We will fix it soon. 18.10 Fill in footnotes Footnotes are another tool we can use to help our table stand on its own. The footnotes give readers additional information that they may need to read and understand our table. Again, there are few hard and fast rules regarding what footnotes you should include, but I can give you some general categories of things to think about. First, use footnotes to explain any abbreviations in your table that aren‚Äôt standard and broadly understood. These abbreviations are typically related to statistics used in the table (e.g., RR = risk ratio) and/or units of measure (e.g., mmHg = millimeters of mercury). Admittedly, there is some subjectivity associated with ‚Äústandard and broadly understood.‚Äù In our example, I did not provide a footnote for ‚Äún‚Äù, ‚Äúsd‚Äù, or ‚Äú%‚Äù because most researchers would agree that these abbreviations are standard and broadly understood, but I typically do provide footnotes for statistics like ‚ÄúOR‚Äù (odds ration) and ‚ÄúRR‚Äù (relative risk or risk ratio). Additionally, I mentioned above that it is desirable, but sometimes challenging, to get your variable names and category labels to fit on a single line. Footnotes can sometimes help with this. In our example, instead of writing ‚ÄúAge in years, mean (sd)‚Äù as a row header I wrote ‚ÄúAge, mean (sd)‚Äù and added a footnote that tells the reader that age is measured in years. This may not be the best possible example, but hopefully you get the idea. 18.10.1 Formatting footnotes When using footnotes, you need to somehow let the reader know which element in the table each footnote goes with. Sometimes, there will be guidelines that require you to use certain symbols (e.g., *, ‚Ä†, and ‚Ä°), but I typically use numbers to match table elements to footnotes when I can. In the example below, there is a superscript ‚Äú1‚Äù immediately after the word ‚ÄúAge‚Äù that lets the reader know that footnote number 1 is adding additional information to this part of the table. If you do use numbers to match table elements to footnotes, make sure you do so in the order people read [English], which is left to right and top to bottom. For example, the following would be inappropriate because the number 2 comes before the number 1 when reading from top to bottom: As another example, the following would be inappropriate because the number 2 comes before the number 1 when reading from left to right: Additionally, when using numbers to match table elements to footnotes, it‚Äôs a good idea to superscript the numbers in the table. This makes it clear that the number is being used to identify a footnote rather than being part of the text or abbreviation. Formatting a number as a superscript is easy in Microsoft Word. Just highlight the number you want to format and click the superscript button like so: At this point, your table should look like this in Microsoft Word: 18.11 Final formatting We have all of our data and explanatory text in our table. The last few remaining steps are just about formatting our table to make it as easy to read and digest as possible. 18.11.1 Adjust column widths As I‚Äôve already mentioned more than once, we don‚Äôt want our text carryover onto multiple lines whenever we can help it. In my experience, this occurs most often in the row headings. Therefore, I will often need to adjust (widen) the first column of my table. You can do that by clicking on the black border that separates the columns and moving your mouse left or right. After you adjust the width of your first column, the widths of the remaining columns will likely be uneven. To distribute the remaining space in the table evenly among the remaining columns, first select the columns by clicking immediately above the first column you want to select and dragging your cursor across the remaining columns. Then, click the layout tab in ribbon above your document and the Distribute Columns button. In our particular example, there was no need to adjust column widths because all of our text fit into the default widths. 18.11.2 Merge cells Now, we can finally merge some cells so that our title and footnote spread the entire width of the table. We waited until now to merge cells because if we had done so earlier it would have made the previous step (i.e., adjust column widths) more difficult. To spread our title out across the entire width of the table, we just need to will select all the cells in the first row, then right click and select merge cells. After merging the footnote cells in exactly the same way, your table should look like this: 18.11.3 Remove cell borders The final step is to clean up our borders. In my experience, students like to do all kinds of creative things with cell borders. However, when it comes to borders, keeping it simple is usually the best approach. Therefore, we will start by removing all borders in the table. We do so by clicking the little cross with arrowheads that pops up diagonal to the top-left corner of the table when you move your mouse over it. Clicking this button will highlight your entire table. Then, we will click the downward facing arrow next to the borders button in the ribbon above your document. Then, we will click the No Border option. Our final step will be to add a single horizontal border under the title, as single horizontal border under the column header row, and a single horizontal border above the footnotes. We will add the borders by highlighting the correct rows and selecting the correct options for the same borders dropdown menu we used above. Notice that there are no vertical lines (borders) anywhere on our table. That should almost always be the case for your tables too. 18.12 Summary Just like with guidelines we‚Äôve discussed about R coding style; you don‚Äôt have to create tables in exactly the same way that I do. But, you should have a good reason for all the decisions you make leading up to the finished table, and you should apply those decisions consistently across all your tables within a given project or report. Having said that, in the absence of needing to adhere to specific guidelines that conflict with the table we‚Äôve created above, this is the general template I would ask someone working on my team to use when creating a table for a report or presentation. References "],
["appendix-a-glossary.html", "Appendix A: Glossary", " Appendix A: Glossary Console. Coming soon. Data frame. For our purposes, data frames are just R‚Äôs term for data set or data table. Data frames are made up of columns (variables) and rows (observations). In R, all columns of a data frame must have the same length. Functions. Coming soon. Arguments: Arguments always go inside the parentheses of a function and give the function the information it needs to give us the result we want. Pass: In programming lingo, you pass a value to a function argument. For example, in the funtion call seq(from = 2, to = 100, by = 2) we could say that we passed a value of 2 to the from argument, we passed a value of 100 to the to argument, and we passed a value of 2 to the by argument. Returns: Instead of saying, ‚Äúthe seq() function gives us a sequence of numbers‚Ä¶‚Äù we could say, ‚Äúthe seq() function returns us a sequence of numbers‚Ä¶‚Äù In programming lingo, functions return one or more results. Global environment. Coming soon. Objects. Coming soon. R. R is an integrated suite of software facilities for data manipulation, calculation and graphical display. R is very much a vehicle for newly developing methods of interactive data analysis. It has developed rapidly and has been extended by a large collection of packages. However, most programs written in R are essentially ephemeral, written for a single piece of data analysis. 7 R markdown documents. R markdown documents are text files that can be used to clean and analyze your data interactively as well as share your final results in many different formats (e.g., Microsoft Word, PDF, and even websites). R markdown documents weave together R code, narrative text, and multimedia content together into a polished final product. 8 RStudio. RStudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management. RStudio is available in open source and commercial editions and runs on the desktop (Windows, Mac, and Linux) or in a browser connected to RStudio Server or RStudio Server Pro (Debian/Ubuntu, Red Hat/CentOS, and SUSE Linux). 9 "]
]
