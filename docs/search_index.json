[["measures-of-association.html", "44 Measures of Association 44.1 Exposures and outcomes 44.2 Contingency tables 44.3 Building contingency tables in R 44.4 Probabilities 44.5 Prediction 44.6 Terminology recap 44.7 Incidence proportion 44.8 Incidence proportion ratio", " 44 Measures of Association This chapter is under heavy development and may still undergo significant changes. As a reminder, we said that epidemiology is usually defined as something like, ‚Äúthe study of the occurrence and distribution of health-related states or events in specified populations, including the study of the determinants influencing such states, and the application of this knowledge to control the health problems‚Äù1 in the Introduction to Epidemiology chapter. In the chapter about measures of occurrence, we focused on some of the ways we can measure the occurrence of those health-related states or events. In this chapter, we will start discussing ways we can measure the distribution of those health-related states or events. In other words, are those health-related states or events equally likely for everyone, or are they more for some people than others. However, we first need to review a couple of new topics that will help us later in our discussion about measures of association. üóíSide Note: Writing (and reading) ‚Äúhealth-related states or events‚Äù over and over starts to get cumbersome after a while. Therefore, we will often just use ‚Äúconditions‚Äù or ‚Äúevents‚Äù in the text below instead. Further, the methods we discuss below can just as easily be applied to better understanding characteristics of populations that would not typically be described as a ‚Äúcondition‚Äù or ‚Äúevent‚Äù. For example, sex assigned at birth or ethnicity. 44.1 Exposures and outcomes Our discussion of associations will inevitably lead us to to compare two or more conditions in our population of interest. For example, we might want to compare diabetes and state of residence to better understand if diabetes is more common in some states than others. It is commonplace to refer to one of the conditions we are comparing as the outcome and the other condition(s) as the exposure(s). For example, we might refer to diabetes as the outcome and state of residence as the exposure. By extension, the members of the population who are living with the exposure(s) are commonly referred to as the exposed group, or simply the exposed. In epidemiology, the terms exposure and outcome are used all of the time. Both terms can refer to pathogens, environmental conditions, behaviors, traits, social conditions, diseases or health conditions, and/or treatments or interventions. Table 44.1 below contains some examples. Table 44.1: Examples of exposures and outcomes. Term Examples Pathogens Tapeworm, Malaria, Ringworm, Streptococcus, and HPV Environmental condtions Smoking, Physical Activity, Sleep, and Sexual behaviors Behaviors Genotype and Sex assigned at birth Social conditions Discrimination, Neighborhood characteristics, and Culture Diseases or condtions Diabetes, Heart disease, and Cancers Treatments or interventions Medications, Smoking cessation programs, and Laws and policies So, what distinguishes an exposure from an outcome? Generally speaking, we do. When we are exploring the relationship between two variables in our data, it is customary to call the first variable an exposure if we believe it is the cause of the second variable, and to call the second variable an outcome if we believe it is caused by the first variable. We haven‚Äôt formally defined causes and effects yet, and we won‚Äôt until the part of the book about causal inference, but our intuitive/colloquial understanding of cause and effect should be sufficient for now. With that in mind, virtually any of the examples from table 44.1 could reasonably be thought of as an exposure or an outcome depending on the question we are trying to answer, and our beliefs about how causes and effects work in the world around us. For example, if we believe that unprotected sex is a cause of HPV infection, then we may want to measure the association between unprotected sex (an exposure) and HPV infection (an outcome). However, we could also believe that HPV infection is a cause of cervical cancer. In that case, we may want to measure the association between HPV infection (an exposure) and the existence of cervical cancer (an outcome). Notice that HPV infection is not inherently an exposure or an outcome. Rather, we labeled it as an exposure or an outcome based on the question we were asking and our beliefs about its relationship to the variable we were comparing it to. The paragraph above may have left you with the impression that measuring associations is the same thing as measuring causes and effects. That is not the case! In fact, you may already be familiar with the popular saying ‚Äúassociation does not equal causation‚Äù or ‚Äúcorrelation does not equal causation.‚Äù These sayings are correct. There is virtually an infinite number of possible examples of statistical associations that have nothing to do with cause and effect relationships. Even in cases were there isn‚Äôt necessarily a cause and effect relationship between two variables being studied, the terms exposure and outcome are still frequently used ‚Äì albeit somewhat arbitrarily ‚Äì in practice. In this book, we will also use the terms exposure and outcome broadly, and the terms do not necessarily imply a cause and effect relationship. 44.2 Contingency tables Many of the concepts and measures that will will explore in this chapter (e.g., incidence proportion differences, incidence proportion ratios, etc.) are easier to understand when we relate them to contingency tables, which are a commonly used tool in epidemiology. Contingency tables are also frequently called 2x2 tables (spoken as ‚Äútwo by two tables‚Äù) or crosstabs. The figure above shows the classic 2x2 contingency table. The ‚Äú2x2‚Äù part is meant to indicate that the table has two rows of data values (not including the column headers) and two columns of data values (not including the row headers), which is the most common contingency table configuration used in epidemiology. However, we can technically construct contingency tables with as many rows and columns as we would like. In the classic 2x2 configuration, the first row of values corresponds to people who were exposed to our first condition of interest and the second row of values corresponds to people who were not exposed to our first condition of interest (i.e., unexposed). Similarly, the first column of values corresponds to people who had the outcome we are interested in and the second column of values corresponds to people who did not have the outcome we are interested in. It is important to note that we can theoretically reconfigure the rows and columns of our contingency table anyway we would like, but this configuration is the configuration that the formulas you will find in nearly every epidemiology textbook will assume you are using. To illustrate how 2x2 contingency tables work, let‚Äôs return to our simulated data from the measures of occurrence chapter that included information from 10 people gathered over a 12-month period. If we came along and did a study of this group of people in month 2, the prevalence of disease would equal the number of people with disease divided by the number of people in the sample. In this case, that would be 2 people (01 and 02) divided by 10. Now let‚Äôs add another dimension to this figure. Specifically, let‚Äôs say that persons 02, 04, 06, 08, and 10 were exposed to some exposure of interest (shaded purple in the figure below). Under these conditions, how would we complete a 2x2 contingency table about this population? Let‚Äôs fill it in together, starting at the top-left corner. Cell a corresponds to people who were exposed and had the disease. Person 02, person 04, and person 06 were in the exposed group and had disease. Cell b corresponds to people who were exposed and did not have the disease. Person 08 and person 10 were in the exposed group and did not have disease. Cell c corresponds to people who were not exposed and had the disease. Person 01, person 03, and person 05 were not in the exposed group and had disease. Cell d corresponds to people who were not exposed and did not have the disease. Person 07 and person 09 were not in the exposed group and did not have disease. To calculate many of the measures we will discuss below, we will also need to fill in the marginal totals ‚Äì the row, column, and overall totals located on the margins of the table. Once we fill in the contingency table, we can easily calculate many measures of interest in epidemiology. For example, in the figure below we add the simulated population‚Äôs values back the contingency table and calculate the total for the first row. There were 5 people who were exposed and developed disease, and there were two people who were exposed and did not develop disease. Therefore, the total number of people who were exposed is 3 + 2 = 5. Before moving on, try to fill out the rest of the contingency table in your head. The answers are shown in the figure below. Now that we have developed an intuition for what contingency tables are and how to fill them in, let‚Äôs learn how to create them in R. 44.3 Building contingency tables in R There are many different possible ways to create contingency tables in R. We prefer building them as a matrix object. We haven‚Äôt talked much about matrix objects up the this point, and for the most part, we don‚Äôt need to use them very often. However, they do provide a convenient structure for replicating contingency tables in R. # Load the packages needed for the code below. library(dplyr, warn.conflicts = FALSE) library(freqtables) 44.3.1 Matrix dimensions In R, a matrix is a vector with 2 dimensions. What does that mean? Perhaps it‚Äôs best to learn what it means with through the use of an example. Here is a character vector: v &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) v ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; If we want to select element a, we can use bracket notation. The first element in the character vector, v, is a. So, we can select it with the code v[1], which means, ‚Äúreturn the first element of v‚Äù. v[1] ## [1] &quot;a&quot; Next, let‚Äôs create a matrix with character values. Remember, all elements of a vector must be of the same type (character, numeric, etc.), and a matrix is a special type of vector. So, all elements of a matrix must be of the same type. Below we will create a matrix object using almost the exact same code we used above. The only difference is that we pass the code used to create our character vector, c(\"a\", \"b\", \"c\", \"d\"), to the matrix() function. # Create a matrix object called mat mat &lt;- matrix(c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)) # Print the value stored in mat to the screen mat ## [,1] ## [1,] &quot;a&quot; ## [2,] &quot;b&quot; ## [3,] &quot;c&quot; ## [4,] &quot;d&quot; Notice that this matrix has 4 rows and 1 column by default. This time, we need to add both coordinates (dimensions) to our bracket notation if we want to select the ‚Äúa‚Äù element. We do so by typing the matrix name followed by square brackets that contain the row we want, a comma, and the column we want. So, matrix_name[row, column]. # Print the value stored in the first row and first column of mat to the screen # Note. In this case, mat[1] would return the same result, but that won&#39;t be # true when we have more complex matrices mat[1, 1] ## [1] &quot;a&quot; 44.3.2 Matrix to contingency tables Now, to make our matrix look and behave like a contingency table, we need our matrix to have 2 rows and 2 columns instead of 4 rows and 1 column. To make that happen, we will first adjust either the ncol or nrow argument to the matrix() function. # Create a matrix object called matrix_ct that has 2 columns instead of 1 matrix_ct &lt;- matrix( c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;), # Enter values in this order ncol = 2 ) # Print the value stored in matrix_ct to the screen matrix_ct ## [,1] [,2] ## [1,] &quot;a&quot; &quot;c&quot; ## [2,] &quot;b&quot; &quot;d&quot; Or‚Ä¶ # Create a matrix object called matrix_ct that has 2 rows instead of 4 matrix_ct &lt;- matrix( c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;), # Enter values in this order nrow = 2 ) # Print the value stored in matrix_ct to the screen matrix_ct ## [,1] [,2] ## [1,] &quot;a&quot; &quot;c&quot; ## [2,] &quot;b&quot; &quot;d&quot; Unfortunately, neither of the solutions above get us exactly the result we are looking for. Can you spot what‚Äôs wrong with them? Remember, we said above that the order of the cells in our contingency tables matters. The formulas we will find in nearly every epidemiology textbook will assume that our contingency tables are configured with a and b in the first row and c and d in the second row. However, the matrix above is configured such that a and c are in the first row and b and d are in the second row. That‚Äôs because, the byrow argument to the matrix() function is set to FALSE by default. This means that R will fill in the matrix values starting at location [1, 1] and filling down columns. We want R to fill in the matrix values starting at location [1, 1] and filling across rows. We do that by changing the default byrow = FALSE to byrow = TRUE. # Create a matrix object called matrix_ct that has 2 rows instead of 4 matrix_ct &lt;- matrix( c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;), # Enter values in this order nrow = 2, byrow = TRUE ) # Print the value stored in matrix_ct to the screen matrix_ct ## [,1] [,2] ## [1,] &quot;a&quot; &quot;b&quot; ## [2,] &quot;c&quot; &quot;d&quot; Now, we have a matrix object that looks and behaves like a standard 2x2 contingency table. 44.3.3 Add row and column names To make our matrix even easier to read and work with, let‚Äôs go ahead and add row and column labels. We do so using the dimnames() function. The dimnames() function is a little bit different than most of the functions we have used so far in that we can use it to get or set values. Meaning, if we only pass an R object (i.e., vector, data frame, or matrix) to the dimnames() function, then it will get ‚Äì or return ‚Äì the object‚Äôs dimension labels to us. Alternatively, we can also use the dimnames() function to set or assign labels to the object‚Äôs dimensions. Let‚Äôs learn how to get and set labels below. Above, we said that our matrix has a row dimension and column dimension. So, passing our matrix to the dimnames() function below will return the labels assigned to the rows and columns of the matrix. # Print the current dimension labels of matrix_ct to the screen dimnames(matrix_ct) ## NULL Of course, we haven‚Äôt set the labels of the rows and columns of matrix_ct, so R simply returns a NULL value (i.e., the lack of a value). Luckily, we can easily set the dimension labels with the dimnames() function as well. Before attempting to set our dimension labels, however, let‚Äôs first take a look at how many dimensions the matrix_ct object contains. # Print the current number of dimensions in matrix_ct to the screen dim(matrix_ct) ## [1] 2 2 It probably isn‚Äôt immediately obvious, but R is telling us that matrix_ct has 2 rows (the first number) and 2 columns (the second number) in the result above. This should make sense if we take another look at matrix_ct. # Print the value stored in matrix_ct to the screen matrix_ct ## [,1] [,2] ## [1,] &quot;a&quot; &quot;b&quot; ## [2,] &quot;c&quot; &quot;d&quot; We can see above that matrix_ct has 2 rows ([1,] and [2,]) and 2 columns ([,1] and [,2]). How should we label them? Well, remember that the value \"a\" corresponds to people who were exposed and had the outcome. So, we could reasonably label [1,] as Yes (i.e., exposed) and [,1] as Yes (i.e., had the outcome). Using the same logic, we could also reasonably label [2,] as No (i.e., unexposed) and [,2] as No (i.e., did not have the outcome).To do so, we create a list that contains the row and column labels, and then assign them to matrix_ct. We create a list object in R with the list() function. We first pass a vector of values we want to assign to the rows of our contingency table to our list, and then pass a vector of values we want to assign to the columns of our contingency table to our list. # Create a list object called my_dimnames that contains labels we want to # assign to the rows and columns of our contingency table my_dimnames &lt;- list( c(&quot;Yes&quot;, &quot;No&quot;), # Row names c(&quot;Yes&quot;, &quot;No&quot;) # Then column names ) # Print the value stored in my_dimnames to the screen my_dimnames ## [[1]] ## [1] &quot;Yes&quot; &quot;No&quot; ## ## [[2]] ## [1] &quot;Yes&quot; &quot;No&quot; We can then set or assign that list of labels to matrix_ct like this: # Assign the values stored in my_dimnames as the row and column labels of matrix_ct dimnames(matrix_ct) &lt;- my_dimnames # Print the value stored in matrix_ct to the screen matrix_ct ## Yes No ## Yes &quot;a&quot; &quot;b&quot; ## No &quot;c&quot; &quot;d&quot; That makes our contingency table a little easier for us to read and work with. However, there could still be some ambiguity as to what each Yes and each No correspond to. We can improve the labels by passing a named vector to the my_dimnames list like this: # Create a list object called my_dimnames that contains labels we want to # assign to the rows and columns of our contingency table my_dimnames &lt;- list( Exposure = c(&quot;Yes&quot;, &quot;No&quot;), # Row names Outcome = c(&quot;Yes&quot;, &quot;No&quot;) # Then column names ) # Print the value stored in my_dimnames to the screen my_dimnames ## $Exposure ## [1] &quot;Yes&quot; &quot;No&quot; ## ## $Outcome ## [1] &quot;Yes&quot; &quot;No&quot; We can once again set or assign the values stored in my_dimnames as row and column labels in matrix_ct. # Assign the values stored in my_dimnames as the row and column labels of matrix_ct dimnames(matrix_ct) &lt;- my_dimnames # Print the value stored in matrix_ct to the screen matrix_ct ## Outcome ## Exposure Yes No ## Yes &quot;a&quot; &quot;b&quot; ## No &quot;c&quot; &quot;d&quot; See how the labels make our contingency table easier to read? Finally, we can put all the steps above together into one code chunk like this: # Assign row and column labels to matrix_ct dimnames(matrix_ct) &lt;- list( Exposure = c(&quot;Yes&quot;, &quot;No&quot;), # Row names Outcome = c(&quot;Yes&quot;, &quot;No&quot;) # Then column names ) # Print the value stored in matrix_ct to the screen matrix_ct ## Outcome ## Exposure Yes No ## Yes &quot;a&quot; &quot;b&quot; ## No &quot;c&quot; &quot;d&quot; 44.3.4 Add margins Finally, let‚Äôs add marginal totals to our contingency table. Because we can‚Äôt add letters together, let‚Äôs use the numbers from the simulated population of people. # Create a contingency table called matrix_ct matrix_ct &lt;- matrix( c(a = 3, b = 2, c = 3, d = 2), ncol = 2, byrow = TRUE ) # Assign row and column labels to matrix_ct dimnames(matrix_ct) &lt;- list( Exposure = c(&quot;Yes&quot;, &quot;No&quot;), # Row names Outcome = c(&quot;Yes&quot;, &quot;No&quot;) # Then column names ) # Print the value stored in matrix_ct to the screen matrix_ct ## Outcome ## Exposure Yes No ## Yes 3 2 ## No 3 2 Next, let‚Äôs add the marginal totals to matrix_ct. The easiest way to do so is by passing our contingency table to the addmargins() function. # Create a contingency table called matrix_ct_margins # It is the matrix_ct with marginal totals added. matrix_ct_margins &lt;- addmargins(matrix_ct) # Print the value stored in matrix_ct_margins to the screen matrix_ct_margins ## Outcome ## Exposure Yes No Sum ## Yes 3 2 5 ## No 3 2 5 ## Sum 6 4 10 And now we have a contingency table, stored as an R object, that we can use to calculate many measures of association we are interested in. But before doing so, we need to quickly review one additional concept ‚Äì probability. 44.4 Probabilities As previously mentioned, statistics and statistical inference are critical tools in the practice of epidemiology, and probability theory is provides a foundation for statistics. Therefore, it is important for us to have a least a basic understanding of probability theory. Unfortunately, a complete review of probability theory is beyond the scope of this book. However, we will discuss a few of the most fundamental aspects of probability theory here and elsewhere in the book. When we talk about the probability of an event in everyday speech, we are typically making a statement about how likely it is that the event has already occurred, or how likely it is that it will occur at some point in the future. For example, we may say that there is a 90% chance that a person has a certain disease, or that there is a 50-50 chance of surviving a particular condition, or that 9 out of 10 people who drop out of our study are experiencing adverse outcomes. As these examples illustrate, we can talk about probabilities in a number of different ways. However, it is mathematically easiest to work with probabilities that are recorded as fractions, which are then converted to decimals. So, we will typically write probabilities as a number between 0 and 1, where the more likely an event is to occur, the closer the probability is to 1. and the less likely an event is to occur, the closer the probability is to 0. Additionally, it will be useful for us to distinguish between at least two conceptually different categories of probability. Frequency probability is the limit of the relative frequency of an event in a sequence of N random trials as N approaches infinity.1 For example, if we flip a coin a really large number of times ‚Äì a nearly infinite number of times ‚Äì then how frequently does the coin come up heads? The proportion of heads (or tails) would constitute a frequency probability. Spoiler alert: It‚Äôs approximately 50%. Central to the concept of frequency probability is the idea that a process is repeated a large number of times. Further, frequency probabilities may or may not have anything to do with the beliefs that humans have about how likely an event is to occur (at least in theory). Rather, frequency probabilities are simply the result of counting the occurrence of something a large number of times (at least in theory). This is the conceptualization of probability that plays a central role in the majority of the statistical tool and procedures we use in epidemiology. Subjective probability is a measure, ranging from 0 to 1, of the degree of belief about the occurrence of an event.1 For example, if we say there is a 50% probability of rain today, then we are making a subjective probability statement. We believe that it is just as likely to rain today as not. This statement can clearly not be a frequency probability. There is no way for us to repeat today a near infinite number of times and then count how times it rained. Today only occurs 1 time. Although this conceptualization of probability is widely considered less objective than frequency probabilities, it is probably the conceptualization that is most often implied when we humans discuss the probability of events in everyday conversation. The gap between the frequency probabilities at the center of many, if not most, epidemiologic methods and the subject probabilities we express colloquially is the source of a great deal of confusion for many people. In this book, when we use the word ‚Äúprobability,‚Äù we are generally referring to frequency probabilities. Further, we will attempt to explicitly state when we are using the word ‚Äúprobability‚Äù in the more subjective sense. 44.4.1 Frequency probabilities Let‚Äôs start with an example. The approximate frequency of Alzheimer‚Äôs Disease among Americans age 65+, by age group, is given in table 44.2. For instructional purposes, we will assume that having Alzheimer‚Äôs Disease (AD+) and not having Alzheimer‚Äôs Disease (AD-) are mutually exclusive (i.e., a person can‚Äôt be in both categories at the same time) and exhaustive (i.e., there aren‚Äôt other possible categories) categories. Further, we will assume that all people in this population are equally likely to be selected if we draw a sample. Table 44.2: Frequency of Alzheimer‚Äôs Disease in America by Age Group Age Group Alzheimer‚Äôs Disease No Alzheimer‚Äôs Disease Total 65-74 1.7 32.0 33.7 75-84 2.1 14.1 16.2 85+ 2.0 4.0 6.0 Total 5.8 50.1 55.9 a all values in millions We can define the probability of having Alzheimer‚Äôs Disease in our population of interest as: \\[P(AD)=\\frac{Number\\, of\\, people\\, with\\, AD}{Total\\, number\\, of\\, people\\, in\\, the\\, population}\\] In this case, the probability of Alzheimer‚Äôs Disease among American‚Äôs age 65+ is: \\[P(AD) = \\frac{5.8}{55.9} = 0.1\\] Equivalently, we can say that the marginal probability (because one of the marginal totals from the table was used as the numerator) of Alzheimer‚Äôs Disease among American‚Äôs age 65+ is 0.1. More generally, we can say that if some process is repeated a large number of times, \\(n\\), and if some resulting event with the characteristic \\(Y\\) occurs, \\(m\\) times, the relative frequency of occurrence of \\(Y\\), \\(\\frac{m}{n}\\) will be approximately equal to the probability of \\(Y\\).2 \\[P(Y)=\\frac{m}{n}\\] üóíSide Note: Notice that probability is sometimes written at \\(Pr()\\) and sometimes written as just \\(P()\\). 44.4.2 Conditional probabilities The probabilities described above were all marginal or unconditional probabilities. Unconditional probabilities describe how likely an event occurrence is without incorporating information about other events that may affect the first event. For example, the unconditional probability of rain would be the probability of rain before knowing if there are clouds in the sky. Conversely, the conditional probability of an event describes how likely an event occurrence is given that some other event has already occurred. Going back to our rain example, the probability of rain would likely change if there were dark clouds looming in the sky. The probability of rain given that (i.e., conditional on) there are dark clouds in the sky is an example of a conditional probability. An equivalent, but slightly different way to think conditional probabilities is as calculating a probability after filtering our data, or on a subset of our data. For example, we may to calculate the probability of Alzheimer‚Äôs Disease given that the randomly sampled person from our population of interest was in the 75-84 age group. This is a conditional probability and can be written as \\(P(AD = 1|Age\\, group = 75-84)\\). The vertical bar means ‚Äúgiven‚Äù. So, we can express this equation in words as ‚ÄúThe probability of Alzheimer‚Äôs Disease given that age group is 75 to 84.‚Äù Because the 75-84 age group is our new popuation of interest ‚Äì and consequently our new denominator ‚Äì we can filter our table to include the 75-84 age group only. The other rows of data are irrelevant. Table 44.3: Frequency of Alzheimer‚Äôs Disease in America by Age Group Age Group Alzheimer‚Äôs Disease No Alzheimer‚Äôs Disease Total 65-74 1.7 32.0 33.7 75-84 2.1 14.1 16.2 85+ 2.0 4.0 6.0 Total 5.8 50.1 55.9 a all values in millions Now, we can calculate our conditional probability of interest as: \\[P(AD)=\\frac{Number\\, of\\, people\\, with\\, AD}{Total\\, number\\, of\\, people\\, in\\, the\\, population\\, age\\, 75\\, to\\, 84}\\] In this case, the probability of Alzheimer‚Äôs Disease among American‚Äôs age 65+, given that a person is between the ages of 75 and 84, is: \\[P(AD = 1|Age\\, group = 75-84) = \\frac{2.1}{16.2} = 0.13\\] Before we can give a more general definition of conditional probabilities, we need to first introduce one additional concept ‚Äì joint probabilities. A joint probability is the probability that two conditions exist at the same time. For example, what is the probability that a person picked at random from our population of interest will be in the 75 to 84 age group AND have Alzheimer‚Äôs Disease? The probability that a person picked at random from our population of interest will be in the 75 to 84 age group and have Alzheimer‚Äôs Disease can be written symbolically as: \\[P(Age\\, group = 75-84\\, \\cap AD = 1)\\] The new symbol, \\(\\cap\\), in the equation above is read as ‚Äúintersects‚Äù and means ‚Äúand‚Äù. If we take look at table 44.4 Table 44.4: Frequency of Alzheimer‚Äôs Disease in America by Age Group Age Group Alzheimer‚Äôs Disease No Alzheimer‚Äôs Disease Total 65-74 1.7 32.0 33.7 75-84 2.1 14.1 16.2 85+ 2.0 4.0 6.0 Total 5.8 50.1 55.9 a all values in millions More generally, we can say that if some process is repeated a large number of times, \\(n\\), and if some resulting event with the characteristic \\(Y\\) occurs, \\(m\\) times, the relative frequency of occurrence of \\(Y\\), \\(\\frac{m}{n}\\) will be approximately equal to the probability of \\(Y\\).2 \\[P(Y)=\\frac{m}{n}\\] We can write out an equation for conditional probabilities that looks like this: \\[\\begin{equation} P(Y|X) = \\frac{P(Y \\cap X)}{P(X)} \\tag{44.1} \\end{equation}\\] The new symbol in the equation above that looks like an upside-down ‚ÄúU‚Äù is read as ‚Äúintersects.‚Äù So we would read equation (44.1) in the following way. \\(P(Y|X)\\) is read as ‚Äúthe probability of Y given X‚Äù. \\(Y \\cap X\\) is read as ‚Äúthe probability of Y and X‚Äù. \\(P(X)\\) is read as ‚Äúthe probability of X‚Äù. So, how would we put all of this together and say it in words? Give it a try in your head before reading on. The probability of Y given X is equal to the probability of Y and X divided by the probability of X. Visually, intersection looks like this: Where the circle on the left represents the area where Y is true, the circle on the right represents the area where X is true, and the overlapping section in the middle represents the area where Y is true AND X is true. With all of this new terminology, we can return to our definition of association. And sometimes, when trying to understand what something is, it can be helpful to understand what it is not. In this case, the opposite of an association is statistical independence. These formulas represent the lack of an association on the risk difference, risk ratio, and odds ration scales respectively. And notice what these equations are equal to when there is no association. These values are called Null values. ‚ÄúThe null value of a measure of association is the value that measure takes when there is no difference between the two groups being compared.‚Äù3 These formulas represent the lack of an association on the incidence proportion difference, incidence proportion ratio, and odds ratio scales respectively. Association exists when the distribution of the thing we are measuring is different, on average, in two groups. Alternatively, we can say that Knowing something about X tells you something (or helps you predict) about Y. 44.4.3 Terminology recap Our Term Definition Equation Probability If some process is repeated a large number of time, n, the relative frequency of occurrence of E, m/n, will be approximately equal to the probability of E. \\(P(E) = \\frac{m}{n}\\) Conditional probability The probability that some event occurs given that we know that some other event has already occurred. \\(P(Y|X) = \\frac{P(Y \\cap X)}{P(X)}\\) 44.5 Prediction Predictions, especially good ones, can obviously be useful on their own. We may know that people of a certain race/ethnicity are most likely to get a particular form of cancer. Knowing that may allow us to concentrate screening efforts more effectively. We may know that older adults who begin to have trouble managing their finances are more likely to develop dementia. We may be able to use that information as an early indicator of important health problems to come. However, in epidemiology, we are very often not content with predictions alone. It is extremely common for our questions and studies to either directly ask causal questions or imply causal relationships between variables. The reason we are often more interested in causal associations than mere predictions can be found directly in our definition of epidemiology. We want to control health problems. Said another way, we want to know why ‚Äùbad‚Äù things happen so that we can stop them from happening and/or why ‚Äúgood‚Äù things happen so that we can make them happen more often. This idea is simultaneously so straightforward and so complex. As we will see throughout the semester. Notice that in the cases above these predictions may be perfectly valid, but do they get us any closer to our ultimate goal of ‚Äúcontrolling health problems?‚Äù We can‚Äôt change anyone‚Äôs race or ethnicity, can we? Even if we could, I‚Äôm hard-pressed to think of an example of a health outcome that is caused directly by a person‚Äôs race or ethnicity. Race and ethnicity are just a proxy for the true unmeasured cause. Likewise, do you really believe that if we hired an accountant to help an older person manage their finances that they would no longer develop dementia? Of course not. We will look at how to calculate some of the most commonly used of these next. Example Smoking and pill Smoking and smoking cessation program 1,000,000 smokers 100,000 will die Pill will save 50% who take it, 10% can afford it. IP = 0.50, 5,000 lives saved Smoking cessation program will save 10%, but 100% get it. IP = 0.01, 10,000 lives saved 44.6 Terminology recap So, how do we calculate these measures of association? 44.7 Incidence proportion Remember that the incidence proportion is the proportion of the population who experiences a new occurrence of the condition of interest among those in the population who are at risk of experiencing a new occurrence the condition of interest during a given time frame. \\[\\frac{Count\\, of\\, new\\, occurrences}{Population\\, at\\, risk}\\] \\[\\frac{5}{9}=0.56\\] When we have two groups ‚Äì e.g., exposed and unexposed ‚Äì one way we can contrast outcomes in those groups is by calculating the ratio of the incidence proportion in the exposed to the incidence proportion in the unexposed. 44.8 Incidence proportion ratio More commonly called relative risk or risk ratio We like incidence proportion ratio because it is unambiguous and consistent with the previous terminology we have been using. \\[\\frac{Incidence\\, proportion\\, in\\, the\\, exposed}{Incidence\\, proportion\\, in\\, the\\, unexposed}\\] References "],["incidence-proportion-ratio-1.html", "45 Incidence proportion ratio 45.1 Interpretation", " 45 Incidence proportion ratio Relative risk Risk ratio \\[\\frac{Incidence\\, proportion\\, in\\, the\\, exposed}{Incidence\\, proportion\\, in\\, the\\, unexposed}\\] \\[\\frac{\\frac{a}{(a+b)}}{\\frac{c}{(c+d)}}\\] matrix_ct &lt;- matrix( c(a = 3, b = 2, c = 2, d = 2), ncol = 2, byrow = TRUE ) matrix_ct &lt;- addmargins(matrix_ct) dimnames(matrix_ct) &lt;- list( Exposure = c(&quot;Yes&quot;, &quot;No&quot;, &quot;col_sum&quot;), # Row names Disease = c(&quot;Yes&quot;, &quot;No&quot;, &quot;row_sum&quot;) # Then column names ) matrix_ct ## Disease ## Exposure Yes No row_sum ## Yes 3 2 5 ## No 2 2 4 ## col_sum 5 4 9 incidence_prop &lt;- matrix_ct[, &quot;Yes&quot;] / matrix_ct[, &quot;row_sum&quot;] incidence_prop ## Yes No col_sum ## 0.6000000 0.5000000 0.5555556 matrix_ct &lt;- cbind(matrix_ct, incidence_prop) matrix_ct ## Yes No row_sum incidence_prop ## Yes 3 2 5 0.6000000 ## No 2 2 4 0.5000000 ## col_sum 5 4 9 0.5555556 Calculate our incidence proportion ratio ipr &lt;- matrix_ct[&quot;Yes&quot;, &quot;incidence_prop&quot;] / matrix_ct[&quot;No&quot;, &quot;incidence_prop&quot;] ipr ## [1] 1.2 45.1 Interpretation Among the members of our population, those who were exposed had 1.2 times the incidence of disease compared to those who were not exposed over the 12 months of follow-up period. Among the members of our population, those who were exposed had 1.2 times the risk of disease compared to those who were not exposed over the 12 months of follow-up period. "],["incidence-proportion-difference.html", "46 Incidence proportion difference", " 46 Incidence proportion difference More commonly called the risk difference We like incidence proportion ratio because it is unambiguous and consistent with the previous terminology we have been using. "],["incidence-proportion-difference-1.html", "47 Incidence proportion difference 47.1 Interpretation", " 47 Incidence proportion difference Risk difference \\[{Incidence\\, proportion\\, in\\, the\\, exposed}-{Incidence\\, proportion\\, in\\, the\\, unexposed}\\] \\[\\frac{a}{(a+b)}-\\frac{c}{(c+d)}\\] matrix_ct ## Yes No row_sum incidence_prop ## Yes 3 2 5 0.6000000 ## No 2 2 4 0.5000000 ## col_sum 5 4 9 0.5555556 Calculate our incidence proportion difference ipd &lt;- matrix_ct[&quot;Yes&quot;, &quot;incidence_prop&quot;] - matrix_ct[&quot;No&quot;, &quot;incidence_prop&quot;] ipd ## [1] 0.1 47.1 Interpretation Among the members of our population, those who were exposed had 0.1 additional cases of disease per person compared to those who were not exposed over the 12 months of follow-up period. ipd * 100 ## [1] 10 Among the members of our population, those who were exposed had 10 additional cases of disease per 100 people compared to those who were not exposed over the 12 months of follow-up period. Tip 1: If you have the incidence proportion expressed as percentage, convert it to convenient fractions so that you can express it as the additional risk in a group of people who have the exposure. Tip 2: Focus your interpretation on the additional risk in the exposed group. Example: ‚ÄúThere were 10 additional cases of disease per 100 people in the group that was exposed, compared to the group without exposure.‚Äù Tip 3: Don‚Äôt forget to specify the time interval when using incidence proportion. Example: ‚ÄúIn the group that failed to adhere closely to the Mediterranean diet there were 120 excess deaths per 1,000 men during the two year period of observation compared to the group that did adhere to the Mediterranean diet.‚Äù "],["incidence-odds-ratio.html", "48 Incidence odds ratio 48.1 Interpretation", " 48 Incidence odds ratio Odds ratio \\[\\frac{Incidence\\, odds\\, in\\, the\\, exposed}{Incidence\\, odds\\, in\\, the\\, unexposed}\\] \\[\\frac{\\frac{a}{b}}{\\frac{c}{d}}\\] matrix_ct ## Yes No row_sum incidence_prop ## Yes 3 2 5 0.6000000 ## No 2 2 4 0.5000000 ## col_sum 5 4 9 0.5555556 Calculate odds incidence_odds &lt;- matrix_ct[, &quot;incidence_prop&quot;] / (1 - matrix_ct[, &quot;incidence_prop&quot;]) incidence_odds ## Yes No col_sum ## 1.50 1.00 1.25 incidence_odds &lt;- matrix_ct[, &quot;Yes&quot;] / matrix_ct[, &quot;No&quot;] incidence_odds ## Yes No col_sum ## 1.50 1.00 1.25 matrix_ct &lt;- cbind(matrix_ct, incidence_odds) matrix_ct ## Yes No row_sum incidence_prop incidence_odds ## Yes 3 2 5 0.6000000 1.50 ## No 2 2 4 0.5000000 1.00 ## col_sum 5 4 9 0.5555556 1.25 Calculate our incidence odds ratio ior &lt;- matrix_ct[&quot;Yes&quot;, &quot;incidence_odds&quot;] / matrix_ct[&quot;No&quot;, &quot;incidence_odds&quot;] ior ## [1] 1.5 48.1 Interpretation Among the members of our population, those who were exposed had 1.5 times the odds of incident disease compared to those who were not exposed over the 12 months of follow-up period. "],["incidence-rate-ratio.html", "49 Incidence rate ratio 49.1 Interpretation", " 49 Incidence rate ratio Incidence density Calculate person time at risk # Have R calculate the total person-months at risk pop_10_exposed_wide |&gt; # Keep only the rows with a status of At Risk filter(status == &quot;At Risk&quot;) |&gt; # Calculate the time between follow-up months for each person mutate(time_at_risk = xend_adjusted - x) |&gt; # Total all of the individual person-months at risk by exposure status group_by(exposed_f) |&gt; summarise(time_at_risk = sum(time_at_risk)) ## # A tibble: 2 √ó 2 ## exposed_f time_at_risk ## &lt;fct&gt; &lt;int&gt; ## 1 No 32 ## 2 Yes 32 matrix_ct &lt;- matrix( c(a = 3, b = 2, c = 2, d = 2), ncol = 2, byrow = TRUE ) matrix_ct &lt;- addmargins(matrix_ct) dimnames(matrix_ct) &lt;- list( Exposure = c(&quot;Yes&quot;, &quot;No&quot;, &quot;col_sum&quot;), # Row names Disease = c(&quot;Yes&quot;, &quot;No&quot;, &quot;row_sum&quot;) # Then column names ) matrix_ct ## Disease ## Exposure Yes No row_sum ## Yes 3 2 5 ## No 2 2 4 ## col_sum 5 4 9 # Create a contingency table as a matrix object matrix_ct_rates &lt;- matrix( c(a = 3, b = 2, c = 2, d = 2), ncol = 2, byrow = TRUE ) # Add person months to the contingency table person_months &lt;- c(exposed = 32, unexposed = 32) matrix_ct_rates &lt;- cbind(matrix_ct_rates, person_months) # Add column totals to the contingency table # Notice the &quot;margin = 1&quot; in the code below. That tells R to calculate the # column totals only -- not the row totals matrix_ct_rates &lt;- addmargins(matrix_ct_rates, margin = 1) # Add row and column labels to improve readability dimnames(matrix_ct_rates) &lt;- list( Exposure = c(&quot;Yes&quot;, &quot;No&quot;, &quot;col_sum&quot;), # Row names Disease = c(&quot;Yes&quot;, &quot;No&quot;, &quot;person_months&quot;) # Then column names ) matrix_ct_rates ## Disease ## Exposure Yes No person_months ## Yes 3 2 32 ## No 2 2 32 ## col_sum 5 4 64 incidence_rate &lt;- matrix_ct_rates[, &quot;Yes&quot;] / matrix_ct_rates[, &quot;person_months&quot;] incidence_rate ## Yes No col_sum ## 0.093750 0.062500 0.078125 matrix_ct_rates &lt;- cbind(matrix_ct_rates, incidence_rate) matrix_ct_rates ## Yes No person_months incidence_rate ## Yes 3 2 32 0.093750 ## No 2 2 32 0.062500 ## col_sum 5 4 64 0.078125 Calculate our incidence rate ratio irr &lt;- matrix_ct_rates[&quot;Yes&quot;, &quot;incidence_rate&quot;] / matrix_ct_rates[&quot;No&quot;, &quot;incidence_rate&quot;] irr ## [1] 1.5 49.1 Interpretation Among the members of our population, those who were exposed had 1.5 times the rate of disease compared to those who were not exposed over the 12 months of follow-up period. "],["incidence-rate-difference.html", "50 Incidence rate difference 50.1 Interpretation", " 50 Incidence rate difference matrix_ct_rates ## Yes No person_months incidence_rate ## Yes 3 2 32 0.093750 ## No 2 2 32 0.062500 ## col_sum 5 4 64 0.078125 Calculate our incidence risk difference ird &lt;- matrix_ct_rates[&quot;Yes&quot;, &quot;incidence_rate&quot;] - matrix_ct_rates[&quot;No&quot;, &quot;incidence_rate&quot;] ird ## [1] 0.03125 50.1 Interpretation Among the members of our population, those who were exposed had 0.03 additional cases of disease per person-month compared to those who were not exposed over the 12 months of follow-up period. ird * 100 ## [1] 3.125 Among the members of our population, those who were exposed had 3.1 additional cases of disease per 100 person-months compared to those who were not exposed over the 12 months of follow-up period. Tip 1: Remember that interpretations are now in person-time. Tip 2: Focus your interpretation on the additional risk in the exposed group. Example: ‚ÄúThere were 10 additional cases of disease per 100 units of person-time in the group that was exposed, compared to the group without exposure.‚Äù Tip 3: Don‚Äôt forget to specify the time interval when using incidence proportion. Example: ‚ÄúIn the group that failed to adhere closely to the Mediterranean diet there were 120 excess deaths per 100 person-years during the two year period of observation compared to the group that did adhere to the Mediterranean diet.‚Äù "],["cut-and-moved.html", "51 Cut and moved 51.1 Measures of association", " 51 Cut and moved 51.1 Measures of association Remember, that our measurement and analysis goals can be broken down into description, prediction, and/or causally explanation of health-related states or events in populations of people. We said that when we describe health-related states or events, we are not necessarily looking for associations in the data between two or more variables. Sometimes, we are simply looking at the distribution of values in a single variable/measure. For example: How many ventilators are available in Texas? What is the average age of people living in Florida? How much time elapses, on average, between exposure to a pathogen and occurrence of disease symptoms? However, there are also times when we want to compare the distribution of value in one variable within levels of another variable. Most people would consider these comparisons to be equivalent to measuring associations. For example: Are there more ventilators available in Texas or New York? Are people older, on average, in Florida or Pennsylvania? Is symptom onset quicker, on average, for Cholera or E. coli? ‚Äú[An association is a] statistical dependence between two or more events, characteristics, or other variables. An association is present if the probability of occurrence of an event or characteristic, or the quantity of a variable, varies with the occurrence of one or more other events, the presence of one or more other characteristics, or the quantity of one or more other variables.‚Äù1 Said another way, when the distribution (e.g., middle, spread, shape, proportion of people) of one variable is different on average across levels of a second variable, then there is an association between the first variable and the second variable. Yet another way to describe an association is to simply say that there is an association between two variables when knowing the value of variable one tells us something about (or helps us predict) the value variable two. Let‚Äôs simulate some data to help us understand what association looks like in action. # Load the packages we will need below library(dplyr, warn.conflicts = FALSE) library(freqtables) # Create a simulated data frame with two variables df &lt;- tibble( # Create one variable, x, that has 50 rows with a value of 0 and 50 rows with # a value of 1. x = c(rep(0, 50), rep(1, 50)), # Create a second variable, y, that has 100 values of either 0 or 1. # If x equals 0, then y have 25 rows with a value of 0 and 25 rows with a # value of 1. # If x equals 1, then y have 25 rows with a value of 0 and 25 rows with a # value of 1. y = c(rep(0, 25), rep(1, 25), rep(0, 25), rep(1, 25)) ) Now, let‚Äôs explore the relationship between x and y in the data we simulated above. Specifically, let‚Äôs ask R to calculate the distribution of y across levels of x. Here, ‚Äúthe distribution of y‚Äù means the proportion of 0‚Äôs and 1‚Äôs, and ‚Äúacross levels of x‚Äù means do the calculation for all rows where x equals 0 and separately for all rows where x equals 1. df |&gt; # Use the freq_table function to calculate the distribution of y across # levels of x freq_table(x, y) |&gt; # Keep a subset of the columns to make the results easier to read. select(row_var:col_cat, percent_row) ## # A tibble: 4 √ó 5 ## row_var row_cat col_var col_cat percent_row ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 x 0 y 0 50 ## 2 x 0 y 1 50 ## 3 x 1 y 0 50 ## 4 x 1 y 1 50 In the results above, we can see that when x is 0 (row_cat = 0), then y is 1 (col_cat = 1) 50% of the time. Similarly, when x is 1 (row_cat = 1), then y is 1 (col_cat = 1) 50% of the time. In this case, does the distribution of y differ across levels of x? No.¬†When x is 0, y equals 1 50% of the time, and when x is 1, y equals 1 50% of the time. In this case, does knowing the value of x tell us something about (or help us predict) the value y. No.¬†The value of y is equally likely to be 1 or 0 no matter what the value of x is. So, is there an association between x and y in the simulated data above? No, there is no association between x and y in the simulated data above. Said another way, x and y and statistically independent of each other. Now, let‚Äôs simulate a second data frame where x and y are associated with each other. # Set the seed for the random number generator so that we can reproduce our results set.seed(123) # Create a simulated data frame with two variables df &lt;- tibble( # Create one variable, x, that has 100 values of either 0 or 1. # The probability of a value being 0 over the long run is 0.5 and the # probability of a value being 1 over the long run is 0.5 x = sample(0:1, 100, TRUE, c(0.5, 0.5)), # Create a second variable, y, that has 100 values of either 0 or 1. # If x equals 0, then the probability of the y value being 0 over the long run is # 0.2 and the probability of the y value being 1 over the long run is 0.8. # If x equals 0, then the probability of the y value being 0 over the long run is # 0.5 and the probability of the y value being 1 over the long run is 0.5. y = if_else( x == 0, sample(0:1, 100, TRUE, c(0.2, 0.8)), sample(0:1, 100, TRUE, c(0.5, 0.5)) ) ) And let‚Äôs once again explore the relationship between x and y in the data we simulated above. df |&gt; # Use the freq_table function to calculate the distribution of y across # levels of x freq_table(x, y) |&gt; # Keep a subset of the columns to make the results easier to read. select(row_var:col_cat, percent_row) ## # A tibble: 4 √ó 5 ## row_var row_cat col_var col_cat percent_row ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 x 0 y 0 19.1 ## 2 x 0 y 1 80.9 ## 3 x 1 y 0 52.8 ## 4 x 1 y 1 47.2 In the results above, we can see that when x is 0 (row_cat = 0), then y is 1 (col_cat = 1) 81% of the time. Alternatively, when x is 1 (row_cat = 1), then y is 1 (col_cat = 1) 47% of the time. In this case, does the distribution of y differ across levels of x? Yes! When x is 0, y equals 1 81% of the time, and when x is 1, y equals 1 only 47% of the time. In this case, does knowing the value of x tell us something about (or help us predict) the value y. Yes! The value of y is more likely to be 1 when x is 0 than when x is 1. So, is there an association between x and y in the simulated data above? Yes! There is an association between x and y in the simulated data above. We can also say that there is a statistical dependence between x and y. At this point, we hope you are starting to develop an intuitive understanding of associations. Let‚Äôs try to simplify and formalize our definition of an association even further by writing it as an equation. We recognize that many people are scared of equations (including us sometimes), but this is an occasion where an equation might actually make it easier for us to understand the concept. Let‚Äôs give it a shot. \\[\\begin{equation} Pr[Y=1|X=1] \\neq Pr[Y=1|X=0] \\tag{51.1} \\end{equation}\\] Now, let‚Äôs break equation (51.1) down into its component parts and see what it tells us. \\(Pr[]\\) is read as ‚Äúthe probability of‚Äù. The probability of is also frequently just written as \\(P()\\) \\(Y=1\\) is read as ‚ÄúY equals 1.‚Äù Typically, Y equals 1 when the thing that Y represents happens. \\(Y=0\\) (not shown above) would be read as ‚ÄúY equals 0,‚Äù and would typically mean that the thing Y represents did not happen. For example, if Y represents Alzheimer‚Äôs Disease, then \\(Y=1\\) for each person who has Alzheimer‚Äôs Disease and \\(Y=0\\) for each person who does not have Alzheimer‚Äôs Disease. \\(|\\) is read as ‚Äúgiven that‚Äù or ‚Äúif‚Äù. \\(X=1\\) is read as ‚ÄúX equals 1.‚Äù Typically, X equals 1 when the thing that X represents happens. \\(X=0\\) (not shown above) would be read as ‚ÄúX equals 0,‚Äù and would typically mean that the thing X represents did not happen. For example, if X represents APOEe4 allele, then \\(X=1\\) for each person who has an APOEe4 allele and \\(X=0\\) for each person who does not have an APOEe4 allele. So, how would we put all of this together and say it in words? Give it a try in your head before reading on. The probability that Y equals 1 given that X equals 1 is not equal to the probability that Y equals 1 given that X equals 0. Or, if we‚Äôre still talking about Alzheimer‚Äôs Disease and APOEe4 allele: The probability of Alzheimer‚Äôs Disease among people who carry at least one APOEe4 allele is not equal to the probability of Alzheimer‚Äôs Disease among people who do not carry any APOEe4 alleles. And what exactly does ‚Äúprobability‚Äù mean? References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
