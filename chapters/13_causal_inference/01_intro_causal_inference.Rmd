# Introduction to Causal Inference

```{=html}
<!-- 
Hidden comments placeholder
---------------------------

To preview:
bookdown::preview_chapter("chapters/13_causal_inference/01_intro_causal_inference.Rmd")

Copy and paste:
üëÜ**Here's what we did above:**

-->
```

::: under-construction
`r fontawesome::fa("hammer", fill = "#000000", height="1em")` This chapter is under heavy development and may still undergo significant changes.
:::

```{r intro-causal-inference, echo=FALSE}
# knitr::include_graphics("img/13_causal_inference/01_intro_causal_inference/figure.png")
```

<!-- 
Define: 
Cause
Causality
Causal inference
-->

<!-- Let's start with a simple question: What does it mean for something to cause something else? -->

Causal inference is one of our favorite topics. Perhaps because it is so challenging (as we will see); yet, it is simultaneously at the very core of so much of our work as epidemiologists. Before getting into the specifics of causal inference, however, we want to briefly remind you about some of the topics we talked about earlier in the book. 

When our goal is to make predictions, our analyses will focus on associations. As discussed in the [measures of association chapter][Measures of Association], association exists when the distribution of the thing we are measuring is different, on average, in two groups. Alternatively, we can say that knowing something about X tells us something (or helps us predict) about Y. 

Predictions, especially good ones, can obviously be useful on their own. We may know that people of a certain race/ethnicity are most likely to get a particular form of cancer. Knowing that may allow us to concentrate screening efforts more effectively. We may know that older adults who begin to have trouble managing their finances are more likely to develop dementia. We may be able to use that information as an early indicator of important health problems to come. 

However, we are often not content with predictions alone in epidemiology. It is extremely common for our questions, and subsequent studies, to either directly ask causal questions -- either directly or indirectly. The reason we are often more interested in causal relationships than mere predictions can be found directly in our definition of epidemiology -- we want to control health problems. Said another way, we want to know _why_ -- not just _if_ -- ‚Äùbad‚Äù things happen so that we can stop them from happening and/or why ‚Äúgood‚Äù things happen so that we can make them happen more often. 

Notice that in the prediction examples we mentioned above (i.e., race/ethnicity predicts cancer, financial management predicts dementia) may be perfectly valid, but do they get us any closer to our ultimate goal of ‚Äúcontrolling health problems?‚Äù We can‚Äôt change anyone's race or ethnicity, can we? Even if we could, do race/ethnicity -- social constructs -- directly cause cancer? Or do race and ethnicity serve as proxies for the true unmeasured causes of cancer that disproportionately affect people of certain races and ethnicity. In other words, if we suddenly decided to start labeling people as members of a different race, while everything else about them remains the same, would we expect an impact on their cancer risk? Likewise, do we really believe that older people will no longer develop dementia if we just hire accountants to help them manage their finances better? Of course not! Again, to accomplish our goal of controlling health problems, we will often need to understand _why_ things happen, not just _if_ they will happen. And when we ask _why_ questions, we are asking causal questions. On the one hand, these questions are usually incredibly exciting and have the potential to lead to real, tangible changes in population health. On the other hand, our two primary tools -- data and statistics -- are not sufficient to answer such questions in and of themselves.

Partially because we can't rely on data and statistics alone to answer causal questions, generations of people were taught that answering causal questions is generally beyond the reach of science. Today, however, causal inference and causal effect estimation are extremely active areas of methodological research and there are several theories and methods we can call upon when we want to answer causal questions -- as long as we are willing to make some assumptions.



So, if we can‚Äôt rely on checklists, what can we rely on? Well, another modern model of causation is Rothman‚Äôs Model of Sufficient and Component Causes (SCC). When using this model, we start with the outcome, or effect, in mind and work backwards to hypothesize what set of causes can bring about the outcome. 

<!-- Add a much broader discussion of multicausality -->

## Rothman's model of sufficient and component causes

<!-- 
Add the following elements to this section: 
- History
- Examples: light switch, broken hip, smoking - lung cancer and heart disease
- Intuition: tendency to identify the final component cause as the unique cause
- The causal pie model
- Multicausality
- Strength of causes 
-->

Rothman‚Äôs Model of Sufficient and Component Causes starts from the effect and works backwards to a set of causes. It answers the question: ‚ÄúGiven a particular effect/outcome, what are the various events that might have been its cause?‚Äù

<!-- The slides below has a ton of text on it that we need to write out. In fact, it would be nice if we could make all of these pies directly in R. -->

```{r intro-causal-inference-rmscc-sufficient-causes, echo=FALSE}
knitr::include_graphics("img/13_causal_inference/01_intro_causal_inference/rmscc_sufficient_causes.png")
```

This proposed SCC gives a more realistic meaning to the statement ‚Äúsmoking causes lung cancer,‚Äù and helps us address questions like, ‚Äúmy uncle smoke for 50 years and didn‚Äôt get lung cancer. So, how can you say that smoking causes lung cancer?‚Äù Or, ‚ÄúMy aunt got lung cancer last year, but she never smoked a cigarette in her life. How can you say that smoking causes lung cancer?‚Äù

```{r intro-causal-inference-rmscc-sufficient-causes-smoking, echo=FALSE}
knitr::include_graphics("img/13_causal_inference/01_intro_causal_inference/rmscc_sufficient_causes_smoking.png")
```

Smoking represents a component cause of lung cancer, but is neither a necessary cause of lung cancer, nor is it sufficient to cause lung cancer in the absence of other component causes.

<!-- This is explaining the figure above. We need to make this. all flow together better. -->

Each component cause has a corresponding causal complement made up of the other factors that make up a sufficient causal set. In the case of smoking sufficient cause 3, U3 and the exposure to pollution represent a causal complement. 

Smoking is also not necessary. There are sufficient causes that do not include smoking.

Here‚Äôs another example ‚Äì three sufficient cause sets for cervical cancer. 

```{r intro-causal-inference-rmscc-sufficient-causes-cervical-cancer, echo=FALSE}
knitr::include_graphics("img/13_causal_inference/01_intro_causal_inference/rmscc_sufficient_causes_cervical_cancer.png")
```

Note that HPV infection is present in all of the sufficient causes. According to the model HPV infection is thus a necessary cause for cervical cancer.

<!-- 
Put all of this into text 
- Useful for pedagogic reasons.
  - Multicausality.
  - Attributable fractions sum to greater than 1. **Give an example
- Application to actual data analysis is yet to be determined.
- Limited to dichotomous exposures and outcomes.
-->

Although Rothman‚Äôs model of sufficient and component causes is useful for causal theory and causal inference, there is significant debate about its usefulness in an applied setting. So, is there a framework that is useful to us in an applied setting?

## Counterfactuals

<!-- Less text. More visual -->

As we already discussed, there isn‚Äôt complete consensus on the definition of causality, or even that it can be defined. However, most people who study causal inference seem to agree that if causality is a testable thing, then counterfactuals provide the best definition we currently have for that thing. Two ways of thinking about counterfactuals that I find appealing are: 

1. What would happen to Y (outcome) had X (exposure) been different and everything else been the same?

2. Do something, see what happens, get in a time machine, go back in time, do a different thing, see what happens. The difference between what happened the first time and what happened the second time is the causal effect.

Of course, we can‚Äôt measure either of these directly. However, there are methods that are theoretically able to give us the same results we would get if we could do these things to populations if we are willing to make certain assumptions. One method is to use an idealized randomized experiment. What do I mean by idealized?

## Idealized random experiments

<!-- Flesh this out more, or maybe just delete it? -->

- When Feasible and ethically possible.
  - No loss to follow-up.
  - Full adherence to the assigned exposure/treatment.
  - Double blind assignment.
  - Reasonably ‚Äúnatural‚Äù conditions.
  - Large enough sample to rule out random error.
- Effect of exposure/treatment on outcome can be interpreted as the average causal effect.

What is an idealized random experiment? It works because there are no common causes or common effects of exposure and outcome. The only thing that determines exposure group composition is the coin flip. Not race, SES, behavior, genetic predisposition, or anything else that could account for an association between exposure/treatment and outcome. So, the ability to estimate true counterfactual average causal effects is within humanities reach. It is theoretically possible to answer the causal questions that we often truly care about.

But, how feasible do these conditions sound in the real world? 

## Observational studies

<!-- Maybe just delete this stuff? -->

In the real world, we often have to rely on data from observational studies to draw our conclusions. Why is that potentially a problem? It‚Äôs potentially a problem because the lack of random assignment opens the door to confounding and bias. These are statistical associations in our data that do not come from causal effects. Another issue that is not unique to observational studies, but is an issue nevertheless, is random error / chance / sampling variability. 

So, the question becomes, ‚Äùis it possible to get the same conclusion from observational data that we would have gotten if we had carried out a randomized experiment?‚Äù

## DAGS

<!-- For now, I think I just want to include stuff about creating DAGs in the book. Trying to distill all of causal inference into a single chapter is biting off more than I can chew right now. -->

The answer is, ‚ÄùYes!‚Äù‚Ä¶. If we are willing to make certain assumptions. 

The field of statistics has produced many tools that we can use to quantify, and therefor estimate, the effect of random error / chance / sampling variability if we are willing to make certain assumptions. 

Further, pioneers in the field of causal inference (e.g., Judea Pearl, Jamie Robbins, Sander Greenland, and Miguel Hernan) have refined and tested tools that allow us to estimate bona fide average causal effects from observational data if we are willing to make certain assumptions. One of those tools -- the one we will focus on in this chapter ‚Äì is called directed acyclic graphs, or DAGs for short. A very simple DAG is shown here. 

<!-- Definitely need to update the figure below. Lots of text. Make this in R. -->

```{r intro-causal-inference-simple-dag, echo=FALSE}
knitr::include_graphics("img/13_causal_inference/01_intro_causal_inference/simple_dag.png")
```

<!-- Note from PP: Move this slide (below) to the end after we talk about what DAGs are. -->

Today, we will talk about some of the basic nuts and bolts of DAGs. In future modules, we will continue to learn more about their application.

The last bullet is key. You‚Äôve all heard that association or correlation does not equal causation, which is true. But, if you are willing to accept the assumptions baked into the qualitative causal model (the graph picture), and you follow the rules of D-separation in your analysis, then any statistical associations ARE casual effects. 

The remaining controversy and uncertainty around DAGs comes for the ‚Äúif you are willing to accept the assumptions‚Äù part of what I just said. The assumptions are not always reasonable and can‚Äôt always be tested directly. However, even in the absence of complete certainty, DAGs have shown themselves to be a useful tool in epidemiology over and over. 

- Not magic, just a tool.
- Works because it encodes mathematical information, given assumptions.
- Association flows in both directions
- No distinction between Causal / Preventive (listens to).
- Made of nodes and edges (arrows).
- Arrows must go in one direction (i.e., not double-headed).
- Acyclic ‚Äì Never get back to where you started if you follow arrows.
- Arrows indicate causal effects.
- Deleting arrows indicates a lack of a causal effect.
- Simultaneously qualitative causal models and statistical models.

<!-- Note from PP: Move this slide (above) to the end after we talk about what DAGs are. -->

As I said before, DAGs are made from nodes and edges. You may see the DAGs where the nodes are literal dots, and you may see DAGs that use variable names directly as the nodes. It seems to me that the second form is more common in the epidemiology literature. 

```{r intro-causal-inference-nodes-and-edges, echo=FALSE}
knitr::include_graphics("img/13_causal_inference/01_intro_causal_inference/nodes_and_edges.png")
```

You will sometimes hear the relationships between variables in a DAG describe in terms of family relationships. In this DAG, X is a parent of Y and a grandparent of Z. Equivalently, Y is a child of X , Z is a child of Y, and Z is a grandchild of X. 

```{r intro-causal-inference-descendants, echo=FALSE}
knitr::include_graphics("img/13_causal_inference/01_intro_causal_inference/descendants.png")
```

Paths are any arrow-based route between two variables on the graph. In this DAG, there is a path from X to Y, from Y to Z, and from X  to Z that goes through Y. Paths are blocked or open according to D-separation rules, which we will discuss in just a moment.

```{r intro-causal-inference-paths, echo=FALSE}
knitr::include_graphics("img/13_causal_inference/01_intro_causal_inference/paths.png")
```

Colliders are when two arrowhead ‚Äúcollide‚Äù into a node. 

```{r intro-causal-inference-colliders-01, echo=FALSE}
knitr::include_graphics("img/13_causal_inference/01_intro_causal_inference/colliders_01.png")
```

Note that colliders are path specific. For example, Y is a collider on the X -> Y <- Z path, but it is not a collider on the X -> Y -> W path. 

```{r intro-causal-inference-colliders-02, echo=FALSE}
knitr::include_graphics("img/13_causal_inference/01_intro_causal_inference/colliders_02.png")
```

Common causes are an important concept when using DAGs. In this DAG, Y is a common cause of X and Z. We know this because an arrow points from Y to X and from Y to Z.

It‚Äôs important to note that statistical association can follow any path regardless of the direction of the arrows (in the absence of colliders). Causal effects only follow the direction of the arrows (assuming your assumptions are correct). So, in this case, we expect X to be associated with Z even though X does not cause Z. We can also say that Y confounds the relationship (or lack of relationship) between X and Z. 

```{r intro-causal-inference-common-causes, echo=FALSE}
knitr::include_graphics("img/13_causal_inference/01_intro_causal_inference/common_causes.png")
```

and from Y to Z. Note that Z is a collider on the path X -> Z <- Y and on the path Y -> Z <- X.

It‚Äôs important to note that statistical association can follow any path regardless of the direction of the arrows (in the absence of colliders). Causal effects only follow the direction of the arrows (assuming your assumptions are correct). So, in this case, we do not expect X to be associated with Y even though there is a path from X to Y. We can also say that Z blocks the association between X and Y.

```{r intro-causal-inference-common-effects, echo=FALSE}
knitr::include_graphics("img/13_causal_inference/01_intro_causal_inference/common_effects.png")
```

### D Separation Rules

More generally, we can use the rules of D-separation to estimate average causal effects from DAGs when all assumptions are met. Here they are in their totality. Let‚Äôs quickly look at each in isolation.

1. If there are no variables being conditioned on, a path is blocked if and only if two arrowheads on the path collide at some variable on the path.
2. Any path that contains a non-collider that has been conditioned on is blocked.
3. A collider that has been conditioned on does not block a path. 
4. A collider that has a descendant that has been conditioned on does not block a path.
5. Two variables are D-separated if all paths between them are blocked.

```{r intro-causal-inference-d-separation-01, echo=FALSE}
knitr::include_graphics("img/13_causal_inference/01_intro_causal_inference/d_separation_01.png")
```

The first path is open. The second path is closed.

```{r intro-causal-inference-d-separation-02, echo=FALSE}
knitr::include_graphics("img/13_causal_inference/01_intro_causal_inference/d_separation_02.png")
```

The first path is open. The second path is closed.

```{r intro-causal-inference-d-separation-03, echo=FALSE}
knitr::include_graphics("img/13_causal_inference/01_intro_causal_inference/d_separation_03.png")
```

The first path is closed The second path is open.

```{r intro-causal-inference-d-separation-04, echo=FALSE}
knitr::include_graphics("img/13_causal_inference/01_intro_causal_inference/d_separation_04.png")
```

Because we conditioned on W, the path X -> Y <- Z is open.

As the course progresses, we will use the rules of D-separation to better understand, and potentially control for, bias and confounding.

Now, let‚Äôs practice a little bit more.

<!-- Below: from lab warm-up -->

## Create DAGs in R

Install and load Dagitty and ggdag

We are learning about causal inference in this module, and we are learning how to use directed acyclic graphs (DAGs) as a tool that can help us reason through causal inference. It turns out that there is also an R package that can help us create and interpret DAGs -- `dagitty`. In the code below, we will introduce `dagitty` and walk through some of the package's basic functionality together.

Here is a [link to a useful website](http://www.dagitty.net/) for learning more about `dagitty`.

Here is a [link to a useful website](https://cran.r-project.org/web/packages/ggdag/vignettes/intro-to-ggdag.ht) for learning more about `ggdag`.

Common effects are also an important concept when using DAGs. In this DAG, Z is a common effect of X and Y. We know this because an arrow points from X to Z 

### Create a DAG in the web browser

<!-- Add screenshots -->

The easiest way to get started with using `dagitty` is arguably to draw our DAGs using `dagitty` in our web browser. You can do that by navigating to http://www.dagitty.net/dags.html.

To start with a blank slate, click **Model**, then **New Model**.

To add a node to the graph, click anywhere on the graph space and name your new node.

To connect nodes, first click the **cause** node and then click the **effect** node.

To save your DAG code, copy it from the **Model code** pane on the right side of the screen. Then, paste that code into an R code chunk. Just make sure you wrap the code you paste from the web browser in single quotes and place it inside the `daggity()` function.

## Summary

In this chapter, we discussed ...

```{r intro-causal-inference-clean-up, echo=FALSE}
# clean up
rm(list = ls())
```

